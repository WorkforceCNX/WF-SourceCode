{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4990a7ea-4935-475f-bb03-d6657f954fa1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Library listü§ñ\n",
    "import glob, logging, warnings, polars as pl, datetime, os, zipfile, xml.dom.minidom\n",
    "from datetime import datetime as dt, time as t, timedelta\n",
    "import pandas as pd, numpy as np, sqlalchemy as sa, xlsxwriter\n",
    "from sqlalchemy import create_engine, text\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "from polars.exceptions import ColumnNotFoundError, PanicException\n",
    "from pathlib import Path\n",
    "from IPython.display import HTML\n",
    "from tabulate import tabulate\n",
    "# -----------------------------------------------------------------------------------------------#\n",
    "# --- Logging configurationüìú ---\n",
    "log_directory = Path(os.environ['USERPROFILE']) / r'Concentrix Corporation//CNXVN - WFM Team - Documents//DataBase//DataFrame//BKN//ScriptLogs//'\n",
    "log_directory.mkdir(parents=True, exist_ok=True) \n",
    "log_filename = log_directory / f\"import_log_{dt.now():%Y%m%d_%H%M%S}.log\"\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # (DEBUG, INFO, WARNING, ERROR, CRITICAL)\n",
    "    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_filename, encoding='utf-8'), \n",
    "    ],force=True)\n",
    "# Create logger object\n",
    "logger = logging.getLogger('ServerImportScript')\n",
    "# -----------------------------------------------------------------------------------------------#\n",
    "# Source collectionüì•\n",
    "user_credential = Path(os.environ['USERPROFILE']) / r'Concentrix Corporation//CNXVN - WFM Team - Documents//'\n",
    "\n",
    "# 0Ô∏è‚É£1Ô∏è‚É£[BKN]AHT2üóÉÔ∏è\n",
    "AHT2_TABLE_NAME = \"BCOM.AHT2\"\n",
    "Folder_AHT2 = user_credential / r'DataBase//DataRaw//BKN//AHT2//'\n",
    "log_AHT2_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//AHT2_log.xlsx'\n",
    "AHT2_schema = ['FileName', 'ModifiedDate', 'Date', 'Agent Name Display', 'Answered Language Name', 'Measure Names', 'Measure Values']\n",
    "# 0Ô∏è‚É£2Ô∏è‚É£[BKN]ROSTERüóÉÔ∏è\n",
    "ROSTER_TABLE_NAME = \"BCOM.ROSTER\"\n",
    "Folder_ROSTER = user_credential / r'DataBase//DataRaw//BKN//ROSTER//'\n",
    "log_ROSTER_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//ROSTER_log.xlsx'\n",
    "ROSTER_schema = ['FileName', 'ModifiedDate', 'Emp ID', 'Name', 'Attribute', 'Value', 'LOB', \n",
    "                 'team_leader', 'week_shift', 'week_off', 'OM', 'DPE', 'Work Type', 'QA']\n",
    "# 0Ô∏è‚É£3Ô∏è‚É£[BKN]EPSüóÉÔ∏è\n",
    "EPS_TABLE_NAME = \"BCOM.EPS\"\n",
    "Folder_EPS = user_credential / r'DataBase//DataRaw//BKN//EPS//'\n",
    "log_EPS_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//EPS_log.xlsx'\n",
    "EPS_schema = ['FileName', 'ModifiedDate', 'sitecode', 'manager_username', 'Username', 'Date', 'Session Login', \n",
    "              'Session Logout', 'Session Time', 'BPE Code', 'Total Time', 'SessionLogin_VN', 'SessionLogout_VN',\n",
    "              'NightTime', 'DayTime', 'Night_BPE', 'Day_BPE']\n",
    "# 0Ô∏è‚É£4Ô∏è‚É£[BKN]CPIüóÉÔ∏è\n",
    "CPI_TABLE_NAME = \"BCOM.CPI\"\n",
    "Folder_CPI = user_credential / r'DataBase//DataRaw//BKN//CPI//'\n",
    "log_CPI_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//CPI_log.xlsx'\n",
    "CPI_schema = ['FileName', 'ModifiedDate', 'Date', 'Staff Name', 'Hour Interval Selected', 'Channel', \n",
    "              'Item Label', 'Item ID', \"'Item ID'\", 'Time Alert', 'Nr. Contacts', 'Item Link', 'Time']\n",
    "# 0Ô∏è‚É£5Ô∏è‚É£[GLB]RAMCOüóÉÔ∏è\n",
    "RAMCO_TABLE_NAME = \"GLB.RAMCO\"\n",
    "Folder_RAMCO = user_credential / r'DataBase//DataRaw//GLOBAL//RAMCO//'\n",
    "log_RAMCO_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//RAMCO_log.xlsx'\n",
    "RAMCO_schema = ['FileName', 'ModifiedDate', 'EID', 'Employee_Name', 'Employee_type', 'Date', 'Code']\n",
    "# 0Ô∏è‚É£6Ô∏è‚É£[GLB]OT_RAMCOüóÉÔ∏è\n",
    "OT_RAMCO_TABLE_NAME = \"GLB.OT_RAMCO\"\n",
    "Folder_OT_RAMCO = user_credential / r'DataBase//DataRaw//GLOBAL//OT_RAMCO//'\n",
    "log_OT_RAMCO_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//OT_RAMCO_log.xlsx'\n",
    "OT_RAMCO_schema = ['FileName', 'ModifiedDate', 'employee_code', 'employee_name', 'Employee Type', 'OT Type', 'Date', 'Status', 'Hours']\n",
    "# 0Ô∏è‚É£7Ô∏è‚É£[GLB]PremHdaysüóÉÔ∏è\n",
    "PremHdays_TABLE_NAME = \"GLB.PremHdays\"\n",
    "Folder_PremHdays = user_credential / r'DataBase//DataRaw//GLOBAL//HOLIDAY_MAPPING//'\n",
    "log_PremHdays_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//PremHdays_log.xlsx'\n",
    "PremHdays_schema = ['FileName', 'ModifiedDate', 'Date', 'Holiday']\n",
    "# 0Ô∏è‚É£8Ô∏è‚É£[GLB]NormHdaysüóÉÔ∏è\n",
    "NormHdays_TABLE_NAME = \"GLB.NormHdays\"\n",
    "Folder_NormHdays = user_credential / r'DataBase//DataRaw//GLOBAL//HOLIDAY_MAPPING_NONBILLABLE//'\n",
    "log_NormHdays_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//NormHdays_log.xlsx'\n",
    "NormHdays_schema = ['FileName', 'ModifiedDate', 'Solar Day', 'Lunar Day', 'Holiday']\n",
    "# 0Ô∏è‚É£9Ô∏è‚É£[GLB]EmpMasterüóÉÔ∏è\n",
    "EmpMaster_TABLE_NAME = \"GLB.EmpMaster\"\n",
    "Folder_EmpMaster = user_credential / r'DataBase//DataRaw//GLOBAL//WDD//'\n",
    "log_EmpMaster_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//EmpMaster_log.xlsx'\n",
    "EmpMaster_schema = ['FileName', 'ModifiedDate', 'EMPLOYEE_NUMBER', 'PREVIOUS_PAYROLL_ID', 'FIRST_NAME', 'MIDDLE_NAME', 'LAST_NAME', \n",
    "                    'FULL_NAME', 'Work Related Status', 'Work Related (Extended Status)', 'Service Type', 'WAH & Hybrid Platform', \t\n",
    "                    'ORIGINAL_DATE_OF_HIRE', 'LEGAL_EMPLOYER_HIRE_DATE', 'Continuous Service Date', 'Fixed Term Hire End Date', \n",
    "                    'Contract End Date', 'PERSON_TYPE', 'WORKER_CATEGORY', 'Time Type', 'Employee Type', 'Last Promotion Date', \n",
    "                    'Assignment Category', 'Email - Work', 'BUSINESS_UNIT', 'Job Code', 'Job Title', 'Business Title', 'Cost Center - ID', \n",
    "                    'Cost Center - Name', 'LOCATION_CODE', 'LOCATION_NAME', 'CNX BU', 'Concentrix LOB', 'Process', 'COMPANY', \n",
    "                    'MANAGEMENT_LEVEL', 'Job Level', 'Compensation Grade', 'JOB_FUNCTION_DESCRIPTION', 'JOB_FAMILY', 'MSA', 'MSA Client', \n",
    "                    'MSA Program', 'ACTIVITY ID', 'SUPERVISOR_ID', 'SUPERVISOR_FULL_NAME', 'SUPERVISOR_EMAIL_ID', 'MANAGER_02_ID', \n",
    "                    'MANAGER_02_FULL_NAME', 'MANAGER_02_EMAIL_ID', 'COMP_CODE', 'CITY', 'Location', 'Country', 'Employee Status', 'Work Shift']\n",
    "# 1Ô∏è‚É£0Ô∏è‚É£[GLB]TerminationüóÉÔ∏è\n",
    "Termination_TABLE_NAME = \"GLB.Termination\"\n",
    "Folder_Termination = user_credential / r'DataBase//DataRaw//GLOBAL//WDD//'\n",
    "log_Termination_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//Termination_log.xlsx'\n",
    "Termination_schema = ['FileName', 'ModifiedDate', 'EMPLOYEE_ID', 'PREVIOUS_PAYROLL_ID', 'FIRST_NAME', 'MIDDLE_NAME', 'LAST_NAME', 'FULL_NAME', \n",
    "                      'EMAIL_ADDRESS', 'HIRE_DATE', 'ORIGINAL_HIRE_DATE', 'END EMPLOYMENT DATE', 'Contract End Date', 'Termination Date', \n",
    "                      'Termination Date (DD/MM/YY)', 'Eligible for Rehire', 'LWD', 'MOST RECENT TERMINATION - DATE INITIATED', \n",
    "                      'MOST RECENT TERMINATION - DATE COMPLETED', 'MOST RECENT TERMINATION - EFFECTIVE DATE', 'MOST RECENT TERMINATION - REASON', \n",
    "                      'Action date', 'DATE INITIATED', 'COMPELETED DATE AND TIME', 'TERMINATION DATE 2', 'Is Initiated through Resignation', \n",
    "                      'Termination Reason', 'Resignation Reason', 'Secondary Termination Reasons', 'Resignation Notice served', 'PERSON_TYPE', \n",
    "                      'Time Type', 'Employee Type', 'Worker Type', 'Assignment Category', 'WORKER_CATEGORY', 'BUSINESS_UNIT', 'Cost Center', \n",
    "                      'Cost Center - ID', 'JOB_CODE', 'JOB_TITLE', 'BUSINESS_TITLE', 'LOCATION_NAME', 'LOCATION_CODE', 'COUNTRY', 'COMPANY', \n",
    "                      'MANAGEMENT LEVEL', 'JOB LEVEL', 'JOB_FAMILY', 'JOB_FUNCTION', 'JOB_ROLE', 'MSA', 'CNX BU', 'Concentrix LOB', 'Process', \n",
    "                      'Client Name ( Process )', 'Compensation Grade', 'SUPERVISOR_ID', 'SUPERVISOR_FULL_NAME', 'SUPERVISOR_EMAIL_ID', 'COMP_CODE', \n",
    "                      'CITY', 'LOCATION_DESCRIPTION', 'EMPLOYEE STATUS', 'Continuous Service Date', 'Work Related Status', \n",
    "                      'Work Related (Extended Status)', 'Activity', 'MSA Legacy Project ID']\n",
    "# 1Ô∏è‚É£1Ô∏è‚É£[GLB]ResignationüóÉÔ∏è\n",
    "Resignation_TABLE_NAME = \"GLB.Resignation\"\n",
    "Folder_Resignation = user_credential / r'DataBase//DataRaw//GLOBAL//WDD//'\n",
    "log_Resignation_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//Resignation_log.xlsx'\n",
    "Resignation_schema = ['FileName', 'ModifiedDate', 'Employee ID', 'Full Name', 'Job Family', 'MSA Client', 'Country', 'Location', 'Action', \n",
    "                      'Action Date', 'Date and Time Initiated', 'Status', 'Primary Reason', 'Secondary Reasons', 'Notification Date', 'Awaiting Persons', \n",
    "                      'Resignation Primary Reason', 'Hire Date', 'Proposed Termination Date', 'Notice Served', 'Sup ID', 'Supervisor Name', \n",
    "                      'Employee Status', 'Activity', 'MSA Legacy Project ID', 'Initiated By']\n",
    "# 1Ô∏è‚É£2Ô∏è‚É£[BKN]CPI_PEGAüóÉÔ∏è\n",
    "CPI_PEGA_TABLE_NAME = \"BCOM.CPI_PEGA\"\n",
    "Folder_CPI_PEGA = user_credential / r'DataBase//DataRaw//BKN//CPI_PEGA//'\n",
    "log_CPI_PEGA_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//CPI_PEGA_log.xlsx'\n",
    "CPI_PEGA_schema = ['FileName', 'ModifiedDate', 'Staff Name', 'Operator Def', 'Service Case Type New', 'Channel Def',\t\n",
    "                   'Lang Def', 'Reason For No Service Case', 'Topic Def New', 'Subtopics', 'Case Id', 'Reservation Id Def',\n",
    "                   'Day of Date', 'Blank', '# Swivels', 'Count of ServiceCase or Interaction']\n",
    "# 1Ô∏è‚É£3Ô∏è‚É£[BKN]StaffüóÉÔ∏è\n",
    "Staff_TABLE_NAME = \"BCOM.Staff\"\n",
    "Folder_Staff = user_credential / r'DataBase//DataRaw//BKN//AGENTS//'\n",
    "log_Staff_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//Staff_log.xlsx'\n",
    "Staff_schema = ['FileName', 'ModifiedDate', 'Employee_ID', 'GEO', 'Site_ID', 'Employee_Last_Name', 'Employee_First_Name', 'Status', 'Wave #', \n",
    "                'Role', 'Booking Login ID', 'Language Start Date', 'TED Name', 'CUIC Name', 'EnterpriseName', 'Hire_Date', 'PST_Start_Date',\n",
    "                'Production_Start_Date', 'LWD', 'Termination_Date', 'Designation', 'cnx_email', 'Booking Email', 'WAH Category', 'Full name',\n",
    "                'IEX', 'serial_number', 'BKN_ID', 'Extension Number']\n",
    "# 1Ô∏è‚É£4Ô∏è‚É£[BKN]ConTracküóÉÔ∏è\n",
    "ConTrack_TABLE_NAME = \"BCOM.ConTrack\"\n",
    "Folder_ConTrack = Path(os.environ['USERPROFILE']) / r'OneDrive - Concentrix Corporation//DataBase//DataRaw//BKN//ContactTracker//'\n",
    "log_ConTrack_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//ConTrack_log.xlsx'\n",
    "ConTrack_schema = ['FileName', 'ModifiedDate', 'Id', 'Start time', 'Completion time', 'Email', 'Name', 'Reservation Number', 'Contact Types',\n",
    "                   'Contact Parties', 'Unbabel Tool Used?', 'Backlog Case', 'How many days since guest contacted? (ex: 30)', 'Topics',\n",
    "                   'Resolutions', 'Reason If Skipped', 'CRM used', 'Outbound to Senior', 'Outbound Status','Reason (Name - Site of Senior)',\n",
    "                   'Note', 'Reason for cannot make OB call to Guest', 'Is it possible to make Outbound call to Guest?¬†', 'Language']\n",
    "# 1Ô∏è‚É£5Ô∏è‚É£[BKN]QualityüóÉÔ∏è\n",
    "Quality_TABLE_NAME = \"BCOM.Quality\"\n",
    "Folder_Quality = user_credential / r'DataBase//DataRaw//BKN//QUALITY//'\n",
    "log_Quality_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//Quality_log.xlsx'\n",
    "Quality_schema = ['FileName', 'ModifiedDate', 'eps_name', 'eval_id', 'eval_date', 'agent_username', 'evaluator_username', 'result',\t\n",
    "                  'final_question_grouping', 'template_group', 'eval_template_name', 'sections', 'sitecode', 'score_n', 'score_question_weight',\n",
    "                  'eval_language', 'eval_reference', 'tix_final_topic', 'tix_final_subtopic', 'csat_language_code', 'csat_satisfied']\n",
    "# 1Ô∏è‚É£6Ô∏è‚É£[BKN]RONAüóÉÔ∏è\n",
    "RONA_TABLE_NAME = \"BCOM.RONA\"\n",
    "Folder_RONA = user_credential / r'DataBase//DataRaw//BKN//RONA//'\n",
    "log_RONA_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//RONA_log.xlsx'\n",
    "RONA_schema = ['FileName', 'ModifiedDate', 'Agent', 'DateTime', 'RONA']\n",
    "# 1Ô∏è‚É£7Ô∏è‚É£[BKN]CUICüóÉÔ∏è\n",
    "CUIC_TABLE_NAME = \"BCOM.CUIC\"\n",
    "Folder_CUIC = user_credential / r'DataBase//DataRaw//BKN//CUIC//'\n",
    "log_CUIC_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//CUIC_log.xlsx'\n",
    "CUIC_schema = ['FileName', 'ModifiedDate', 'FullName', 'LoginName', 'Interval', 'AgentAvailTime', 'AgentLoggedOnTime']\n",
    "# 1Ô∏è‚É£8Ô∏è‚É£[BKN]KPI_TargetüóÉÔ∏è\n",
    "KPI_Target_TABLE_NAME = \"BCOM.KPI_Target\"\n",
    "Folder_KPI_Target = user_credential / r'DataBase//DataRaw//BKN//KPI_TARGET//'\n",
    "log_KPI_Target_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//KPI_Target_log.xlsx'\n",
    "KPI_Target_schema = ['FileName', 'ModifiedDate', 'LOB', 'LOB Group', 'Week', 'Tenure days', 'Overall CPH tar', 'Phone CPH tar', 'Non Phone CPH tar',\t\n",
    "                     'Quality - Customer Impact tar', 'Quality - Business Impact tar', 'Quality - Compliance Impact tar', 'Quality - Overall tar', 'AHT Phone tar',\t\n",
    "                     'AHT Non-phone tar', 'AHT Overall tar', 'Hold (phone) tar', 'AACW (phone) tar', 'Avg Talk Time tar', 'Phone CSAT tar', 'Non phone CSAT tar',\t\n",
    "                     'Overall CSAT tar', 'PSAT tar', 'PSAT Vietnamese tar', 'PSAT English (American) tar', 'PSAT English (Great Britain) tar', 'CSAT Reso tar',\n",
    "                     'Quality - personalization tar', 'Quality - proactivity tar', 'Quality - resolution tar']\n",
    "# 1Ô∏è‚É£9Ô∏è‚É£[BKN]LogoutCountüóÉÔ∏è\n",
    "LogoutCount_TABLE_NAME = \"BCOM.LogoutCount\"\n",
    "Folder_LogoutCount = user_credential / r'DataBase//DataRaw//BKN//LOGOUT_COUNT//'\n",
    "log_LogoutCount_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//LogoutCount_log.xlsx'\n",
    "LogoutCount_schema = ['FileName', 'ModifiedDate', 'Aggregation', 'TimeDimension', 'KPI Value Formatted']\n",
    "# 2Ô∏è‚É£0Ô∏è‚É£[BKN]WpDetailüóÉÔ∏è\n",
    "WpDetail_TABLE_NAME = \"BCOM.WpDetail\"\n",
    "Folder_WpDetail = user_credential / r'DataBase//DataRaw//BKN//WP_DETAIL//'\n",
    "log_WpDetail_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//WpDetail_log.xlsx'\n",
    "WpDetail_schema = ['FileName', 'ModifiedDate', 'LOB', 'ID', 'DateTime_Start', 'DateTime_End', 'Date_Start', 'Date_end', 'Time_Start', 'Time_End', \n",
    "                   'Dur', 'Action', 'DateTime_Act_Start', 'DateTime_Act_End', 'Date_Act_Start', 'Date_Act_End', 'Time_Act_Start', 'Time_Act_End', 'Act_Dur']\n",
    "# 2Ô∏è‚É£1Ô∏è‚É£[BKN]WpSummaryüóÉÔ∏è\n",
    "WpSummary_TABLE_NAME = \"BCOM.WpSummary\"\n",
    "Folder_WpSummary = user_credential / r'DataBase//DataRaw//BKN//WP_SUMMARY//'\n",
    "log_WpSummary_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//WpSummary_log.xlsx'\n",
    "WpSummary_schema = ['FileName', 'ModifiedDate', 'LOB', 'Date', 'Agent ID', 'Agent Name', 'Scheduled Activity', 'Length', 'Percent']\n",
    "# 2Ô∏è‚É£2Ô∏è‚É£[BKN]RegisteredOTüóÉÔ∏è\n",
    "RegisteredOT_TABLE_NAME = \"BCOM.RegisteredOT\"\n",
    "Folder_RegisteredOT = user_credential / r'DataBase//DataRaw//BKN//OVERTIME//'\n",
    "log_RegisteredOT_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//RegisteredOT_log.xlsx'\n",
    "RegisteredOT_schema = ['FileName', 'ModifiedDate', 'Emp ID', 'Name', 'Date', 'Value', 'OT', 'LOB','Type']\n",
    "# 2Ô∏è‚É£3Ô∏è‚É£[BKN]CSAT_TPüóÉÔ∏è\n",
    "CSAT_TP_TABLE_NAME = \"BCOM.CSAT_TP\"\n",
    "Folder_CSAT_TP = user_credential / r'DataBase//DataRaw//BKN//CSAT//'\n",
    "log_CSAT_TP_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//CSAT_TP_log.xlsx'\n",
    "CSAT_TP_schema = ['FileName', 'ModifiedDate', 'Sort by Dimension', 'Survey Id', 'Reservation', 'Team', 'Channel', 'Staff', 'Type', 'Date',\n",
    "                  'Topic of the first Ticket', 'Language', 'Csat 2.0 Score', 'Has Comment', '\"Comment\"', 'Reservation Link', 'View comment',\n",
    "                  'Sort by Dimension (copy)', 'Max. Sort by Dimension']\n",
    "# 2Ô∏è‚É£4Ô∏è‚É£[BKN]CSAT_RSüóÉÔ∏è\n",
    "CSAT_RS_TABLE_NAME = \"BCOM.CSAT_RS\"\n",
    "Folder_CSAT_RS = user_credential / r'DataBase//DataRaw//BKN//CSAT_RESO//'\n",
    "log_CSAT_RS_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//CSAT_RS_log.xlsx'\n",
    "CSAT_RS_schema = ['FileName', 'ModifiedDate', 'Sort by Dimension', 'Survey Id', 'Reservation', 'Team', 'Channel', 'Staff', 'Type', 'Date',\n",
    "                  'Topic of the first Ticket', 'Language', 'Csat 2.0 Score', 'Has Comment', '\"Comment\"', 'Reservation Link', 'View comment',\n",
    "                  'Sort by Dimension (copy)', 'Max. Sort by Dimension']\n",
    "# 2Ô∏è‚É£5Ô∏è‚É£[BKN]PSATüóÉÔ∏è\n",
    "PSAT_TABLE_NAME = \"BCOM.PSAT\"\n",
    "Folder_PSAT = user_credential / r'DataBase//DataRaw//BKN//PSAT//'\n",
    "log_PSAT_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//PSAT_log.xlsx'\n",
    "PSAT_schema = ['FileName', 'ModifiedDate', 'Sorted By Dimension', 'Survey Id', 'Date', 'Staff Name', 'Language', 'Final Topics',\n",
    "               'How satisfied were you with our service?', 'How difficult did we make it or you to solve your issue?', 'Agent understood my question',\n",
    "               'Agent did everything possible to help me', 'Did we fully resolve your issue?', 'Channel', 'Hotel Id', '\"Comment\"',\n",
    "               'Has Comment', 'Sorted BY Dimension (copy)']\n",
    "# 2Ô∏è‚É£6Ô∏è‚É£[BKN]IEX_HrsüóÉÔ∏è\n",
    "IEX_Hrs_TABLE_NAME = \"BCOM.IEX_Hrs\"\n",
    "Folder_IEX_Hrs = user_credential / r'DataBase//DataRaw//BKN//WP_INTERVAL//'\n",
    "log_IEX_Hrs_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//IEX_Hrs_log.xlsx'\n",
    "IEX_Hrs_schema = ['FileName', 'ModifiedDate', 'LOB', 'VNT', 'CET', 'HC', 'Hour']\n",
    "# 2Ô∏è‚É£7Ô∏è‚É£[BKN]IntervalReqüóÉÔ∏è\n",
    "IntervalReq_TABLE_NAME = \"BCOM.IntervalReq\"\n",
    "Folder_IntervalReq = user_credential / r'DataBase//DataRaw//BKN//INTERVAL_REQUIREMENT//'\n",
    "log_IntervalReq_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//IntervalReq_log.xlsx'\n",
    "IntervalReq_schema = ['FileName', 'ModifiedDate', 'LOB', 'Datetime_CET', 'Datetime_VN', 'Value', 'Delivery_Req']\n",
    "# 2Ô∏è‚É£8Ô∏è‚É£[BKN]ExceptionReqüóÉÔ∏è\n",
    "ExceptionReq_TABLE_NAME = \"BCOM.ExceptionReq\"\n",
    "Folder_ExceptionReq = user_credential / r'DataBase//DataRaw//BKN//EXCEPTION_REQ//'\n",
    "log_ExceptionReq_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//ExceptionReq_log.xlsx'\n",
    "ExceptionReq_schema = ['FileName', 'ModifiedDate', 'Emp ID', 'Date (MM/DD/YYYY)', 'Exception request (Minute)', 'Reason', 'TL', 'OM']\n",
    "# 2Ô∏è‚É£9Ô∏è‚É£[BKN]LTTransfersüóÉÔ∏è\n",
    "LTTransfers_TABLE_NAME = \"BCOM.LTTransfers\"\n",
    "Folder_LTTransfers = user_credential / r'DataBase//DataRaw//BKN//HC_TRANSFER//'\n",
    "log_LTTransfers_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//LTTransfers_log.xlsx'\n",
    "LTTransfers_schema = ['FileName', 'ModifiedDate', 'EID', 'Full Name', 'Employee Status', 'LWD', 'Remarks']\n",
    "# 3Ô∏è‚É£0Ô∏è‚É£[BKN]DailyReqüóÉÔ∏è\n",
    "DailyReq_TABLE_NAME = \"BCOM.DailyReq\"\n",
    "Folder_DailyReq = user_credential / r'DataBase//DataRaw//BKN//REQUIREMENT_HOURS//'\n",
    "log_DailyReq_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//DailyReq_log.xlsx'\n",
    "DailyReq_schema = ['FileName', 'ModifiedDate', 'LOB', 'Date', 'Daily Requirement', 'Prod Requirement']\n",
    "# 3Ô∏è‚É£1Ô∏è‚É£[BKN]ProjectedShrinküóÉÔ∏è\n",
    "ProjectedShrink_TABLE_NAME = \"BCOM.ProjectedShrink\"\n",
    "Folder_ProjectedShrink = user_credential / r'DataBase//DataRaw//BKN//SHRINKAGE_TARGET//'\n",
    "log_ProjectedShrink_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//ProjectedShrink_log.xlsx'\n",
    "ProjectedShrink_schema = ['FileName', 'ModifiedDate', 'LOB', 'Week', 'Ratio']\n",
    "# 3Ô∏è‚É£2Ô∏è‚É£[BKN]OTReqüóÉÔ∏è\n",
    "OTReq_TABLE_NAME = \"BCOM.OTReq\"\n",
    "Folder_OTReq = user_credential / r'DataBase//DataRaw//BKN//OT_REQ//'\n",
    "log_OTReq_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//OTReq_log.xlsx'\n",
    "OTReq_schema = ['FileName', 'ModifiedDate', 'Date', 'LOB', 'OT Hour', 'Type']\n",
    "# 3Ô∏è‚É£3Ô∏è‚É£[BKN]CapHCüóÉÔ∏è\n",
    "CapHC_TABLE_NAME = \"BCOM.CapHC\"\n",
    "Folder_CapHC = user_credential / r'DataBase//DataRaw//BKN//CAPACITY_HC//'\n",
    "log_CapHC_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//CapHC_log.xlsx'\n",
    "CapHC_schema = ['FileName', 'ModifiedDate', 'LOB', 'Date', 'Client Requirement (Hours)']\n",
    "# 3Ô∏è‚É£4Ô∏è‚É£[BKN]ProjectedHCüóÉÔ∏è\n",
    "ProjectedHC_TABLE_NAME = \"BCOM.ProjectedHC\"\n",
    "Folder_ProjectedHC = user_credential / r'DataBase//DataRaw//BKN//PROJECTED_HEADCOUNT//'\n",
    "log_ProjectedHC_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//ProjectedHC_log.xlsx'\n",
    "ProjectedHC_schema = ['FileName', 'ModifiedDate', 'Date', 'LOB', 'FTE Required', 'Projected HC', 'Plan Leave', \n",
    "                      'Actual Projected HC', '%OO', '%IO', 'Projected HC with Shrink', 'OT', 'Leave allow for Shrink', '% Deli']\n",
    "# 3Ô∏è‚É£5Ô∏è‚É£[BKN]RampHCüóÉÔ∏è\n",
    "RampHC_TABLE_NAME = \"BCOM.RampHC\"\n",
    "Folder_RampHC = user_credential / r'DataBase//DataRaw//BKN//RAMPUP_HC//'\n",
    "log_RampHC_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//RampHC_log.xlsx'\n",
    "RampHC_schema = ['FileName', 'ModifiedDate', 'Date', 'LOB', 'Headcount', 'Hours']\n",
    "\n",
    "# 3Ô∏è‚É£6Ô∏è‚É£[BKN]SEATüóÉÔ∏è\n",
    "SEAT_TABLE_NAME = \"BCOM.SEAT\"\n",
    "Folder_SEAT = user_credential / r'DataBase//DataFrame//BKN//SEAT_MAP//History data//'\n",
    "Folder_SEAT2 = user_credential / r'DataBase//DataFrame//BKN//SEAT_MAP//SEAT_MAP_DATA//'\n",
    "log_SEAT_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//SEAT_log.xlsx'\n",
    "SEAT_schema = ['FileName','ModifiedDate','Date','Emp ID','TED Name','Week_day','Seat No','Floor','Building']\n",
    "\n",
    "# 3Ô∏è‚É£7Ô∏è‚É£[BKN]SEAT_AVAILüóÉÔ∏è\n",
    "SEAT_AVAIL_TABLE_NAME = \"BCOM.SEAT_Avail\"\n",
    "Folder_SEAT_AVAIL = user_credential / r'DataBase//DataFrame//BKN//SEAT_MAP//History data//'\n",
    "Folder_SEAT_AVAIL2 = user_credential / r'DataBase//DataFrame//BKN//SEAT_MAP//SEAT_MAP_DATA//'\n",
    "log_SEAT_AVAIL_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//SEAT_AVAIL_log.xlsx'\n",
    "SEAT_AVAIL_schema = ['FileName','ModifiedDate','Seat No','Floor','Building','Code','Date','Open Hours','Closed Hours']\n",
    "\n",
    "# 3Ô∏è‚É£8Ô∏è‚É£[BKN]CSAT_CompüóÉÔ∏è\n",
    "CSAT_Comp_TABLE_NAME = \"BCOM.CSAT_Comp\"\n",
    "Folder_CSAT_Comp = user_credential / r'DataBase//DataRaw//BKN//CSAT_Comp//'\n",
    "log_CSAT_Comp_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//CSAT_Comp_log.xlsx'\n",
    "CSAT_Comp_schema = ['FileName','ModifiedDate','Date','REGION','Site type','Site Name','Company','Language',\n",
    "                    'Overall Surveys','Overall CSAT','Messaging surveys','Messaging CSAT','Email surveys',\n",
    "                    'Email CSAT','Phone surveys','Phone CSAT']\n",
    "# -----------------------------------------------------------------------------------------------#\n",
    "# Database_Connecterüß¨\n",
    "summary_results = {\n",
    "    \"new_files_imported\": [],\n",
    "    \"updated_files_reimported\": [],\n",
    "    \"skipped_files_unchanged\": [],\n",
    "    \"failed_imports\": []\n",
    "}\n",
    "server_name = \"PHMANVMDEV01V\"\n",
    "server_ip = \"10.5.11.60\"\n",
    "database = \"wfm_vn_dev\"\n",
    "user = \"usr_wfmvn_dev\"\n",
    "password = \"12guWU2OdEj5kEspl9Rlfoglf\"\n",
    "# SQL Server Authentication üîó\n",
    "connection_string = f\"mssql+pyodbc://{user}:{password}@{server_ip}/{database}?driver=ODBC+Driver+17+for+SQL+Server\"\n",
    "# Windows Authentication üîó\n",
    "# connection_string = f\"mssql+pyodbc://{server_name}/{database}?driver=ODBC+Driver+17+for+SQL+Server&Trusted_Connection=yes\"\n",
    "try:\n",
    "    engine = create_engine(connection_string, fast_executemany=True)\n",
    "    logger.info(f\"‚úÖ Successfully connected to DB: {database} server: {server_ip}\")\n",
    "except Exception as e:\n",
    "    logger.exception(\"‚ùå DB Connection error\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52637df5-a0d3-4131-9c4b-ab715459f181",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function Definitionüõ†Ô∏è\n",
    "\n",
    "# Log Color viewüí°\n",
    "def print_colored(text, color):\n",
    "    display(HTML(f'<span style=\"color: {color};\">{text}</span>'))\n",
    "\n",
    "# Check existing log fileüí°\n",
    "def read_or_create_log(log_path):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore') # Ignor WarningüìÉ\n",
    "        try:\n",
    "            logger.debug(f\"Reading log file: {log_path}\")\n",
    "            log_df = pl.read_excel(log_path)\n",
    "            log_df = log_df.with_columns([pl.col(\"ModifiedDate\").dt.cast_time_unit(\"ms\")], strict=False)\n",
    "            logger.info(f\"Success read log file: {log_path}\")\n",
    "        except FileNotFoundError: # Create new log if can't find logüìÉ\n",
    "            logger.warning(f\"Log file not found: {log_path}. Create new log.\")\n",
    "            log_df = pl.DataFrame(\n",
    "                {\n",
    "                    \"FileName\": pl.Series([], dtype=pl.Utf8),\n",
    "                    \"ModifiedDate\": pl.Series([], dtype=pl.Datetime),\n",
    "                    \"Error\": pl.Series([], dtype=pl.Utf8),})\n",
    "        except Exception as e: # Create new log if can't open logüìÉ\n",
    "            logger.exception(f\"Error reading log file: {log_path}\")\n",
    "            print(f\"Error reading log file: {e}\")\n",
    "            log_df = pl.DataFrame(\n",
    "                {\n",
    "                    \"FileName\": pl.Series([], dtype=pl.Utf8),\n",
    "                    \"ModifiedDate\": pl.Series([], dtype=pl.Datetime),\n",
    "                    \"Error\": pl.Series([], dtype=pl.Utf8),})\n",
    "        return log_df\n",
    "        \n",
    "# Update log_dfüí°\n",
    "def process_and_save_log(log_df, log_entries, log_path):\n",
    "    if log_entries:\n",
    "        new_log_df = pl.DataFrame(log_entries)\n",
    "        log_df = log_df.with_columns(pl.col('ModifiedDate').dt.cast_time_unit(\"ms\"))\n",
    "        log_df = (pl.concat([log_df, new_log_df], how=\"diagonal_relaxed\") # Combine and remove duplicate New_Log and Old_LogüìÉ\n",
    "                  .sort(\"ModifiedDate\", descending=[False])\n",
    "                  .unique(subset=[\"FileName\"], keep=\"last\")\n",
    "                  .sort(\"FileName\", descending=[False])\n",
    "                  .select([\"FileName\", \"ModifiedDate\", \"Error\"]))\n",
    "        try:\n",
    "            log_df.write_excel(log_path, worksheet=\"ImportLog\", autofit=True)\n",
    "            print(f\"Import log saved to: {log_path}\")\n",
    "            logger.info(f\"Import log saved to: {log_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error writing log file: {e}\")\n",
    "            logger.error(f\"Error writing log file: {log_path} - {e}\")\n",
    "\n",
    "# write_dataüí°\n",
    "def write_data(engine, table_name, df): # write to databaseüìÉ\n",
    "     df.write_database(table_name=table_name, connection=engine, if_table_exists=\"append\")\n",
    "    \n",
    "# delete_dataüí°\n",
    "def delete_data(engine, table_name, filename):\n",
    "    try:\n",
    "        with engine.connect() as connection:\n",
    "            print_colored(f\"Prepare to delete old data for '{filename}' in '{table_name}'\", \"DarkTurquoise\")\n",
    "            logger.warning(f\"Prepare to delete old data for '{filename}' in '{table_name}'\")\n",
    "            delete_query = text(f\"DELETE FROM {table_name} WHERE [FileName] = :filename\")\n",
    "            connection.execute(delete_query, {\"filename\": filename})\n",
    "            connection.commit()\n",
    "            print_colored(f\"Old data deleted successfullyüßπ\", \"DarkTurquoise\")\n",
    "            logger.info(f\"'{filename}' data deleted successfully in '{table_name}' üßπ.\")\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Error while delete data for '{filename}' in '{table_name}'\")\n",
    "        print_colored(f\"Error while delete data for '{filename}' in '{table_name}'\", \"DarkTurquoise\")\n",
    "        raise \n",
    "        \n",
    "# Check Timeüí°\n",
    "def is_time_between(begin_time, end_time, check_time=None):\n",
    "    check_time = check_time or datetime.utcnow().time() # If check time is not given, default to current UTC timeüìÉ\n",
    "    if begin_time < end_time:\n",
    "        return check_time >= begin_time and check_time <= end_time\n",
    "    else: # crosses midnightüìÉ\n",
    "        return check_time >= begin_time or check_time <= end_time\n",
    "def time_difference(time1, time2):\n",
    "    seconds1 = time1.hour * 3600 + time1.minute * 60 + time1.second # Convert times to secondsüìÉ\n",
    "    seconds2 = time2.hour * 3600 + time2.minute * 60 + time2.second\n",
    "    diff_seconds = seconds1 - seconds2\n",
    "    return diff_seconds\n",
    "\n",
    "# Final Summaryüí°\n",
    "def display_summary(source_name: str, error_count: int) -> None:\n",
    "    \"\"\"Final Notice.\"\"\"\n",
    "    if error_count > 0:\n",
    "        print_colored(f\"Finished processing all files ({error_count} have errorsüõ†Ô∏è).\", \"OrangeRed\")\n",
    "        logger.warning(f\"Finished processing all files ({error_count} have errorsüõ†Ô∏è).\")\n",
    "    else:\n",
    "        print_colored(f\"Finished processing all files (no errorsüéâ).\", \"PaleVioletRed\")\n",
    "        logger.info(f\"Finished processing [{source_name}] (no errorsüéâ).\")\n",
    "\n",
    "# Default_variableüí°\n",
    "def Default_variable():\n",
    "    log_entries = []\n",
    "    error_count = 0\n",
    "    return log_entries, error_count\n",
    "\n",
    "# parse_dateüí°\n",
    "def parse_date(col: pl.Expr) -> pl.Expr:\n",
    "    return pl.coalesce(\n",
    "        col.str.strptime(pl.Date, format=\"%m/%d/%Y\", strict=False),\n",
    "        col.str.strptime(pl.Date, format=\"%Y-%m-%d\", strict=False),\n",
    "        col.str.strptime(pl.Date, format=\"%d %B %Y\", strict=False),\n",
    "        col.str.strptime(pl.Date, format=\"%B %d, %Y\", strict=False),\n",
    "        col.str.strptime(pl.Date, format=\"%d-%b-%y\", strict=False),\n",
    "        col.str.strptime(pl.Date, format=\"%Y%m%d\", strict=False),\n",
    "        col.str.strptime(pl.Date, format=\"%d/%m/%y\", strict=False),\n",
    "        col.str.strptime(pl.Date, format=\"%d-%m-%Y\", strict=False),\n",
    "    )\n",
    "\n",
    "# validate_schemaüí°\n",
    "def validate_schema(df: pl.DataFrame, expected_schema: list[str], filename: str) -> tuple[bool, str | None]:\n",
    "    # Start validation\n",
    "    start_msg = f\"üîç Starting schema validation for file: {filename}\"\n",
    "    logger.info(start_msg)\n",
    "    print_colored(start_msg, \"DodgerBlue\")\n",
    "    actual_columns = df.columns\n",
    "    expected_set = set(expected_schema)\n",
    "    actual_set = set(actual_columns)\n",
    "    missing_columns = expected_set - actual_set\n",
    "    extra_columns = actual_set - expected_set\n",
    "    has_critical_error = False\n",
    "    critical_error_message = None\n",
    "    has_warnings = False\n",
    "    # 1. Schema error (Missing columns)\n",
    "    if missing_columns:\n",
    "        has_critical_error = True\n",
    "        critical_error_message = f\"Schema error in the file: '{filename}'. Missing columns: {sorted(list(missing_columns))}\"\n",
    "        logger.error(critical_error_message)\n",
    "        print_colored(f\"‚ùóÔ∏è {critical_error_message}\", \"OrangeRed\")\n",
    "    # 2. warning extra columns\n",
    "    if extra_columns:\n",
    "        has_warnings = True\n",
    "        warning_message = f\"warning schema for file '{filename}'. Extra columns: {sorted(list(extra_columns))}. These columns will be excluded from the import process.\"\n",
    "        logger.warning(warning_message)\n",
    "        print_colored(f\"‚ö†Ô∏è {warning_message}\", \"Gold\")\n",
    "    # 3. Final results announcement\n",
    "    if not has_critical_error and not has_warnings:\n",
    "        final_msg = f\"‚úÖ Completely valid schema for the file: {filename}.\"\n",
    "        logger.info(final_msg)\n",
    "        print_colored(final_msg, \"MediumSeaGreen\")\n",
    "    elif not has_critical_error and has_warnings:\n",
    "        final_msg = f\"‚ö†Ô∏è File schema check: {filename} Passed (No missing columns, extra columns warned)\"\n",
    "        logger.info(final_msg)\n",
    "        print_colored(final_msg, \"MediumSeaGreen\") # V·∫´n d√πng m√†u xanh l√°\n",
    "    elif has_critical_error:\n",
    "        final_msg = f\"‚ùå Schema validation failed due to missing column(s) for file: {filename}.\"\n",
    "        logger.warning(final_msg) # Log ·ªü m·ª©c warning ho·∫∑c error t√πy √Ω\n",
    "        print_colored(final_msg, \"OrangeRed\")\n",
    "    return has_critical_error, critical_error_message\n",
    "    \n",
    "# DF Infoüí°\n",
    "def info_polars(df: pl.DataFrame):\n",
    "    print_colored(f\"‚öôÔ∏èFinal structure\", \"Olive\")\n",
    "    logger.info(f\"‚öôÔ∏èFinal structure\")\n",
    "    shape = df.shape\n",
    "    print(f\"Shape: {shape}\")\n",
    "    print(\"Data columns:\")  \n",
    "    table_data = []\n",
    "    for i, name in enumerate(df.columns):\n",
    "        dtype = df.dtypes[i]\n",
    "        non_null_count = df.select(pl.col(name).is_not_null().sum()).item()\n",
    "        table_data.append([i, name, non_null_count, dtype])  \n",
    "    headers = [\"#\", \"Column\", \"Non-Null Count\", \"Dtype\"]\n",
    "    print(tabulate(table_data, headers=headers, tablefmt=\"grid\"))\n",
    "    logger.info(tabulate(table_data, headers=headers, tablefmt=\"grid\"))\n",
    "\n",
    "# Summaryüí°\n",
    "def generate_summary_report(results):\n",
    "    report = []\n",
    "    total_processed = (len(results[\"new_files_imported\"]) + \n",
    "                       len(results[\"updated_files_reimported\"]) + \n",
    "                       len(results[\"failed_imports\"]))\n",
    "    \n",
    "    report.append(\"‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\")\n",
    "    report.append(\"‚ïë üìä Server Import Summary üìä ‚ïë\")\n",
    "    report.append(\"‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\")\n",
    "    report.append(f\"\\n‚ú® Processed a total of {total_processed} files.\")\n",
    "    \n",
    "    # 1. C√°c t·ªáp m·ªõi ƒë∆∞·ª£c nh·∫≠p\n",
    "    report.append(\"\\n--- ‚úÖ New file has been imported successfully ---\")\n",
    "    if results[\"new_files_imported\"]:\n",
    "        for file in results[\"new_files_imported\"]:\n",
    "            report.append(f\"  - {file}\")\n",
    "    else:\n",
    "        report.append(\"  (Nothing)\")\n",
    "\n",
    "    # 2. C√°c t·ªáp ƒë∆∞·ª£c c·∫≠p nh·∫≠t\n",
    "    report.append(\"\\n--- üîÑÔ∏è File Updated Successfully ---\")\n",
    "    if results[\"updated_files_reimported\"]:\n",
    "        for file in results[\"updated_files_reimported\"]:\n",
    "            report.append(f\"  - {file}\")\n",
    "    else:\n",
    "        report.append(\"  (Nothing)\")\n",
    "\n",
    "    # 3. C√°c t·ªáp x·ª≠ l√Ω l·ªói\n",
    "    report.append(\"\\n--- ‚ùóÔ∏èFailed Files ---\")\n",
    "    if results[\"failed_imports\"]:\n",
    "        for file, error in results[\"failed_imports\"]:\n",
    "            report.append(f\"  - {file}\")\n",
    "            report.append(f\"    ‚îî‚îÄ L·ªói: {error}\")\n",
    "    else:\n",
    "        report.append(\"  (All Good üéâ)\")\n",
    "\n",
    "    # 4. C√°c t·ªáp ƒë∆∞·ª£c b·ªè qua\n",
    "    report.append(\"\\n--- üöÄ skipped files (unchanged) ---\")\n",
    "    if results[\"skipped_files_unchanged\"]:\n",
    "         report.append(f\"  (T·ªïng c·ªông: {len(results['skipped_files_unchanged'])} t·ªáp)\")\n",
    "        # report.append(\"  \" + \", \".join(results[\"skipped_files_unchanged\"])) # B·ªè comment d√≤ng n√†y n·∫øu b·∫°n mu·ªën li·ªát k√™ t·∫•t c·∫£\n",
    "    else:\n",
    "        report.append(\"  (Nothing)\")\n",
    "        \n",
    "    report.append(\"\\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê Finish ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\")\n",
    "    return \"\\n\".join(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95153293-c891-489f-9553-117b58dabf17",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 0Ô∏è‚É£1Ô∏è‚É£[BKN]AHT2üíæ\n",
    "logger.info(\"===== Start AHT2 Process =====\") #üß© Log the start of the data processing.\n",
    "log_df = read_or_create_log(log_AHT2_path) #üß© Read the existing Excel log file or create a new one if it doesn't exist.\n",
    "log_entries, error_count = Default_variable() #üìÉ Initialize a list to store log results for the current run and an error counter.\n",
    "#üí° Iterate through all files in the specified folder.\n",
    "for filename in Folder_AHT2.glob(\"*.csv\"): #üß© Get files in the Folder directory.\n",
    "    #üìÉ Get the current modification timestamp and filename of the current file.\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Get modification timestamp, including microseconds.\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0) # Remove microseconds for easier comparison.\n",
    "    file_basename = filename.name # Get the base name of the file (e.g., \"data_20230101.csv\").\n",
    "    #üìÉ File is being processed.\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    #üìÉ Check if this file has been logged before.\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Filter the old log_df to find an entry for this filename.\n",
    "    is_new_file = log_entry.is_empty() # If no entry is found (log_entry is empty), it's a new file.\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1)  # Initialize the previous modification date with a very old date.\n",
    "    #üìÉ If this file was processed before (exists in the log).  \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Get the modification date recorded in the log from the previous processing.\n",
    "    #üìÉ Condition to process the file:\n",
    "    # 1. The file is new (not in the log).\n",
    "    # 2. Or, the file is old (in the log), but its current modification date (current_modified_date) is more recent than the modification date recorded in the log (previous_modified_date).\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # If it's an old file but has changed (ModifiedDate is newer).\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # If it's a completely new file.\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try: # Start a try-except block to handle potential errors during the import process.\n",
    "            if not is_new_file: # If this file already exists in the DB (because it's not new and its ModifiedDate changed) then delete the old data for this file from the df.\n",
    "                delete_data(engine, AHT2_TABLE_NAME, file_basename)  #üß© Call the function to delete data.\n",
    "            #üìÉ Read data from the excel file using Polars.\n",
    "            AHT2 = (pl.read_csv(filename, infer_schema_length=None) #üß© Read the entire file to infer the schema.Add two new columns to the DataFrame: FileName and ModifiedDate.\n",
    "                   .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   )) #üß©        \n",
    "            #üìÉ Validate if the columns in the DataFrame match the defined schema\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(AHT2, AHT2_schema, file_basename) #üß©\n",
    "            if has_critical_error: # If critical columns are missing.\n",
    "                raise ValueError(critical_schema_error_msg) # Raise an error to stop processing this file.\n",
    "            #üìÉ Adjust the column structure.\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            AHT2 = AHT2.with_columns(parse_date(pl.col(\"Date\")).alias(\"Date\"),).select(AHT2_schema) #üß© Select columns in the correct order and names as per df_schema.\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(AHT2) #üß© Display information about the DataFrame (shape, columns, dtypes).\n",
    "            #üìÉ Start writing data to the database.\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, AHT2_TABLE_NAME, AHT2) #üß© Call the function to write the DataFrame to SQL.\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            #üìÉ Record to summary\n",
    "            if is_new_file:\n",
    "                summary_results[\"new_files_imported\"].append(f\"[AHT2] {file_basename}\")#üß©\n",
    "            else:\n",
    "                summary_results[\"updated_files_reimported\"].append(f\"[AHT2] {file_basename}\")#üß©    \n",
    "            #üìÉ Update log_entries for successfully import: Add information to the log list for this run (successful).\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "        #üìÉ Handle exceptions if any error occurred in the try block.\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception # Catch various types of potential errors.\n",
    "        ) as e:\n",
    "            error_count += 1 # Increment the error counter.        \n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"  # Get a short error message (the first line).   \n",
    "            #üìÉ Update Failed file to Summary\n",
    "            summary_results[\"failed_imports\"].append((f\"[AHT2] {file_basename}\", error_msg_short))#üß©   \n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True) # Log the full error traceback.\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            # Add information to the log list for this run (failed).\n",
    "            # Note: ModifiedDate here is previous_modified_date, as this file wasn't successfully imported with current_modified_date.\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: #üìÉ If the file is not new and its modification date hasn't changed.\n",
    "          #üìÉ Update Skip file to Summary\n",
    "        summary_results[\"skipped_files_unchanged\"].append(f\"[AHT2] {file_basename}\")#üß©\n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "#üí° After iterating through all files in the directory.\n",
    "display_summary(\"AHT2\", error_count) #üß© Display a summary of the results (number of errors).\n",
    "process_and_save_log(log_df, log_entries, log_AHT2_path) #üß© Update the Excel log file with information\n",
    "logger.info(\"===== Processing of the AHT2 data source is complete =====\") #üß© Log finish data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ee02f0-2bdc-4e98-8893-cba402f08557",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 0Ô∏è‚É£2Ô∏è‚É£[BKN]ROSTERüíæ\n",
    "logger.info(\"===== Start ROSTER Process =====\")  #üß© Log the start of the data processing.\n",
    "log_df = read_or_create_log(log_ROSTER_path) #üß© Read the existing Excel log file or create a new one if it doesn't exist.\n",
    "log_entries, error_count = Default_variable() #üìÉ Initialize a list to store log results for the current run and an error counter.\n",
    "#üí° Iterate through all files in the specified folder.\n",
    "for filename in Folder_ROSTER.glob(\"*.xlsx\"): #üß© Get files in the Folder directory.\n",
    "    #üìÉ Get the current modification timestamp and filename of the current file.\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Get modification timestamp, including microseconds.\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0) # Remove microseconds for easier comparison.\n",
    "    file_basename = filename.name # Get the base name of the file (e.g., \"data_20230101.csv\").\n",
    "    #üìÉ File is being processed.\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    #üìÉ Check if this file has been logged before.\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Filter the old log_df to find an entry for this filename.\n",
    "    is_new_file = log_entry.is_empty() # If no entry is found (log_entry is empty), it's a new file.\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1)  # Initialize the previous modification date with a very old date.\n",
    "    #üìÉ If this file was processed before (exists in the log).  \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Get the modification date recorded in the log from the previous processing.\n",
    "    #üìÉ Condition to process the file:\n",
    "    # 1. The file is new (not in the log).\n",
    "    # 2. Or, the file is old (in the log), but its current modification date (current_modified_date) is more recent than the modification date recorded in the log (previous_modified_date).\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # If it's an old file but has changed (ModifiedDate is newer).\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # If it's a completely new file.\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try: # Start a try-except block to handle potential errors during the import process.\n",
    "            if not is_new_file: # If this file already exists in the DB (because it's not new and its ModifiedDate changed) then delete the old data for this file from the df.\n",
    "                delete_data(engine, ROSTER_TABLE_NAME, file_basename)  #üß© Call the function to delete data.\n",
    "            #üìÉ Read data from the excel file using Polars.\n",
    "            ROSTER = (pl.read_excel(filename, infer_schema_length=None, engine=\"calamine\", sheet_name=\"Sheet1\") #üß© Read the entire file to infer the schema.Add two new columns to the DataFrame: FileName and ModifiedDate.\n",
    "                    .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   )) #üß©\n",
    "            #üìÉ Validate if the columns in the DataFrame match the defined schema\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(ROSTER, ROSTER_schema, file_basename) #üß©\n",
    "            if has_critical_error: # If critical columns are missing.\n",
    "                raise ValueError(critical_schema_error_msg) # Raise an error to stop processing this file.\n",
    "            #üìÉ Adjust the column structure.\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            ROSTER = ROSTER.with_columns(pl.col(\"Attribute\").cast(pl.Date)).select(ROSTER_schema) #üß© Select columns in the correct order and names as per df_schema.\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(ROSTER) #üß© Display information about the DataFrame (shape, columns, dtypes).\n",
    "            #üìÉ Start writing data to the database.\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, ROSTER_TABLE_NAME, ROSTER) #üß© Call the function to write the DataFrame to SQL.\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            #üìÉ Record to summary\n",
    "            if is_new_file:\n",
    "                summary_results[\"new_files_imported\"].append(f\"[ROSTER] {file_basename}\")#üß©\n",
    "            else:\n",
    "                summary_results[\"updated_files_reimported\"].append(f\"[ROSTER] {file_basename}\")#üß©\n",
    "            #üìÉ Update log_entries for successfully import: Add information to the log list for this run (successful).\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None}) \n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            #üìÉ Handle exceptions if any error occurred in the try block.\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception # Catch various types of potential errors.\n",
    "        ) as e:\n",
    "            error_count += 1 # Increment the error counter.        \n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"  # Get a short error message (the first line).\n",
    "            #üìÉ Update Failed file to Summary\n",
    "            summary_results[\"failed_imports\"].append((f\"[ROSTER] {file_basename}\", error_msg_short))#üß©    \n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True) # Log the full error traceback.\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            # Add information to the log list for this run (failed).\n",
    "            # Note: ModifiedDate here is previous_modified_date, as this file wasn't successfully imported with current_modified_date.\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: #üìÉ If the file is not new and its modification date hasn't changed.\n",
    "          #üìÉ Update Skip file to Summary\n",
    "        summary_results[\"skipped_files_unchanged\"].append(f\"[ROSTER] {file_basename}\")#üß©       \n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "#üí° After iterating through all files in the directory.\n",
    "display_summary(\"ROSTER\", error_count) #üß© Display a summary of the results (number of errors).\n",
    "process_and_save_log(log_df, log_entries, log_ROSTER_path) #üß© Update the Excel log file with information\n",
    "logger.info(\"===== Processing of the ROSTER data source is complete =====\") #üß© Log finish data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f852dc5-8889-4844-b7e1-4bf3bfc6de1b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 0Ô∏è‚É£3Ô∏è‚É£[BKN]EPSüíæ\n",
    "logger.info(\"===== Start EPS Process =====\")  #üß© Log the start of the data processing.\n",
    "log_df = read_or_create_log(log_EPS_path) #üß© Read the existing Excel log file or create a new one if it doesn't exist.\n",
    "log_entries, error_count = Default_variable() #üìÉ Initialize a list to store log results for the current run and an error counter.\n",
    "#üí° Iterate through all files in the specified folder.\n",
    "for filename in Folder_EPS.glob(\"*.csv\"): #üß© Get files in the Folder directory.\n",
    "    #üìÉ Get the current modification timestamp and filename of the current file.\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Get modification timestamp, including microseconds.\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0) # Remove microseconds for easier comparison.\n",
    "    file_basename = filename.name # Get the base name of the file (e.g., \"data_20230101.csv\").\n",
    "    #üìÉ File is being processed.\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    #üìÉ Check if this file has been logged before.\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Filter the old log_df to find an entry for this filename.\n",
    "    is_new_file = log_entry.is_empty() # If no entry is found (log_entry is empty), it's a new file.\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1)  # Initialize the previous modification date with a very old date.\n",
    "    #üìÉ If this file was processed before (exists in the log).  \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Get the modification date recorded in the log from the previous processing.\n",
    "    #üìÉ Condition to process the file:\n",
    "    # 1. The file is new (not in the log).\n",
    "    # 2. Or, the file is old (in the log), but its current modification date (current_modified_date) is more recent than the modification date recorded in the log (previous_modified_date).\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # If it's an old file but has changed (ModifiedDate is newer).\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # If it's a completely new file.\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try: # Start a try-except block to handle potential errors during the import process.\n",
    "            if not is_new_file: # If this file already exists in the DB (because it's not new and its ModifiedDate changed) then delete the old data for this file from the df.\n",
    "                delete_data(engine, EPS_TABLE_NAME, file_basename)  #üß© Call the function to delete data.\n",
    "            #üìÉ Read data from the excel file using Polars.\n",
    "            EPS = (pl.read_csv(filename, infer_schema_length=0, encoding='latin-1') #üß© Read the entire file to infer the schema.Add two new columns to the DataFrame: FileName and ModifiedDate.\n",
    "                   .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   ))\n",
    "            try:\n",
    "                EPS = EPS.rename({'√Ø¬ª¬øsitecode': 'sitecode'})\n",
    "            except pl.exceptions.ColumnNotFoundError:\n",
    "                pass           \n",
    "            EPS = EPS.with_columns(pl.col('Session Login', 'Session Logout').str.strptime(pl.Datetime, format='%m/%d/%Y %H:%M'))\n",
    "            EPS = EPS.with_columns(pl.col('Session Login' #Setup Login VNüìÉ\n",
    "                                ).dt.replace_time_zone(\"Europe/Berlin\", ambiguous=\"earliest\"\n",
    "                                ).dt.convert_time_zone(\"Asia/Bangkok\").alias(\"SessionLogin_VN\"))\n",
    "            EPS = EPS.with_columns(pl.col('Session Logout' #Setup Logout VNüìÉ\n",
    "                                ).dt.replace_time_zone(\"Europe/Berlin\", ambiguous=\"earliest\"\n",
    "                                ).dt.convert_time_zone(\"Asia/Bangkok\").alias(\"SessionLogout_VN\"))\n",
    "            EPS = EPS.with_columns(pl.col('SessionLogin_VN', 'SessionLogout_VN').dt.strftime('%Y-%m-%d %H:%M:%S')) \n",
    "            EPS = EPS.to_pandas()\n",
    "            time_difference(t(23,0,0),t(7,0,0))\n",
    "            EPS['SessionLogin_VN'] =  pd.to_datetime(EPS['SessionLogin_VN'])\n",
    "            EPS['SessionLogout_VN'] =  pd.to_datetime(EPS['SessionLogout_VN'])\n",
    "            EPS['Hour Difference'] = (EPS['SessionLogout_VN'] - EPS['SessionLogin_VN']).dt.total_seconds()\n",
    "            EPS['is_night_login'] = [is_time_between(t(23,0),t(7,0),i) for i in EPS['SessionLogin_VN'].dt.time]\n",
    "            EPS['is_night_logout'] = [is_time_between(t(23,0),t(7,0),i) for i in EPS['SessionLogout_VN'].dt.time]\n",
    "            EPS['Log In Adjusted'] =  [time_difference(t(23,0),i) if a == False and b == True else 0 for i,a,b in zip(EPS['SessionLogin_VN'].dt.time,EPS['is_night_login'],EPS['is_night_logout'])]\n",
    "            EPS['Log Out Adjusted'] =  [time_difference(i,t(7,0)) if a == True and b == False else 0 for i,a,b in zip(EPS['SessionLogout_VN'].dt.time,EPS['is_night_login'],EPS['is_night_logout'])]\n",
    "            EPS['NightTime'] = [i-j-k if a != False or b != False else 0 for i,j,k,a,b in zip(EPS['Hour Difference'],EPS['Log In Adjusted'],EPS['Log Out Adjusted'],EPS['is_night_login'],EPS['is_night_logout'])]\n",
    "            EPS['Hour Difference'] = [time_difference(i,j) for i,j in zip(EPS['SessionLogout_VN'].dt.time,EPS['SessionLogin_VN'].dt.time)]\n",
    "            EPS['is_day_login'] = [is_time_between(t(7,0),t(23,0),i) for i in EPS['SessionLogin_VN'].dt.time]\n",
    "            EPS['is_day_logout'] = [is_time_between(t(7,0),t(23,0),i) for i in EPS['SessionLogout_VN'].dt.time]\n",
    "            EPS['Log In Adjusted'] =  [time_difference(t(7,0),i) if a == False and b == True else 0 for i,a,b in zip(EPS['SessionLogin_VN'].dt.time,EPS['is_day_login'],EPS['is_day_logout'])]\n",
    "            EPS['Log Out Adjusted'] =  [time_difference(i,t(23,0)) if a == True and b == False else 0 for i,a,b in zip(EPS['SessionLogout_VN'].dt.time,EPS['is_day_login'],EPS['is_day_logout'])]\n",
    "            EPS['DayTime'] = [i-j-k if a != False or b != False else 0 for i,j,k,a,b in zip(EPS['Hour Difference'],EPS['Log In Adjusted'],EPS['Log Out Adjusted'],EPS['is_day_login'],EPS['is_day_logout'])]\n",
    "            EPS = pl.from_pandas(EPS)\n",
    "            EPS = EPS.with_columns(pl.col(\"Total Time\").cast(pl.Int64))\n",
    "            EPS = EPS.with_columns(pl.when(pl.col(\"Total Time\") - pl.col(\"NightTime\") < 0).then(pl.col(\"Total Time\")).otherwise(pl.col(\"NightTime\")).alias(\"Night_BPE\"))\n",
    "            EPS = EPS.with_columns(pl.when(pl.col(\"Total Time\") - pl.col(\"NightTime\") < 0).then(0).otherwise(pl.col(\"Total Time\") - pl.col(\"Night_BPE\")).alias(\"Day_BPE\"))\n",
    "            #üìÉ Validate if the columns in the DataFrame match the defined schema\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(EPS, EPS_schema, file_basename) #üß©\n",
    "            if has_critical_error: # If critical columns are missing.\n",
    "                raise ValueError(critical_schema_error_msg) # Raise an error to stop processing this file.\n",
    "            #üìÉ Adjust the column structure.\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")    \n",
    "            EPS = (EPS.select(EPS_schema)) #üß© Select columns in the correct order and names as per df_schema.\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(EPS) #üß© Display information about the DataFrame (shape, columns, dtypes).\n",
    "            #üìÉ Start writing data to the database.\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, EPS_TABLE_NAME, EPS) #üß© Call the function to write the DataFrame to SQL.\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            #üìÉ Record to summary\n",
    "            if is_new_file:\n",
    "                summary_results[\"new_files_imported\"].append(f\"[EPS] {file_basename}\")#üß©\n",
    "            else:\n",
    "                summary_results[\"updated_files_reimported\"].append(f\"[EPS] {file_basename}\")#üß©\n",
    "            #üìÉ Update log_entries for successfully import: Add information to the log list for this run (successful).\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            #üìÉ Handle exceptions if any error occurred in the try block.\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception # Catch various types of potential errors.\n",
    "        ) as e:\n",
    "            error_count += 1 # Increment the error counter.        \n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"  # Get a short error message (the first line).\n",
    "            #üìÉ Update Failed file to Summary\n",
    "            summary_results[\"failed_imports\"].append((f\"[EPS] {file_basename}\", error_msg_short))#üß©\n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True) # Log the full error traceback.\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            # Add information to the log list for this run (failed).\n",
    "            # Note: ModifiedDate here is previous_modified_date, as this file wasn't successfully imported with current_modified_date.\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: #üìÉ If the file is not new and its modification date hasn't changed.\n",
    "          #üìÉ Update Skip file to Summary\n",
    "        summary_results[\"skipped_files_unchanged\"].append(f\"[EPS] {file_basename}\")#üß©\n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "#üí° After iterating through all files in the directory.\n",
    "display_summary(\"EPS\", error_count) #üß© Display a summary of the results (number of errors).\n",
    "process_and_save_log(log_df, log_entries, log_EPS_path) #üß© Update the Excel log file with information\n",
    "logger.info(\"===== Processing of the EPS data source is complete =====\") #üß© Log finish data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19724984-b665-414e-8b02-a3ad8108c1fc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 0Ô∏è‚É£4Ô∏è‚É£[BKN]CPIüíæ\n",
    "logger.info(\"===== Start CPI Process =====\")  #üß© Log the start of the data processing.\n",
    "log_df = read_or_create_log(log_CPI_path) #üß© Read the existing Excel log file or create a new one if it doesn't exist.\n",
    "log_entries, error_count = Default_variable() #üìÉ Initialize a list to store log results for the current run and an error counter.\n",
    "#üí° Iterate through all files in the specified folder.\n",
    "for filename in Folder_CPI.glob(\"*.csv\"): #üß© Get files in the Folder directory.\n",
    "    #üìÉ Get the current modification timestamp and filename of the current file.\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Get modification timestamp, including microseconds.\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0) # Remove microseconds for easier comparison.\n",
    "    file_basename = filename.name # Get the base name of the file (e.g., \"data_20230101.csv\").\n",
    "    #üìÉ File is being processed.\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    #üìÉ Check if this file has been logged before.\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Filter the old log_df to find an entry for this filename.\n",
    "    is_new_file = log_entry.is_empty() # If no entry is found (log_entry is empty), it's a new file.\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1)  # Initialize the previous modification date with a very old date.\n",
    "    #üìÉ If this file was processed before (exists in the log).  \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Get the modification date recorded in the log from the previous processing.\n",
    "    #üìÉ Condition to process the file:\n",
    "    # 1. The file is new (not in the log).\n",
    "    # 2. Or, the file is old (in the log), but its current modification date (current_modified_date) is more recent than the modification date recorded in the log (previous_modified_date).\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # If it's an old file but has changed (ModifiedDate is newer).\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # If it's a completely new file.\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try: # Start a try-except block to handle potential errors during the import process.\n",
    "            if not is_new_file: # If this file already exists in the DB (because it's not new and its ModifiedDate changed) then delete the old data for this file from the df.\n",
    "                delete_data(engine, CPI_TABLE_NAME, file_basename)  #üß© Call the function to delete data.\n",
    "            #üìÉ Read data from the excel file using Polars.\n",
    "            CPI = (pl.read_csv(filename, infer_schema_length=None) #üß© Read the entire file to infer the schema.Add two new columns to the DataFrame: FileName and ModifiedDate.\n",
    "                   .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   ))\n",
    "            try:\n",
    "                CPI = CPI.with_columns(pl.col('Nr. Contacts').cast(pl.Int64))\n",
    "            except:\n",
    "                CPI = CPI.with_columns(pl.col(\"Nr. Contacts\").str.replace_all(r\"[()]\", \"\").cast(pl.Int64).alias(\"Nr. Contacts\"))\n",
    "            CPI = CPI.with_columns(\n",
    "                pl.when(pl.col(\"Nr. Contacts\").is_null()).then(1).otherwise(pl.col(\"Nr. Contacts\").abs()).alias(\"Nr. Contacts\"))\n",
    "            # --- \"Date\" processing ---\n",
    "            if \"Date\" in CPI.columns:\n",
    "                logger.info(f\"'Date' column found in {file_basename}. Current dtype: {CPI['Date'].dtype}\")\n",
    "                print_colored(f\"'Date' column found in {file_basename}. Current dtype: {CPI['Date'].dtype}\", \"Olive\")\n",
    "                # Parse if \"Date\" in Utf8 or Object\n",
    "                if CPI[\"Date\"].dtype in [pl.Utf8, pl.Object]:\n",
    "                    try:\n",
    "                        logger.info(f\"Attempting to parse 'Date' column from string in {file_basename}.\")\n",
    "                        print_colored(f\"Attempting to parse 'Date' column from string in {file_basename}.\", \"Olive\")\n",
    "                        CPI = CPI.with_columns(parse_date(pl.col(\"Date\")).alias(\"Date\"),)\n",
    "                        logger.info(f\"‚úÖ Parse 'Date' successfully in {file_basename}.\")\n",
    "                        print_colored(f\"‚úÖ Parse 'Date' successfully in {file_basename}.\", \"MediumSeaGreen\")\n",
    "                    except Exception as parse_err:\n",
    "                        logger.error(f\"Could not parse string 'Date' column in {file_basename}: {parse_err}\", exc_info=True)\n",
    "                        print_colored(f\"‚ùóÔ∏è Failed to parse 'Date' column in {file_basename}. It will remain as string/object or become null.\", \"OrangeRed\")\n",
    "                        CPI = CPI.with_columns(pl.lit(None).cast(pl.Date).alias(\"Date\")) # add Date null\n",
    "                elif CPI[\"Date\"].dtype not in [pl.Date, pl.Datetime]:\n",
    "                    # if \"Date\" not in Utf8,Object or Date/Datetime (Ex: Int)\n",
    "                    logger.warning(f\"Existing 'Date' column in {file_basename} has unexpected type: {CPI['Date'].dtype}. Attempting direct cast to Date.\")\n",
    "                    print_colored(f\"Existing 'Date' column in {file_basename} has unexpected type: {CPI['Date'].dtype}. Attempting direct cast to Date.\", \"OrangeRed\")\n",
    "                    try:\n",
    "                        CPI = CPI.with_columns(pl.col(\"Date\").cast(pl.Date, strict=False))\n",
    "                        logger.info(f\"‚úÖ Cast 'Date' successfully in {file_basename}.\")\n",
    "                        print_colored(f\"‚úÖ Carse 'Date' successfully in {file_basename}.\", \"MediumSeaGreen\")\n",
    "                    except Exception as cast_err:\n",
    "                        logger.error(f\"Could not cast 'Date' column of type {CPI['Date'].dtype} to Date in {file_basename}: {cast_err}\")\n",
    "                        print_colored(f\"Could not cast 'Date' column of type {CPI['Date'].dtype} to Date in {file_basename}: {cast_err}\", \"OrangeRed\")\n",
    "                        CPI = CPI.with_columns(pl.lit(None).cast(pl.Date).alias(\"Date\")) # add Date null\n",
    "                else:\n",
    "                    # if \"Date\" in Date/Datetime\n",
    "                    logger.info(f\"'Date' column in {file_basename} is already Date/Datetime. Ensuring pl.Date type.\")\n",
    "                    print_colored(f\"'Date' column in {file_basename} is already Date/Datetime. Ensuring pl.Date type.\", \"Olive\")\n",
    "                    CPI = CPI.with_columns(pl.col(\"Date\").cast(pl.Date, strict=False))\n",
    "                    logger.info(f\"‚úÖ Cast 'Date' successfully in {file_basename}.\")\n",
    "                    print_colored(f\"‚úÖ Carse 'Date' successfully in {file_basename}.\", \"MediumSeaGreen\")\n",
    "            else:\n",
    "                # If \"Date\" not exist, extract from file name\n",
    "                logger.warning(f\"'Date' column not found in {file_basename}. Attempting to extract from filename.\")\n",
    "                print_colored(f\"'Date' column not found in {file_basename}. Attempting to extract from filename.\", \"Olive\")\n",
    "                try:\n",
    "                    date_from_filename = (\n",
    "                         pl.lit(file_basename)\n",
    "                        .str.replace(r\"\\.csv$\", \"\", literal=False) # remove .csv\n",
    "                        .str.extract(r\"(\\d{8})\", 1) # extract YYYYMMDD\n",
    "                        .str.strptime(pl.Date, \"%Y%m%d\", strict=False) # Parse YYYYMMDD\n",
    "                     )\n",
    "                    CPI = CPI.with_columns(date_from_filename.alias(\"Date\"))\n",
    "                    logger.info(f\"Successfully extracted 'Date' from filename {file_basename}.\")\n",
    "                    print_colored(f\"Successfully extracted 'Date' from filename {file_basename}.\", \"Olive\")\n",
    "                except Exception as extract_err:\n",
    "                    logger.error(f\"Could not extract date from filename {file_basename}: {extract_err}. Adding null 'Date' column.\", exc_info=True)\n",
    "                    print_colored(f\"Could not extract date from filename {file_basename}: {extract_err}. Adding null 'Date' column.\", \"OrangeRed\")\n",
    "                    CPI = CPI.with_columns(pl.lit(None).cast(pl.Date).alias(\"Date\")) # add Date null           \n",
    "            CPI = CPI.rename({'Time ': 'Time'}) #üß©\n",
    "            #üìÉ Validate if the columns in the DataFrame match the defined schema\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(CPI, CPI_schema, file_basename) #üß©\n",
    "            if has_critical_error: # If critical columns are missing.\n",
    "                raise ValueError(critical_schema_error_msg) # Raise an error to stop processing this file.\n",
    "            #üìÉ Adjust the column structure.\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            CPI = CPI.select(CPI_schema) #üß© Select columns in the correct order and names as per df_schema.\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(CPI) #üß© Display information about the DataFrame (shape, columns, dtypes).\n",
    "            #üìÉ Start writing data to the database.\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, CPI_TABLE_NAME, CPI) #üß© Call the function to write the DataFrame to SQL.\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            #üìÉ Record to summary\n",
    "            if is_new_file:\n",
    "                summary_results[\"new_files_imported\"].append(f\"[CPI] {file_basename}\")#üß©\n",
    "            else:\n",
    "                summary_results[\"updated_files_reimported\"].append(f\"[CPI] {file_basename}\")#üß©\n",
    "            #üìÉ Update log_entries for successfully import: Add information to the log list for this run (successful).\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            #üìÉ Handle exceptions if any error occurred in the try block. \n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception # Catch various types of potential errors.\n",
    "        ) as e:\n",
    "            error_count += 1 # Increment the error counter.        \n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"  # Get a short error message (the first line).\n",
    "\n",
    "            #üìÉ Update Failed file to Summary\n",
    "            summary_results[\"failed_imports\"].append((f\"[CPI] {file_basename}\", error_msg_short))#üß©       \n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True) # Log the full error traceback.\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            # Add information to the log list for this run (failed).\n",
    "            # Note: ModifiedDate here is previous_modified_date, as this file wasn't successfully imported with current_modified_date.\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: #üìÉ If the file is not new and its modification date hasn't changed.\n",
    "          #üìÉ Update Skip file to Summary\n",
    "        summary_results[\"skipped_files_unchanged\"].append(f\"[CPI] {file_basename}\")#üß©\n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "#üí° After iterating through all files in the directory.\n",
    "display_summary(\"CPI\", error_count) #üß© Display a summary of the results (number of errors).\n",
    "process_and_save_log(log_df, log_entries, log_CPI_path) #üß© Update the Excel log file with information\n",
    "logger.info(\"===== Processing of the CPI data source is complete =====\") #üß© Log finish data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4a35d7-ced3-4579-8991-f0cb3177e090",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 0Ô∏è‚É£5Ô∏è‚É£[GLB]RAMCOüíæ\n",
    "logger.info(\"===== Start RAMCO Process =====\")  #üß© Log the start of the data processing.\n",
    "log_df = read_or_create_log(log_RAMCO_path) #üß© Read the existing Excel log file or create a new one if it doesn't exist.\n",
    "log_entries, error_count = Default_variable() #üìÉ Initialize a list to store log results for the current run and an error counter.\n",
    "#üí° Iterate through all files in the specified folder.\n",
    "for filename in Folder_RAMCO.glob(\"*.csv\"): #üß© Get files in the Folder directory.\n",
    "    #üìÉ Get the current modification timestamp and filename of the current file.\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Get modification timestamp, including microseconds.\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0) # Remove microseconds for easier comparison.\n",
    "    file_basename = filename.name # Get the base name of the file (e.g., \"data_20230101.csv\").\n",
    "    #üìÉ File is being processed.\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    #üìÉ Check if this file has been logged before.\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Filter the old log_df to find an entry for this filename.\n",
    "    is_new_file = log_entry.is_empty() # If no entry is found (log_entry is empty), it's a new file.\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1)  # Initialize the previous modification date with a very old date.\n",
    "    #üìÉ If this file was processed before (exists in the log).  \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Get the modification date recorded in the log from the previous processing.\n",
    "    #üìÉ Condition to process the file:\n",
    "    # 1. The file is new (not in the log).\n",
    "    # 2. Or, the file is old (in the log), but its current modification date (current_modified_date) is more recent than the modification date recorded in the log (previous_modified_date).\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # If it's an old file but has changed (ModifiedDate is newer).\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # If it's a completely new file.\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try: # Start a try-except block to handle potential errors during the import process.\n",
    "            if not is_new_file: # If this file already exists in the DB (because it's not new and its ModifiedDate changed) then delete the old data for this file from the df.\n",
    "                delete_data(engine, RAMCO_TABLE_NAME, file_basename)  #üß© Call the function to delete data.\n",
    "            #üìÉ Read data from the excel file using Polars.\n",
    "            RAMCO = (pl.read_csv(filename, infer_schema_length=0, encoding='latin-1') #üß© Read the entire file to infer the schema.Add two new columns to the DataFrame: FileName and ModifiedDate.\n",
    "                       .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")))\n",
    "            RAMCO = RAMCO.with_columns(\n",
    "                pl.col('Attribute').str.strptime(pl.Date, format='%m/%d/%Y'),\n",
    "                pl.col('EID').cast(pl.Int64)).rename({'Attribute': 'Date', 'Value': 'Code'}) #üß©\n",
    "            #üìÉ Validate if the columns in the DataFrame match the defined schema\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(RAMCO, RAMCO_schema, file_basename) #üß©\n",
    "            if has_critical_error: # If critical columns are missing.\n",
    "                raise ValueError(critical_schema_error_msg) # Raise an error to stop processing this file.\n",
    "            #üìÉ Adjust the column structure.\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            RAMCO = RAMCO.select(RAMCO_schema) #üß© Select columns in the correct order and names as per df_schema.\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(RAMCO) #üß© Display information about the DataFrame (shape, columns, dtypes).\n",
    "            #üìÉ Start writing data to the database.\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, RAMCO_TABLE_NAME, RAMCO) #üß© Call the function to write the DataFrame to SQL.\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            #üìÉ Record to summary\n",
    "            if is_new_file:\n",
    "                summary_results[\"new_files_imported\"].append(f\"[RAMCO] {file_basename}\")#üß©\n",
    "            else:\n",
    "                summary_results[\"updated_files_reimported\"].append(f\"[RAMCO] {file_basename}\")#üß©\n",
    "            #üìÉ Update log_entries for successfully import: Add information to the log list for this run (successful).\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            #üìÉ Handle exceptions if any error occurred in the try block.\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception # Catch various types of potential errors.\n",
    "        ) as e:\n",
    "            error_count += 1 # Increment the error counter.        \n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"  # Get a short error message (the first line).\n",
    "            #üìÉ Update Failed file to Summary\n",
    "            summary_results[\"failed_imports\"].append((f\"[RAMCO] {file_basename}\", error_msg_short))#üß©  \n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True) # Log the full error traceback.\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            # Add information to the log list for this run (failed).\n",
    "            # Note: ModifiedDate here is previous_modified_date, as this file wasn't successfully imported with current_modified_date.\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: #üìÉ If the file is not new and its modification date hasn't changed.\n",
    "          #üìÉ Update Skip file to Summary\n",
    "        summary_results[\"skipped_files_unchanged\"].append(f\"[RAMCO] {file_basename}\")#üß©  \n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "#üí° After iterating through all files in the directory.\n",
    "display_summary(\"RAMCO\", error_count) #üß© Display a summary of the results (number of errors).\n",
    "process_and_save_log(log_df, log_entries, log_RAMCO_path) #üß© Update the Excel log file with information\n",
    "logger.info(\"===== Processing of the RAMCO data source is complete =====\") #üß© Log finish data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067593bc-a572-472f-808c-07aa0c6e080f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 0Ô∏è‚É£6Ô∏è‚É£[GLB]OT_RAMCOüíæ\n",
    "logger.info(\"===== Start OT_ROSTER Process =====\")  #üß© Log the start of the data processing.\n",
    "log_df = read_or_create_log(log_OT_RAMCO_path) #üß© Read the existing Excel log file or create a new one if it doesn't exist.\n",
    "log_entries, error_count = Default_variable() #üìÉ Initialize a list to store log results for the current run and an error counter.\n",
    "#üí° Iterate through all files in the specified folder.\n",
    "for filename in Folder_OT_RAMCO.glob(\"*.xlsx\"): #üß© Get files in the Folder directory.\n",
    "    #üìÉ Get the current modification timestamp and filename of the current file.\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Get modification timestamp, including microseconds.\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0) # Remove microseconds for easier comparison.\n",
    "    file_basename = filename.name # Get the base name of the file (e.g., \"data_20230101.csv\").\n",
    "    #üìÉ File is being processed.\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    #üìÉ Check if this file has been logged before.\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Filter the old log_df to find an entry for this filename.\n",
    "    is_new_file = log_entry.is_empty() # If no entry is found (log_entry is empty), it's a new file.\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1)  # Initialize the previous modification date with a very old date.\n",
    "    #üìÉ If this file was processed before (exists in the log).  \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Get the modification date recorded in the log from the previous processing.\n",
    "    #üìÉ Condition to process the file:\n",
    "    # 1. The file is new (not in the log).\n",
    "    # 2. Or, the file is old (in the log), but its current modification date (current_modified_date) is more recent than the modification date recorded in the log (previous_modified_date).\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # If it's an old file but has changed (ModifiedDate is newer).\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # If it's a completely new file.\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try: # Start a try-except block to handle potential errors during the import process.\n",
    "            if not is_new_file: # If this file already exists in the DB (because it's not new and its ModifiedDate changed) then delete the old data for this file from the df.\n",
    "                delete_data(engine, OT_RAMCO_TABLE_NAME, file_basename)  #üß© Call the function to delete data.\n",
    "            #üìÉ Read data from the excel file using Polars.\n",
    "            OT_RAMCO = (pl.read_excel(filename, infer_schema_length=None, engine=\"calamine\", sheet_name=\"Sheet1\") #üß© Read the entire file to infer the schema.Add two new columns to the DataFrame: FileName and ModifiedDate.\n",
    "                   .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   ).rename({'Attribute': 'Date'})) #üß©\n",
    "            #üìÉ Validate if the columns in the DataFrame match the defined schema\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(OT_RAMCO, OT_RAMCO_schema, file_basename) #üß©\n",
    "            if has_critical_error: # If critical columns are missing.\n",
    "                raise ValueError(critical_schema_error_msg) # Raise an error to stop processing this file.\n",
    "            #üìÉ Adjust the column structure.\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            OT_RAMCO = OT_RAMCO.select(OT_RAMCO_schema) #üß© Select columns in the correct order and names as per df_schema.\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(OT_RAMCO) #üß© Display information about the DataFrame (shape, columns, dtypes).\n",
    "            #üìÉ Start writing data to the database.\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, OT_RAMCO_TABLE_NAME, OT_RAMCO) #üß© Call the function to write the DataFrame to SQL.\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            #üìÉ Record to summary\n",
    "            if is_new_file:\n",
    "                summary_results[\"new_files_imported\"].append(f\"[OT_RAMCO] {file_basename}\")#üß©\n",
    "            else:\n",
    "                summary_results[\"updated_files_reimported\"].append(f\"[OT_RAMCO] {file_basename}\")#üß©   \n",
    "            #üìÉ Update log_entries for successfully import: Add information to the log list for this run (successful).\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            #üìÉ Handle exceptions if any error occurred in the try block.\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception # Catch various types of potential errors.\n",
    "        ) as e:\n",
    "            error_count += 1 # Increment the error counter.        \n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"  # Get a short error message (the first line).\n",
    "            #üìÉ Update Failed file to Summary\n",
    "            summary_results[\"failed_imports\"].append((f\"[OT_RAMCO] {file_basename}\", error_msg_short))#üß©\n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True) # Log the full error traceback.\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            # Add information to the log list for this run (failed).\n",
    "            # Note: ModifiedDate here is previous_modified_date, as this file wasn't successfully imported with current_modified_date.\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: #üìÉ If the file is not new and its modification date hasn't changed.\n",
    "          #üìÉ Update Skip file to Summary\n",
    "        summary_results[\"skipped_files_unchanged\"].append(f\"[OT_RAMCO] {file_basename}\")#üß©\n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "#üí° After iterating through all files in the directory.\n",
    "display_summary(\"OT_RAMCO\", error_count) #üß© Display a summary of the results (number of errors).\n",
    "process_and_save_log(log_df, log_entries, log_OT_RAMCO_path) #üß© Update the Excel log file with information\n",
    "logger.info(\"===== Processing of the OT_RAMCO data source is complete =====\") #üß© Log finish data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57aab6b-a5dc-4fce-a47b-3593c4ff2c7c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 0Ô∏è‚É£7Ô∏è‚É£[GLB]PremHdaysüíæ\n",
    "logger.info(\"===== Start PremHdays Process =====\")  #üß© Log the start of the data processing.\n",
    "log_df = read_or_create_log(log_PremHdays_path) #üß© Read the existing Excel log file or create a new one if it doesn't exist.\n",
    "log_entries, error_count = Default_variable() #üìÉ Initialize a list to store log results for the current run and an error counter.\n",
    "#üí° Iterate through all files in the specified folder.\n",
    "for filename in Folder_PremHdays.glob(\"*.csv\"): #üß© Get files in the Folder directory.\n",
    "    #üìÉ Get the current modification timestamp and filename of the current file.\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Get modification timestamp, including microseconds.\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0) # Remove microseconds for easier comparison.\n",
    "    file_basename = filename.name # Get the base name of the file (e.g., \"data_20230101.csv\").\n",
    "    #üìÉ File is being processed.\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    #üìÉ Check if this file has been logged before.\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Filter the old log_df to find an entry for this filename.\n",
    "    is_new_file = log_entry.is_empty() # If no entry is found (log_entry is empty), it's a new file.\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1)  # Initialize the previous modification date with a very old date.\n",
    "    #üìÉ If this file was processed before (exists in the log).  \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Get the modification date recorded in the log from the previous processing.\n",
    "    #üìÉ Condition to process the file:\n",
    "    # 1. The file is new (not in the log).\n",
    "    # 2. Or, the file is old (in the log), but its current modification date (current_modified_date) is more recent than the modification date recorded in the log (previous_modified_date).\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # If it's an old file but has changed (ModifiedDate is newer).\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # If it's a completely new file.\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try: # Start a try-except block to handle potential errors during the import process.\n",
    "            if not is_new_file: # If this file already exists in the DB (because it's not new and its ModifiedDate changed) then delete the old data for this file from the df.\n",
    "                delete_data(engine, PremHdays_TABLE_NAME, file_basename)  #üß© Call the function to delete data.\n",
    "            #üìÉ Read data from the excel file using Polars.\n",
    "            PremHdays = (pl.read_csv(filename, infer_schema_length=None) #üß© Read the entire file to infer the schema.Add two new columns to the DataFrame: FileName and ModifiedDate.\n",
    "                    .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   )) # Import Schemaüß©\n",
    "            #üìÉ Validate if the columns in the DataFrame match the defined schema\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(PremHdays, PremHdays_schema, file_basename) #üß©\n",
    "            if has_critical_error: # If critical columns are missing.\n",
    "                raise ValueError(critical_schema_error_msg) # Raise an error to stop processing this file.\n",
    "            #üìÉ Adjust the column structure.\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            PremHdays = PremHdays.with_columns(pl.col('Date').str.strptime(pl.Date, format='%m/%d/%Y')) #üß©\n",
    "            PremHdays = PremHdays.select(PremHdays_schema) #üß© Select columns in the correct order and names as per df_schema.\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(PremHdays) #üß© Display information about the DataFrame (shape, columns, dtypes).\n",
    "            #üìÉ Start writing data to the database.\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, PremHdays_TABLE_NAME, PremHdays) #üß© Call the function to write the DataFrame to SQL.\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            #üìÉ Record to summary\n",
    "            if is_new_file:\n",
    "                summary_results[\"new_files_imported\"].append(f\"[PremHdays] {file_basename}\")#üß©\n",
    "            else:\n",
    "                summary_results[\"updated_files_reimported\"].append(f\"[PremHdays] {file_basename}\")#üß©       \n",
    "            #üìÉ Update log_entries for successfully import: Add information to the log list for this run (successful).\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            #üìÉ Handle exceptions if any error occurred in the try block.\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception # Catch various types of potential errors.\n",
    "        ) as e:\n",
    "            error_count += 1 # Increment the error counter.        \n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"  # Get a short error message (the first line).\n",
    "            #üìÉ Update Failed file to Summary\n",
    "            summary_results[\"failed_imports\"].append((f\"[PremHdays] {file_basename}\", error_msg_short))#üß©       \n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True) # Log the full error traceback.\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            # Add information to the log list for this run (failed).\n",
    "            # Note: ModifiedDate here is previous_modified_date, as this file wasn't successfully imported with current_modified_date.\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: #üìÉ If the file is not new and its modification date hasn't changed.\n",
    "          #üìÉ Update Skip file to Summary\n",
    "        summary_results[\"skipped_files_unchanged\"].append(f\"[PremHdays] {file_basename}\")#üß©\n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "#üí° After iterating through all files in the directory.\n",
    "display_summary(\"PremHdays\", error_count) #üß© Display a summary of the results (number of errors).\n",
    "process_and_save_log(log_df, log_entries, log_PremHdays_path) #üß© Update the Excel log file with information\n",
    "logger.info(\"===== Processing of the PremHdays data source is complete =====\") #üß© Log finish data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ba2009-31b0-4ec4-a511-2cddfa75be08",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 0Ô∏è‚É£8Ô∏è‚É£[GLB]NormHdaysüíæ\n",
    "logger.info(\"===== Start NormHdays Process =====\")  #üß© Log the start of the data processing.\n",
    "log_df = read_or_create_log(log_NormHdays_path) #üß© Read the existing Excel log file or create a new one if it doesn't exist.\n",
    "log_entries, error_count = Default_variable() #üìÉ Initialize a list to store log results for the current run and an error counter.\n",
    "#üí° Iterate through all files in the specified folder.\n",
    "for filename in Folder_NormHdays.glob(\"*.csv\"): #üß© Get files in the Folder directory.\n",
    "    #üìÉ Get the current modification timestamp and filename of the current file.\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Get modification timestamp, including microseconds.\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0) # Remove microseconds for easier comparison.\n",
    "    file_basename = filename.name # Get the base name of the file (e.g., \"data_20230101.csv\").\n",
    "    #üìÉ File is being processed.\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    #üìÉ Check if this file has been logged before.\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Filter the old log_df to find an entry for this filename.\n",
    "    is_new_file = log_entry.is_empty() # If no entry is found (log_entry is empty), it's a new file.\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1)  # Initialize the previous modification date with a very old date.\n",
    "    #üìÉ If this file was processed before (exists in the log).  \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Get the modification date recorded in the log from the previous processing.\n",
    "    #üìÉ Condition to process the file:\n",
    "    # 1. The file is new (not in the log).\n",
    "    # 2. Or, the file is old (in the log), but its current modification date (current_modified_date) is more recent than the modification date recorded in the log (previous_modified_date).\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # If it's an old file but has changed (ModifiedDate is newer).\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # If it's a completely new file.\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try: # Start a try-except block to handle potential errors during the import process.\n",
    "            if not is_new_file: # If this file already exists in the DB (because it's not new and its ModifiedDate changed) then delete the old data for this file from the df.\n",
    "                delete_data(engine, NormHdays_TABLE_NAME, file_basename)  #üß© Call the function to delete data.\n",
    "            #üìÉ Read data from the excel file using Polars.\n",
    "            NormHdays = (pl.read_csv(filename, infer_schema_length=None) #üß© Read the entire file to infer the schema.Add two new columns to the DataFrame: FileName and ModifiedDate.\n",
    "                    .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   )) #üß©\n",
    "            #üìÉ Validate if the columns in the DataFrame match the defined schema\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(NormHdays, NormHdays_schema, file_basename) #üß©\n",
    "            if has_critical_error: # If critical columns are missing.\n",
    "                raise ValueError(critical_schema_error_msg) # Raise an error to stop processing this file.\n",
    "            #üìÉ Adjust the column structure.\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            NormHdays = NormHdays.with_columns(pl.col('Solar Day', 'Lunar Day').str.strptime(pl.Date, format='%m/%d/%Y')) #üß©\n",
    "            NormHdays = NormHdays.select(NormHdays_schema) #üß© Select columns in the correct order and names as per df_schema.\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(NormHdays) #üß© Display information about the DataFrame (shape, columns, dtypes).\n",
    "            #üìÉ Start writing data to the database.\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, NormHdays_TABLE_NAME, NormHdays) #üß© Call the function to write the DataFrame to SQL.\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            #üìÉ Record to summary\n",
    "            if is_new_file:\n",
    "                summary_results[\"new_files_imported\"].append(f\"[NormHdays] {file_basename}\")#üß©\n",
    "            else:\n",
    "                summary_results[\"updated_files_reimported\"].append(f\"[NormHdays] {file_basename}\")#üß©  \n",
    "            #üìÉ Update log_entries for successfully import: Add information to the log list for this run (successful).\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            #üìÉ Handle exceptions if any error occurred in the try block.\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception # Catch various types of potential errors.\n",
    "        ) as e:\n",
    "            error_count += 1 # Increment the error counter.        \n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"  # Get a short error message (the first line).\n",
    "            #üìÉ Update Failed file to Summary\n",
    "            summary_results[\"failed_imports\"].append((f\"[NormHdays] {file_basename}\", error_msg_short))#üß©\n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True) # Log the full error traceback.\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            # Add information to the log list for this run (failed).\n",
    "            # Note: ModifiedDate here is previous_modified_date, as this file wasn't successfully imported with current_modified_date.\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: #üìÉ If the file is not new and its modification date hasn't changed.\n",
    "          #üìÉ Update Skip file to Summary\n",
    "        summary_results[\"skipped_files_unchanged\"].append(f\"[NormHdays] {file_basename}\")#üß©\n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "#üí° After iterating through all files in the directory.\n",
    "display_summary(\"NormHdays\", error_count) #üß© Display a summary of the results (number of errors).\n",
    "process_and_save_log(log_df, log_entries, log_NormHdays_path) #üß© Update the Excel log file with information\n",
    "logger.info(\"===== Processing of the NormHdays data source is complete =====\") #üß© Log finish data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cef690e-9148-4a5a-8159-5500e1b67826",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 0Ô∏è‚É£9Ô∏è‚É£[GLB]EmpMasterüíæ\n",
    "logger.info(\"===== Start EmpMaster Process =====\")  #üß© Log the start of the data processing.\n",
    "log_df = read_or_create_log(log_EmpMaster_path) #üß© Read the existing Excel log file or create a new one if it doesn't exist.\n",
    "log_entries, error_count = Default_variable() #üìÉ Initialize a list to store log results for the current run and an error counter.\n",
    "#üí° Iterate through all files in the specified folder.\n",
    "for filename in Folder_EmpMaster.glob(\"*.xlsx\"): #üß© Get files in the Folder directory.\n",
    "    #üìÉ Get the current modification timestamp and filename of the current file.\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Get modification timestamp, including microseconds.\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0) # Remove microseconds for easier comparison.\n",
    "    file_basename = filename.name # Get the base name of the file (e.g., \"data_20230101.csv\").\n",
    "    #üìÉ File is being processed.\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    #üìÉ Check if this file has been logged before.\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Filter the old log_df to find an entry for this filename.\n",
    "    is_new_file = log_entry.is_empty() # If no entry is found (log_entry is empty), it's a new file.\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1)  # Initialize the previous modification date with a very old date.\n",
    "    #üìÉ If this file was processed before (exists in the log).  \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Get the modification date recorded in the log from the previous processing.\n",
    "    #üìÉ Condition to process the file:\n",
    "    # 1. The file is new (not in the log).\n",
    "    # 2. Or, the file is old (in the log), but its current modification date (current_modified_date) is more recent than the modification date recorded in the log (previous_modified_date).\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # If it's an old file but has changed (ModifiedDate is newer).\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # If it's a completely new file.\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try: # Start a try-except block to handle potential errors during the import process.\n",
    "            if not is_new_file: # If this file already exists in the DB (because it's not new and its ModifiedDate changed) then delete the old data for this file from the df.\n",
    "                delete_data(engine, EmpMaster_TABLE_NAME, file_basename)  #üß© Call the function to delete data.\n",
    "            #üìÉ Read data from the excel file using Polars.\n",
    "            EmpMaster = (pl.read_excel(filename, infer_schema_length=None, engine=\"calamine\", sheet_name=\"Employee Master\") #üß© Read the entire file to infer the schema.Add two new columns to the DataFrame: FileName and ModifiedDate.\n",
    "                   .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   )) #üß©\n",
    "            #üìÉ Validate if the columns in the DataFrame match the defined schema\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(EmpMaster, EmpMaster_schema, file_basename) #üß©\n",
    "            if has_critical_error: # If critical columns are missing.\n",
    "                raise ValueError(critical_schema_error_msg) # Raise an error to stop processing this file.\n",
    "            #üìÉ Adjust the column structure.\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            EmpMaster = EmpMaster.with_columns(pl.col(\"ORIGINAL_DATE_OF_HIRE\", \"LEGAL_EMPLOYER_HIRE_DATE\", \"Continuous Service Date\", \\\n",
    "                                                      \"Fixed Term Hire End Date\", \"Contract End Date\", \"Last Promotion Date\",).cast(pl.Date)) #üß©\n",
    "            EmpMaster = EmpMaster.select(EmpMaster_schema) #üß© Select columns in the correct order and names as per df_schema.\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(EmpMaster) #üß© Display information about the DataFrame (shape, columns, dtypes).\n",
    "            #üìÉ Start writing data to the database.\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, EmpMaster_TABLE_NAME, EmpMaster) #üß© Call the function to write the DataFrame to SQL.\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            #üìÉ Record to summary\n",
    "            if is_new_file:\n",
    "                summary_results[\"new_files_imported\"].append(f\"[EmpMaster] {file_basename}\")#üß©\n",
    "            else:\n",
    "                summary_results[\"updated_files_reimported\"].append(f\"[EmpMaster] {file_basename}\")#üß©\n",
    "            #üìÉ Update log_entries for successfully import: Add information to the log list for this run (successful).\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            #üìÉ Handle exceptions if any error occurred in the try block.\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception # Catch various types of potential errors.\n",
    "        ) as e:\n",
    "            error_count += 1 # Increment the error counter.        \n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"  # Get a short error message (the first line).\n",
    "            #üìÉ Update Failed file to Summary\n",
    "            summary_results[\"failed_imports\"].append((f\"[EmpMaster] {file_basename}\", error_msg_short))#üß©\n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True) # Log the full error traceback.\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            # Add information to the log list for this run (failed).\n",
    "            # Note: ModifiedDate here is previous_modified_date, as this file wasn't successfully imported with current_modified_date.\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: #üìÉ If the file is not new and its modification date hasn't changed.\n",
    "          #üìÉ Update Skip file to Summary\n",
    "        summary_results[\"skipped_files_unchanged\"].append(f\"[EmpMaster] {file_basename}\")#üß©\n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "#üí° After iterating through all files in the directory.\n",
    "display_summary(\"EmpMaster\", error_count) #üß© Display a summary of the results (number of errors).\n",
    "process_and_save_log(log_df, log_entries, log_EmpMaster_path) #üß© Update the Excel log file with information\n",
    "logger.info(\"===== Processing of the EmpMaster data source is complete =====\") #üß© Log finish data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd483483-5974-474e-bc42-7cf5f97eb53a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 1Ô∏è‚É£0Ô∏è‚É£[GLB]Terminationüíæ\n",
    "logger.info(\"===== Start Termination Process =====\")  #üß© Log the start of the data processing.\n",
    "log_df = read_or_create_log(log_Termination_path) #üß© Read the existing Excel log file or create a new one if it doesn't exist.\n",
    "log_entries, error_count = Default_variable() #üìÉ Initialize a list to store log results for the current run and an error counter.\n",
    "#üí° Iterate through all files in the specified folder.\n",
    "for filename in Folder_Termination.glob(\"*.xlsx\"): #üß© Get files in the Folder directory.\n",
    "    #üìÉ Get the current modification timestamp and filename of the current file.\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Get modification timestamp, including microseconds.\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0) # Remove microseconds for easier comparison.\n",
    "    file_basename = filename.name # Get the base name of the file (e.g., \"data_20230101.csv\").\n",
    "    #üìÉ File is being processed.\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    #üìÉ Check if this file has been logged before.\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Filter the old log_df to find an entry for this filename.\n",
    "    is_new_file = log_entry.is_empty() # If no entry is found (log_entry is empty), it's a new file.\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1)  # Initialize the previous modification date with a very old date.\n",
    "    #üìÉ If this file was processed before (exists in the log).  \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Get the modification date recorded in the log from the previous processing.\n",
    "    #üìÉ Condition to process the file:\n",
    "    # 1. The file is new (not in the log).\n",
    "    # 2. Or, the file is old (in the log), but its current modification date (current_modified_date) is more recent than the modification date recorded in the log (previous_modified_date).\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # If it's an old file but has changed (ModifiedDate is newer).\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # If it's a completely new file.\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try: # Start a try-except block to handle potential errors during the import process.\n",
    "            if not is_new_file: # If this file already exists in the DB (because it's not new and its ModifiedDate changed) then delete the old data for this file from the df.\n",
    "                delete_data(engine, Termination_TABLE_NAME, file_basename)  #üß© Call the function to delete data.\n",
    "            #üìÉ Read data from the excel file using Polars.\n",
    "            Termination = (pl.read_excel(filename, infer_schema_length=None, engine=\"calamine\", sheet_name=\"Termination Dump\") #üß© Read the entire file to infer the schema.Add two new columns to the DataFrame: FileName and ModifiedDate.\n",
    "                    .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   )) #üß©\n",
    "            #üìÉ Validate if the columns in the DataFrame match the defined schema\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(Termination, Termination_schema, file_basename) #üß©\n",
    "            if has_critical_error: # If critical columns are missing.\n",
    "                raise ValueError(critical_schema_error_msg) # Raise an error to stop processing this file.\n",
    "            #üìÉ Adjust the column structure.\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            Termination = Termination.with_columns(pl.col(\"HIRE_DATE\", \"ORIGINAL_HIRE_DATE\", \"END EMPLOYMENT DATE\", \n",
    "                                                          \"Contract End Date\", \"Termination Date\", \"Termination Date (DD/MM/YY)\", \n",
    "                                                          \"LWD\", \"MOST RECENT TERMINATION - DATE INITIATED\", \"MOST RECENT TERMINATION - DATE COMPLETED\",\n",
    "                                                          \"MOST RECENT TERMINATION - EFFECTIVE DATE\", \"DATE INITIATED\",\n",
    "                                                          \"TERMINATION DATE 2\", \"Continuous Service Date\").cast(pl.Date)) #üß©\n",
    "            Termination = Termination.with_columns(pl.col(\"Action date\", \"COMPELETED DATE AND TIME\").cast(pl.Datetime)) #üß©\n",
    "            Termination = Termination.with_columns(pl.col(\"Resignation Notice served\").cast(pl.Int64)) #üß©\n",
    "            Termination = Termination.select(Termination_schema) #üß© Select columns in the correct order and names as per df_schema.\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(Termination) #üß© Display information about the DataFrame (shape, columns, dtypes).\n",
    "            #üìÉ Start writing data to the database.\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, Termination_TABLE_NAME, Termination) #üß© Call the function to write the DataFrame to SQL.\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            #üìÉ Record to summary\n",
    "            if is_new_file:\n",
    "                summary_results[\"new_files_imported\"].append(f\"[Termination] {file_basename}\")#üß©\n",
    "            else:\n",
    "                summary_results[\"updated_files_reimported\"].append(f\"[Termination] {file_basename}\")#üß©    \n",
    "            #üìÉ Update log_entries for successfully import: Add information to the log list for this run (successful).\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            #üìÉ Handle exceptions if any error occurred in the try block.\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception # Catch various types of potential errors.\n",
    "        ) as e:\n",
    "            error_count += 1 # Increment the error counter.        \n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"  # Get a short error message (the first line).\n",
    "            #üìÉ Update Failed file to Summary\n",
    "            summary_results[\"failed_imports\"].append((f\"[Termination] {file_basename}\", error_msg_short))#üß©      \n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True) # Log the full error traceback.\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            # Add information to the log list for this run (failed).\n",
    "            # Note: ModifiedDate here is previous_modified_date, as this file wasn't successfully imported with current_modified_date.\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: #üìÉ If the file is not new and its modification date hasn't changed.\n",
    "          #üìÉ Update Skip file to Summary\n",
    "        summary_results[\"skipped_files_unchanged\"].append(f\"[Termination] {file_basename}\")#üß©\n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "#üí° After iterating through all files in the directory.\n",
    "display_summary(\"Termination\", error_count) #üß© Display a summary of the results (number of errors).\n",
    "process_and_save_log(log_df, log_entries, log_Termination_path) #üß© Update the Excel log file with information\n",
    "logger.info(\"===== Processing of the Termination data source is complete =====\") #üß© Log finish data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cef9f2-5889-464e-9daf-9b93077f1b85",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 1Ô∏è‚É£1Ô∏è‚É£[GLB]Resignationüíæ\n",
    "logger.info(\"===== Start Resignation Process =====\")  #üß© Log the start of the data processing.\n",
    "log_df = read_or_create_log(log_Resignation_path) #üß© Read the existing Excel log file or create a new one if it doesn't exist.\n",
    "log_entries, error_count = Default_variable() #üìÉ Initialize a list to store log results for the current run and an error counter.\n",
    "#üí° Iterate through all files in the specified folder.\n",
    "for filename in Folder_Resignation.glob(\"*.xlsx\"): #üß© Get files in the Folder directory.\n",
    "    #üìÉ Get the current modification timestamp and filename of the current file.\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Get modification timestamp, including microseconds.\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0) # Remove microseconds for easier comparison.\n",
    "    file_basename = filename.name # Get the base name of the file (e.g., \"data_20230101.csv\").\n",
    "    #üìÉ File is being processed.\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    #üìÉ Check if this file has been logged before.\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Filter the old log_df to find an entry for this filename.\n",
    "    is_new_file = log_entry.is_empty() # If no entry is found (log_entry is empty), it's a new file.\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1)  # Initialize the previous modification date with a very old date.\n",
    "    #üìÉ If this file was processed before (exists in the log).  \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Get the modification date recorded in the log from the previous processing.\n",
    "    #üìÉ Condition to process the file:\n",
    "    # 1. The file is new (not in the log).\n",
    "    # 2. Or, the file is old (in the log), but its current modification date (current_modified_date) is more recent than the modification date recorded in the log (previous_modified_date).\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # If it's an old file but has changed (ModifiedDate is newer).\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # If it's a completely new file.\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try: # Start a try-except block to handle potential errors during the import process.\n",
    "            if not is_new_file: # If this file already exists in the DB (because it's not new and its ModifiedDate changed) then delete the old data for this file from the df.\n",
    "                delete_data(engine, Resignation_TABLE_NAME, file_basename)  #üß© Call the function to delete data.\n",
    "            #üìÉ Read data from the excel file using Polars.\n",
    "            Resignation = (pl.read_excel(filename, infer_schema_length=None, engine=\"calamine\", sheet_name=\"Resignation\") #üß© Read the entire file to infer the schema.Add two new columns to the DataFrame: FileName and ModifiedDate.\n",
    "                   .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   )) #üß©\n",
    "            #üìÉ Validate if the columns in the DataFrame match the defined schema\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(Resignation, Resignation_schema, file_basename) #üß©\n",
    "            if has_critical_error: # If critical columns are missing.\n",
    "                raise ValueError(critical_schema_error_msg) # Raise an error to stop processing this file.\n",
    "            #üìÉ Adjust the column structure.\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            Resignation = Resignation.with_columns(pl.col(\"Notification Date\", \"Hire Date\", \"Proposed Termination Date\",).cast(pl.Date)) #üß©\n",
    "            Resignation = Resignation.with_columns(pl.col(\"Action Date\", \"Date and Time Initiated\").cast(pl.Datetime)) #üß©\n",
    "            Resignation = Resignation.with_columns(pl.col(\"Notice Served\").cast(pl.Int64)) #üß©\n",
    "            Resignation = Resignation.select(Resignation_schema) #üß© Select columns in the correct order and names as per df_schema.\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(Resignation) #üß© Display information about the DataFrame (shape, columns, dtypes).\n",
    "            #üìÉ Start writing data to the database.\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, Resignation_TABLE_NAME, Resignation) #üß© Call the function to write the DataFrame to SQL.\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            #üìÉ Record to summary\n",
    "            if is_new_file:\n",
    "                summary_results[\"new_files_imported\"].append(f\"[Resignation] {file_basename}\")#üß©\n",
    "            else:\n",
    "                summary_results[\"updated_files_reimported\"].append(f\"[Resignation] {file_basename}\")#üß©        \n",
    "            #üìÉ Update log_entries for successfully import: Add information to the log list for this run (successful).\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            #üìÉ Handle exceptions if any error occurred in the try block.\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception # Catch various types of potential errors.\n",
    "        ) as e:\n",
    "            error_count += 1 # Increment the error counter.        \n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"  # Get a short error message (the first line).\n",
    "            #üìÉ Update Failed file to Summary\n",
    "            summary_results[\"failed_imports\"].append((f\"[Resignation] {file_basename}\", error_msg_short))#üß©\n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True) # Log the full error traceback.\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            # Add information to the log list for this run (failed).\n",
    "            # Note: ModifiedDate here is previous_modified_date, as this file wasn't successfully imported with current_modified_date.\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: #üìÉ If the file is not new and its modification date hasn't changed.\n",
    "          #üìÉ Update Skip file to Summary\n",
    "        summary_results[\"skipped_files_unchanged\"].append(f\"[Resignation] {file_basename}\")#üß©\n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "#üí° After iterating through all files in the directory.\n",
    "display_summary(\"Resignation\", error_count) #üß© Display a summary of the results (number of errors).\n",
    "process_and_save_log(log_df, log_entries, log_Resignation_path) #üß© Update the Excel log file with information\n",
    "logger.info(\"===== Processing of the Resignation data source is complete =====\") #üß© Log finish data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3e7e4c-fd0a-48d5-8ca6-801fdc2a0240",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 1Ô∏è‚É£2Ô∏è‚É£[BKN]CPI_PEGAüíæ\n",
    "logger.info(\"===== Start CPI_PEGA Process =====\")  #üß© Log the start of the data processing.\n",
    "log_df = read_or_create_log(log_CPI_PEGA_path) #üß© Read the existing Excel log file or create a new one if it doesn't exist.\n",
    "log_entries, error_count = Default_variable() #üìÉ Initialize a list to store log results for the current run and an error counter.\n",
    "#üí° Iterate through all files in the specified folder.\n",
    "for filename in Folder_CPI_PEGA.glob(\"*.csv\"): #üß© Get files in the Folder directory.\n",
    "    #üìÉ Get the current modification timestamp and filename of the current file.\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Get modification timestamp, including microseconds.\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0) # Remove microseconds for easier comparison.\n",
    "    file_basename = filename.name # Get the base name of the file (e.g., \"data_20230101.csv\").\n",
    "    #üìÉ File is being processed.\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    #üìÉ Check if this file has been logged before.\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Filter the old log_df to find an entry for this filename.\n",
    "    is_new_file = log_entry.is_empty() # If no entry is found (log_entry is empty), it's a new file.\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1)  # Initialize the previous modification date with a very old date.\n",
    "    #üìÉ If this file was processed before (exists in the log).  \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Get the modification date recorded in the log from the previous processing.\n",
    "    #üìÉ Condition to process the file:\n",
    "    # 1. The file is new (not in the log).\n",
    "    # 2. Or, the file is old (in the log), but its current modification date (current_modified_date) is more recent than the modification date recorded in the log (previous_modified_date).\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # If it's an old file but has changed (ModifiedDate is newer).\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # If it's a completely new file.\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try: # Start a try-except block to handle potential errors during the import process.\n",
    "            if not is_new_file: # If this file already exists in the DB (because it's not new and its ModifiedDate changed) then delete the old data for this file from the df.\n",
    "                delete_data(engine, CPI_PEGA_TABLE_NAME, file_basename)  #üß© Call the function to delete data.\n",
    "            #üìÉ Read data from the excel file using Polars.\n",
    "            CPI_PEGA = (pl.read_csv(filename, infer_schema_length=None) #üß© Read the entire file to infer the schema.Add two new columns to the DataFrame: FileName and ModifiedDate.\n",
    "                    .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   )) #üß©\n",
    "            #üìÉ Validate if the columns in the DataFrame match the defined schema\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(CPI_PEGA, CPI_PEGA_schema, file_basename) #üß©\n",
    "            if has_critical_error: # If critical columns are missing.\n",
    "                raise ValueError(critical_schema_error_msg) # Raise an error to stop processing this file.\n",
    "            #üìÉ Adjust the column structure.\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            CPI_PEGA = CPI_PEGA.with_columns(parse_date(pl.col(\"Day of Date\")).alias(\"Day of Date\")) #üß©\n",
    "            CPI_PEGA = CPI_PEGA.with_columns(pl.col(\"# Swivels\", \"Count of ServiceCase or Interaction\").cast(pl.Int64)) #üß©\n",
    "            CPI_PEGA = CPI_PEGA.select(CPI_PEGA_schema) #üß© Select columns in the correct order and names as per df_schema.\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(CPI_PEGA) #üß© Display information about the DataFrame (shape, columns, dtypes).\n",
    "            #üìÉ Start writing data to the database.\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, CPI_PEGA_TABLE_NAME, CPI_PEGA) #üß© Call the function to write the DataFrame to SQL.\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            #üìÉ Record to summary\n",
    "            if is_new_file:\n",
    "                summary_results[\"new_files_imported\"].append(f\"[CPI_PEGA] {file_basename}\")#üß©\n",
    "            else:\n",
    "                summary_results[\"updated_files_reimported\"].append(f\"[CPI_PEGA] {file_basename}\")#üß©    \n",
    "            #üìÉ Update log_entries for successfully import: Add information to the log list for this run (successful).\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            #üìÉ Handle exceptions if any error occurred in the try block.\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception # Catch various types of potential errors.\n",
    "        ) as e:\n",
    "            error_count += 1 # Increment the error counter.        \n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"  # Get a short error message (the first line).\n",
    "            #üìÉ Update Failed file to Summary\n",
    "            summary_results[\"failed_imports\"].append((f\"[CPI_PEGA] {file_basename}\", error_msg_short))#üß©  \n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True) # Log the full error traceback.\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            # Add information to the log list for this run (failed).\n",
    "            # Note: ModifiedDate here is previous_modified_date, as this file wasn't successfully imported with current_modified_date.\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: #üìÉ If the file is not new and its modification date hasn't changed.\n",
    "          #üìÉ Update Skip file to Summary\n",
    "        summary_results[\"skipped_files_unchanged\"].append(f\"[CPI_PEGA] {file_basename}\")#üß©\n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "#üí° After iterating through all files in the directory.\n",
    "display_summary(\"CPI_PEGA\", error_count) #üß© Display a summary of the results (number of errors).\n",
    "process_and_save_log(log_df, log_entries, log_CPI_PEGA_path) #üß© Update the Excel log file with information\n",
    "logger.info(\"===== Processing of the CPI_PEGA data source is complete =====\") #üß© Log finish data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c00afc0-01a4-4c18-a3d9-8303ba35b4e9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 1Ô∏è‚É£3Ô∏è‚É£[BKN]Staffüíæ\n",
    "logger.info(\"===== Start Staff Process =====\")  #üß© Log the start of the data processing.\n",
    "log_df = read_or_create_log(log_Staff_path) #üß© Read the existing Excel log file or create a new one if it doesn't exist.\n",
    "log_entries, error_count = Default_variable() #üìÉ Initialize a list to store log results for the current run and an error counter.\n",
    "#üí° Iterate through all files in the specified folder.\n",
    "for filename in Folder_Staff.glob(\"*.xlsx\"): #üß© Get files in the Folder directory.\n",
    "    #üìÉ Get the current modification timestamp and filename of the current file.\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Get modification timestamp, including microseconds.\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0) # Remove microseconds for easier comparison.\n",
    "    file_basename = filename.name # Get the base name of the file (e.g., \"data_20230101.csv\").\n",
    "    #üìÉ File is being processed.\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    #üìÉ Check if this file has been logged before.\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Filter the old log_df to find an entry for this filename.\n",
    "    is_new_file = log_entry.is_empty() # If no entry is found (log_entry is empty), it's a new file.\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1)  # Initialize the previous modification date with a very old date.\n",
    "    #üìÉ If this file was processed before (exists in the log).  \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Get the modification date recorded in the log from the previous processing.\n",
    "    #üìÉ Condition to process the file:\n",
    "    # 1. The file is new (not in the log).\n",
    "    # 2. Or, the file is old (in the log), but its current modification date (current_modified_date) is more recent than the modification date recorded in the log (previous_modified_date).\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # If it's an old file but has changed (ModifiedDate is newer).\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # If it's a completely new file.\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try: # Start a try-except block to handle potential errors during the import process.\n",
    "            if not is_new_file: # If this file already exists in the DB (because it's not new and its ModifiedDate changed) then delete the old data for this file from the df.\n",
    "                delete_data(engine, Staff_TABLE_NAME, file_basename)  #üß© Call the function to delete data.\n",
    "            #üìÉ Read data from the excel file using Polars.\n",
    "            Staff = (pl.read_excel(filename, infer_schema_length=None, engine=\"calamine\", sheet_name=\"Sheet1\") #üß© Read the entire file to infer the schema.Add two new columns to the DataFrame: FileName and ModifiedDate.\n",
    "                   .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   )) #üß©\n",
    "            #üìÉ Validate if the columns in the DataFrame match the defined schema\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(Staff, Staff_schema, file_basename) #üß©\n",
    "            if has_critical_error: # If critical columns are missing.\n",
    "                raise ValueError(critical_schema_error_msg) # Raise an error to stop processing this file.\n",
    "            #üìÉ Adjust the column structure.\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            Staff = Staff.with_columns(pl.col(\"Language Start Date\", \"Hire_Date\", \"PST_Start_Date\", \"Production_Start_Date\", \n",
    "                                              \"LWD\", \"Termination_Date\").cast(pl.Date))\n",
    "            Staff = Staff.select(Staff_schema) #üß© Select columns in the correct order and names as per df_schema.\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(Staff) #üß© Display information about the DataFrame (shape, columns, dtypes).\n",
    "            #üìÉ Start writing data to the database.\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, Staff_TABLE_NAME, Staff) #üß© Call the function to write the DataFrame to SQL.\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            #üìÉ Record to summary\n",
    "            if is_new_file:\n",
    "                summary_results[\"new_files_imported\"].append(f\"[Staff] {file_basename}\")#üß©\n",
    "            else:\n",
    "                summary_results[\"updated_files_reimported\"].append(f\"[Staff] {file_basename}\")#üß©     \n",
    "            #üìÉ Update log_entries for successfully import: Add information to the log list for this run (successful).\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            #üìÉ Handle exceptions if any error occurred in the try block.\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception # Catch various types of potential errors.\n",
    "        ) as e:\n",
    "            error_count += 1 # Increment the error counter.        \n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"  # Get a short error message (the first line).\n",
    "            #üìÉ Update Failed file to Summary\n",
    "            summary_results[\"failed_imports\"].append((f\"[Staff] {file_basename}\", error_msg_short))#üß©        \n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True) # Log the full error traceback.\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            # Add information to the log list for this run (failed).\n",
    "            # Note: ModifiedDate here is previous_modified_date, as this file wasn't successfully imported with current_modified_date.\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: #üìÉ If the file is not new and its modification date hasn't changed.\n",
    "          #üìÉ Update Skip file to Summary\n",
    "        summary_results[\"skipped_files_unchanged\"].append(f\"[Staff] {file_basename}\")#üß©      \n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "#üí° After iterating through all files in the directory.\n",
    "display_summary(\"Staff\", error_count) #üß© Display a summary of the results (number of errors).\n",
    "process_and_save_log(log_df, log_entries, log_Staff_path) #üß© Update the Excel log file with information\n",
    "logger.info(\"===== Processing of the Staff data source is complete =====\") #üß© Log finish data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbc1428-f85c-434d-b7ab-578e9939c33f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 1Ô∏è‚É£4Ô∏è‚É£[BKN]ConTracküíæ\n",
    "logger.info(\"===== Start ConTrack Process =====\")  #üß© Log the start of the data processing.\n",
    "log_df = read_or_create_log(log_ConTrack_path) #üß© Read the existing Excel log file or create a new one if it doesn't exist.\n",
    "log_entries, error_count = Default_variable() #üìÉ Initialize a list to store log results for the current run and an error counter.\n",
    "#üí° Iterate through all files in the specified folder.\n",
    "for filename in Folder_ConTrack.glob(\"*.xlsx\"): #üß© Get files in the Folder directory.\n",
    "    #üìÉ Get the current modification timestamp and filename of the current file.\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Get modification timestamp, including microseconds.\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0) # Remove microseconds for easier comparison.\n",
    "    file_basename = filename.name # Get the base name of the file (e.g., \"data_20230101.csv\").\n",
    "    #üìÉ File is being processed.\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    #üìÉ Check if this file has been logged before.\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Filter the old log_df to find an entry for this filename.\n",
    "    is_new_file = log_entry.is_empty() # If no entry is found (log_entry is empty), it's a new file.\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1)  # Initialize the previous modification date with a very old date.\n",
    "    #üìÉ If this file was processed before (exists in the log).  \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Get the modification date recorded in the log from the previous processing.\n",
    "    #üìÉ Condition to process the file:\n",
    "    # 1. The file is new (not in the log).\n",
    "    # 2. Or, the file is old (in the log), but its current modification date (current_modified_date) is more recent than the modification date recorded in the log (previous_modified_date).\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # If it's an old file but has changed (ModifiedDate is newer).\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # If it's a completely new file.\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try: # Start a try-except block to handle potential errors during the import process.\n",
    "            if not is_new_file: # If this file already exists in the DB (because it's not new and its ModifiedDate changed) then delete the old data for this file from the df.\n",
    "                delete_data(engine, ConTrack_TABLE_NAME, file_basename)  #üß© Call the function to delete data.\n",
    "            #üìÉ Read data from the excel file using Polars.\n",
    "            ConTrack = (pl.read_excel(filename, infer_schema_length=1000, sheet_name=\"Sheet1\",\n",
    "                         schema_overrides={\"Reservation Number\": pl.String, \"Note\": pl.String, \n",
    "                                           \"Reason If Skipped\": pl.String}) #üß© Read the entire file to infer the schema.Add two new columns to the DataFrame: FileName and ModifiedDate.\n",
    "                   .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   )) #üß©\n",
    "            #üìÉ Validate if the columns in the DataFrame match the defined schema\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(ConTrack, ConTrack_schema, file_basename) #üß©\n",
    "            if has_critical_error: # If critical columns are missing.\n",
    "                raise ValueError(critical_schema_error_msg) # Raise an error to stop processing this file.\n",
    "            #üìÉ Adjust the column structure.\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            ConTrack = ConTrack.with_columns(pl.col(\"Start time\", \"Completion time\").cast(pl.Datetime)) #üß©\n",
    "            ConTrack = ConTrack.select(ConTrack_schema) #üß© Select columns in the correct order and names as per df_schema.\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(ConTrack) #üß© Display information about the DataFrame (shape, columns, dtypes).\n",
    "            #üìÉ Start writing data to the database.\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, ConTrack_TABLE_NAME, ConTrack) #üß© Call the function to write the DataFrame to SQL.\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            #üìÉ Record to summary\n",
    "            if is_new_file:\n",
    "                summary_results[\"new_files_imported\"].append(f\"[ConTrack] {file_basename}\")#üß©\n",
    "            else:\n",
    "                summary_results[\"updated_files_reimported\"].append(f\"[ConTrack] {file_basename}\")#üß© \n",
    "            #üìÉ Update log_entries for successfully import: Add information to the log list for this run (successful).\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            #üìÉ Handle exceptions if any error occurred in the try block.\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception # Catch various types of potential errors.\n",
    "        ) as e:\n",
    "            error_count += 1 # Increment the error counter.        \n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"  # Get a short error message (the first line).\n",
    "            #üìÉ Update Failed file to Summary\n",
    "            summary_results[\"failed_imports\"].append((f\"[ConTrack] {file_basename}\", error_msg_short))#üß©       \n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True) # Log the full error traceback.\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            # Add information to the log list for this run (failed).\n",
    "            # Note: ModifiedDate here is previous_modified_date, as this file wasn't successfully imported with current_modified_date.\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: #üìÉ If the file is not new and its modification date hasn't changed.\n",
    "          #üìÉ Update Skip file to Summary\n",
    "        summary_results[\"skipped_files_unchanged\"].append(f\"[ConTrack] {file_basename}\")#üß©        \n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "#üí° After iterating through all files in the directory.\n",
    "display_summary(\"ConTrack\", error_count) #üß© Display a summary of the results (number of errors).\n",
    "process_and_save_log(log_df, log_entries, log_ConTrack_path) #üß© Update the Excel log file with information\n",
    "logger.info(\"===== Processing of the ConTrack data source is complete =====\") #üß© Log finish data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c86925-ee19-44ee-9810-1971b64f54ae",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 1Ô∏è‚É£5Ô∏è‚É£[BKN]Qualityüíæ\n",
    "logger.info(\"===== Start Quality Process =====\")  #üß© Log the start of the data processing.\n",
    "log_df = read_or_create_log(log_Quality_path) #üß© Read the existing Excel log file or create a new one if it doesn't exist.\n",
    "log_entries, error_count = Default_variable() #üìÉ Initialize a list to store log results for the current run and an error counter.\n",
    "#üí° Iterate through all files in the specified folder.\n",
    "for filename in Folder_Quality.glob(\"*.xlsx\"): #üß© Get files in the Folder directory.\n",
    "    #üìÉ Get the current modification timestamp and filename of the current file.\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Get modification timestamp, including microseconds.\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0) # Remove microseconds for easier comparison.\n",
    "    file_basename = filename.name # Get the base name of the file (e.g., \"data_20230101.csv\").\n",
    "    #üìÉ File is being processed.\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    #üìÉ Check if this file has been logged before.\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Filter the old log_df to find an entry for this filename.\n",
    "    is_new_file = log_entry.is_empty() # If no entry is found (log_entry is empty), it's a new file.\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1)  # Initialize the previous modification date with a very old date.\n",
    "    #üìÉ If this file was processed before (exists in the log).  \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Get the modification date recorded in the log from the previous processing.\n",
    "    #üìÉ Condition to process the file:\n",
    "    # 1. The file is new (not in the log).\n",
    "    # 2. Or, the file is old (in the log), but its current modification date (current_modified_date) is more recent than the modification date recorded in the log (previous_modified_date).\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # If it's an old file but has changed (ModifiedDate is newer).\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # If it's a completely new file.\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try: # Start a try-except block to handle potential errors during the import process.\n",
    "            if not is_new_file: # If this file already exists in the DB (because it's not new and its ModifiedDate changed) then delete the old data for this file from the df.\n",
    "                delete_data(engine, Quality_TABLE_NAME, file_basename)  #üß© Call the function to delete data.\n",
    "            #üìÉ Read data from the excel file using Polars.\n",
    "            Quality = (pl.read_excel(filename, infer_schema_length=None, engine=\"calamine\", sheet_name=\"Sheet1\") #üß© Read the entire file to infer the schema.Add two new columns to the DataFrame: FileName and ModifiedDate.\n",
    "                   .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   ).rename({' score_question_weight': 'score_question_weight'})) #üß©\n",
    "            #üìÉ Validate if the columns in the DataFrame match the defined schema\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(Quality, Quality_schema, file_basename) #üß©\n",
    "            if has_critical_error: # If critical columns are missing.\n",
    "                raise ValueError(critical_schema_error_msg) # Raise an error to stop processing this file.\n",
    "            #üìÉ Adjust the column structure.\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            Quality = Quality.with_columns(pl.col(\"eval_date\").cast(pl.Date)) #üß©\n",
    "            Quality = Quality.with_columns(pl.col(\"score_n\",\"score_question_weight\").cast(pl.Int64)) #üß©\n",
    "            Quality = Quality.select(Quality_schema) #üß© Select columns in the correct order and names as per df_schema.\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(Quality) #üß© Display information about the DataFrame (shape, columns, dtypes).\n",
    "            #üìÉ Start writing data to the database.\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, Quality_TABLE_NAME, Quality) #üß© Call the function to write the DataFrame to SQL.\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            #üìÉ Record to summary\n",
    "            if is_new_file:\n",
    "                summary_results[\"new_files_imported\"].append(f\"[Quality] {file_basename}\")#üß©\n",
    "            else:\n",
    "                summary_results[\"updated_files_reimported\"].append(f\"[Quality] {file_basename}\")#üß©          \n",
    "            #üìÉ Update log_entries for successfully import: Add information to the log list for this run (successful).\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            #üìÉ Handle exceptions if any error occurred in the try block.\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception # Catch various types of potential errors.\n",
    "        ) as e:\n",
    "            error_count += 1 # Increment the error counter.        \n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"  # Get a short error message (the first line).\n",
    "            #üìÉ Update Failed file to Summary\n",
    "            summary_results[\"failed_imports\"].append((f\"[Quality] {file_basename}\", error_msg_short))#üß©           \n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True) # Log the full error traceback.\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            # Add information to the log list for this run (failed).\n",
    "            # Note: ModifiedDate here is previous_modified_date, as this file wasn't successfully imported with current_modified_date.\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: #üìÉ If the file is not new and its modification date hasn't changed.\n",
    "          #üìÉ Update Skip file to Summary\n",
    "        summary_results[\"skipped_files_unchanged\"].append(f\"[Quality] {file_basename}\")#üß©\n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "#üí° After iterating through all files in the directory.\n",
    "display_summary(\"Quality\", error_count) #üß© Display a summary of the results (number of errors).\n",
    "process_and_save_log(log_df, log_entries, log_Quality_path) #üß© Update the Excel log file with information\n",
    "logger.info(\"===== Processing of the Quality data source is complete =====\") #üß© Log finish data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3f262a-62bf-4d19-9642-91c9049c0f3c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 1Ô∏è‚É£6Ô∏è‚É£[BKN]RONAüíæ\n",
    "logger.info(\"===== Start RONA Process =====\")  #üß© Log the start of the data processing.\n",
    "log_df = read_or_create_log(log_RONA_path) #üß© Read the existing Excel log file or create a new one if it doesn't exist.\n",
    "log_entries, error_count = Default_variable() #üìÉ Initialize a list to store log results for the current run and an error counter.\n",
    "#üí° Iterate through all files in the specified folder.\n",
    "for filename in Folder_RONA.glob(\"*.xlsx\"): #üß© Get files in the Folder directory.\n",
    "    #üìÉ Get the current modification timestamp and filename of the current file.\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Get modification timestamp, including microseconds.\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0) # Remove microseconds for easier comparison.\n",
    "    file_basename = filename.name # Get the base name of the file (e.g., \"data_20230101.csv\").\n",
    "    #üìÉ File is being processed.\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    #üìÉ Check if this file has been logged before.\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Filter the old log_df to find an entry for this filename.\n",
    "    is_new_file = log_entry.is_empty() # If no entry is found (log_entry is empty), it's a new file.\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1)  # Initialize the previous modification date with a very old date.\n",
    "    #üìÉ If this file was processed before (exists in the log).  \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Get the modification date recorded in the log from the previous processing.\n",
    "    #üìÉ Condition to process the file:\n",
    "    # 1. The file is new (not in the log).\n",
    "    # 2. Or, the file is old (in the log), but its current modification date (current_modified_date) is more recent than the modification date recorded in the log (previous_modified_date).\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # If it's an old file but has changed (ModifiedDate is newer).\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # If it's a completely new file.\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try: # Start a try-except block to handle potential errors during the import process.\n",
    "            if not is_new_file: # If this file already exists in the DB (because it's not new and its ModifiedDate changed) then delete the old data for this file from the df.\n",
    "                delete_data(engine, RONA_TABLE_NAME, file_basename)  #üß© Call the function to delete data.\n",
    "            #üìÉ Read data from the excel file using Polars.\n",
    "            RONA = (pl.read_excel(filename, infer_schema_length=None, engine=\"calamine\", sheet_name=\"HCM_RONA-Agent Team Historical \") #üß© Read the entire file to infer the schema.Add two new columns to the DataFrame: FileName and ModifiedDate.\n",
    "                   .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   )) #üß©\n",
    "            #üìÉ Validate if the columns in the DataFrame match the defined schema\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(RONA, RONA_schema, file_basename) #üß©\n",
    "            if has_critical_error: # If critical columns are missing.\n",
    "                raise ValueError(critical_schema_error_msg) # Raise an error to stop processing this file.\n",
    "            #üìÉ Adjust the column structure.\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            RONA = RONA.select(RONA_schema) #üß© Select columns in the correct order and names as per df_schema.\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(RONA) #üß© Display information about the DataFrame (shape, columns, dtypes).\n",
    "            #üìÉ Start writing data to the database.\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, RONA_TABLE_NAME, RONA) #üß© Call the function to write the DataFrame to SQL.\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            #üìÉ Record to summary\n",
    "            if is_new_file:\n",
    "                summary_results[\"new_files_imported\"].append(f\"[RONA] {file_basename}\")#üß©\n",
    "            else:\n",
    "                summary_results[\"updated_files_reimported\"].append(f\"[RONA] {file_basename}\")#üß©    \n",
    "            #üìÉ Update log_entries for successfully import: Add information to the log list for this run (successful).\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            #üìÉ Handle exceptions if any error occurred in the try block.\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception # Catch various types of potential errors.\n",
    "        ) as e:\n",
    "            error_count += 1 # Increment the error counter.        \n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"  # Get a short error message (the first line).\n",
    "            #üìÉ Update Failed file to Summary\n",
    "            summary_results[\"failed_imports\"].append((f\"[RONA] {file_basename}\", error_msg_short))#üß©\n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True) # Log the full error traceback.\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            # Add information to the log list for this run (failed).\n",
    "            # Note: ModifiedDate here is previous_modified_date, as this file wasn't successfully imported with current_modified_date.\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: #üìÉ If the file is not new and its modification date hasn't changed.\n",
    "          #üìÉ Update Skip file to Summary\n",
    "        summary_results[\"skipped_files_unchanged\"].append(f\"[RONA] {file_basename}\")#üß©\n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "#üí° After iterating through all files in the directory.\n",
    "display_summary(\"RONA\", error_count) #üß© Display a summary of the results (number of errors).\n",
    "process_and_save_log(log_df, log_entries, log_RONA_path) #üß© Update the Excel log file with information\n",
    "logger.info(\"===== Processing of the RONA data source is complete =====\") #üß© Log finish data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f031ed1f-f439-4022-b27c-02b5bd9031f8",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1Ô∏è‚É£7Ô∏è‚É£[BKN]CUICüíæ\n",
    "logger.info(\"===== Start CUIC Process =====\")  #üß© Log the start of the data processing.\n",
    "log_df = read_or_create_log(log_CUIC_path) #üß© Read the existing Excel log file or create a new one if it doesn't exist.\n",
    "log_entries, error_count = Default_variable() #üìÉ Initialize a list to store log results for the current run and an error counter.\n",
    "#üí° Iterate through all files in the specified folder.\n",
    "for filename in Folder_CUIC.glob(\"*.xlsx\"): #üß© Get files in the Folder directory.\n",
    "    #üìÉ Get the current modification timestamp and filename of the current file.\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Get modification timestamp, including microseconds.\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0) # Remove microseconds for easier comparison.\n",
    "    file_basename = filename.name # Get the base name of the file (e.g., \"data_20230101.csv\").\n",
    "    #üìÉ File is being processed.\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    #üìÉ Check if this file has been logged before.\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Filter the old log_df to find an entry for this filename.\n",
    "    is_new_file = log_entry.is_empty() # If no entry is found (log_entry is empty), it's a new file.\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1)  # Initialize the previous modification date with a very old date.\n",
    "    #üìÉ If this file was processed before (exists in the log).  \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Get the modification date recorded in the log from the previous processing.\n",
    "    #üìÉ Condition to process the file:\n",
    "    # 1. The file is new (not in the log).\n",
    "    # 2. Or, the file is old (in the log), but its current modification date (current_modified_date) is more recent than the modification date recorded in the log (previous_modified_date).\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # If it's an old file but has changed (ModifiedDate is newer).\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # If it's a completely new file.\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try: # Start a try-except block to handle potential errors during the import process.\n",
    "            if not is_new_file: # If this file already exists in the DB (because it's not new and its ModifiedDate changed) then delete the old data for this file from the df.\n",
    "                delete_data(engine, CUIC_TABLE_NAME, file_basename)  #üß© Call the function to delete data.\n",
    "            #üìÉ Read data from the excel file using Polars.\n",
    "            CUIC = (pl.read_excel(filename, infer_schema_length=None, engine=\"calamine\", sheet_name=\"Attendance Check-Sample_Attenda\") #üß© Read the entire file to infer the schema.Add two new columns to the DataFrame: FileName and ModifiedDate.\n",
    "                    .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   )) #üß©\n",
    "            #üìÉ Validate if the columns in the DataFrame match the defined schema\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(CUIC, CUIC_schema, file_basename) #üß©\n",
    "            if has_critical_error: # If critical columns are missing.\n",
    "                raise ValueError(critical_schema_error_msg) # Raise an error to stop processing this file.\n",
    "            #üìÉ Adjust the column structure.\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            CUIC = CUIC.with_columns(pl.col(\"Interval\").str.strptime(pl.Datetime, \"%m/%d/%y %I:%M:%S %p\")) #üß©\n",
    "            CUIC = CUIC.with_columns(pl.col(\"AgentAvailTime\", \"AgentLoggedOnTime\").cast(pl.Float64)) #üß©\n",
    "            CUIC = CUIC.select(CUIC_schema) #üß© Select columns in the correct order and names as per df_schema.\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(CUIC) #üß© Display information about the DataFrame (shape, columns, dtypes).\n",
    "            #üìÉ Start writing data to the database.\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, CUIC_TABLE_NAME, CUIC) #üß© Call the function to write the DataFrame to SQL.\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            #üìÉ Record to summary\n",
    "            if is_new_file:\n",
    "                summary_results[\"new_files_imported\"].append(f\"[CUIC] {file_basename}\")#üß©\n",
    "            else:\n",
    "                summary_results[\"updated_files_reimported\"].append(f\"[CUIC] {file_basename}\")#üß© \n",
    "            #üìÉ Update log_entries for successfully import: Add information to the log list for this run (successful).\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            #üìÉ Handle exceptions if any error occurred in the try block.\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception # Catch various types of potential errors.\n",
    "        ) as e:\n",
    "            error_count += 1 # Increment the error counter.        \n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"  # Get a short error message (the first line).\n",
    "            #üìÉ Update Failed file to Summary\n",
    "            summary_results[\"failed_imports\"].append((f\"[CUIC] {file_basename}\", error_msg_short))#üß©       \n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True) # Log the full error traceback.\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            # Add information to the log list for this run (failed).\n",
    "            # Note: ModifiedDate here is previous_modified_date, as this file wasn't successfully imported with current_modified_date.\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: #üìÉ If the file is not new and its modification date hasn't changed.\n",
    "          #üìÉ Update Skip file to Summary\n",
    "        summary_results[\"skipped_files_unchanged\"].append(f\"[CUIC] {file_basename}\")#üß©\n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "#üí° After iterating through all files in the directory.\n",
    "display_summary(\"CUIC\", error_count) #üß© Display a summary of the results (number of errors).\n",
    "process_and_save_log(log_df, log_entries, log_CUIC_path) #üß© Update the Excel log file with information\n",
    "logger.info(\"===== Processing of the CUIC data source is complete =====\") #üß© Log finish data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1009fa11-fceb-4a37-9be5-8452bcec3e46",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 1Ô∏è‚É£8Ô∏è‚É£[BKN]KPI_Targetüíæ\n",
    "logger.info(\"===== Start KPI_Target Process =====\")  #üß© Log the start of the data processing.\n",
    "log_df = read_or_create_log(log_KPI_Target_path) #üß© Read the existing Excel log file or create a new one if it doesn't exist.\n",
    "log_entries, error_count = Default_variable() #üìÉ Initialize a list to store log results for the current run and an error counter.\n",
    "#üí° Iterate through all files in the specified folder.\n",
    "for filename in Folder_KPI_Target.glob(\"*.xlsx\"): #üß© Get files in the Folder directory.\n",
    "    #üìÉ Get the current modification timestamp and filename of the current file.\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Get modification timestamp, including microseconds.\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0) # Remove microseconds for easier comparison.\n",
    "    file_basename = filename.name # Get the base name of the file (e.g., \"data_20230101.csv\").\n",
    "    #üìÉ File is being processed.\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    #üìÉ Check if this file has been logged before.\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Filter the old log_df to find an entry for this filename.\n",
    "    is_new_file = log_entry.is_empty() # If no entry is found (log_entry is empty), it's a new file.\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1)  # Initialize the previous modification date with a very old date.\n",
    "    #üìÉ If this file was processed before (exists in the log).  \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Get the modification date recorded in the log from the previous processing.\n",
    "    #üìÉ Condition to process the file:\n",
    "    # 1. The file is new (not in the log).\n",
    "    # 2. Or, the file is old (in the log), but its current modification date (current_modified_date) is more recent than the modification date recorded in the log (previous_modified_date).\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # If it's an old file but has changed (ModifiedDate is newer).\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # If it's a completely new file.\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try: # Start a try-except block to handle potential errors during the import process.\n",
    "            if not is_new_file: # If this file already exists in the DB (because it's not new and its ModifiedDate changed) then delete the old data for this file from the df.\n",
    "                delete_data(engine, KPI_Target_TABLE_NAME, file_basename)  #üß© Call the function to delete data.\n",
    "            #üìÉ Read data from the excel file using Polars.\n",
    "            KPI_Target = (pl.read_excel(filename, infer_schema_length=None, engine=\"calamine\", sheet_name=\"Sheet1\") #üß© Read the entire file to infer the schema.Add two new columns to the DataFrame: FileName and ModifiedDate.\n",
    "                   .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   )) #üß©\n",
    "            #üìÉ Validate if the columns in the DataFrame match the defined schema\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(KPI_Target, KPI_Target_schema, file_basename) #üß©\n",
    "            if has_critical_error: # If critical columns are missing.\n",
    "                raise ValueError(critical_schema_error_msg) # Raise an error to stop processing this file.\n",
    "            #üìÉ Adjust the column structure.\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            KPI_Target = KPI_Target.with_columns(pl.col(\"Week\").cast(pl.Int64)) #üß©\n",
    "            KPI_Target = KPI_Target.with_columns(pl.col('Overall CPH tar', 'Phone CPH tar', 'Non Phone CPH tar', 'Quality - Customer Impact tar', \n",
    "                                                        'Quality - Business Impact tar', 'Quality - Compliance Impact tar', 'Quality - Overall tar', \n",
    "                                                        'AHT Phone tar', 'AHT Non-phone tar', 'AHT Overall tar', 'Hold (phone) tar', 'AACW (phone) tar', \n",
    "                                                        'Avg Talk Time tar', 'Phone CSAT tar', 'Non phone CSAT tar', 'Overall CSAT tar', 'PSAT tar', \n",
    "                                                        'PSAT Vietnamese tar', 'PSAT English (American) tar', 'PSAT English (Great Britain) tar', 'CSAT Reso tar').cast(pl.Float64)) # Import Schemaüß©\n",
    "            KPI_Target = KPI_Target.select(KPI_Target_schema) #üß© Select columns in the correct order and names as per df_schema.\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(KPI_Target) #üß© Display information about the DataFrame (shape, columns, dtypes).\n",
    "            #üìÉ Start writing data to the database.\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, KPI_Target_TABLE_NAME, KPI_Target) #üß© Call the function to write the DataFrame to SQL.\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            #üìÉ Record to summary\n",
    "            if is_new_file:\n",
    "                summary_results[\"new_files_imported\"].append(f\"[KPI_Target] {file_basename}\")#üß©\n",
    "            else:\n",
    "                summary_results[\"updated_files_reimported\"].append(f\"[KPI_Target] {file_basename}\")#üß© \n",
    "            #üìÉ Update log_entries for successfully import: Add information to the log list for this run (successful).\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            #üìÉ Handle exceptions if any error occurred in the try block.\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception # Catch various types of potential errors.\n",
    "        ) as e:\n",
    "            error_count += 1 # Increment the error counter.        \n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"  # Get a short error message (the first line).\n",
    "            #üìÉ Update Failed file to Summary\n",
    "            summary_results[\"failed_imports\"].append((f\"[KPI_Target] {file_basename}\", error_msg_short))#üß©       \n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True) # Log the full error traceback.\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            # Add information to the log list for this run (failed).\n",
    "            # Note: ModifiedDate here is previous_modified_date, as this file wasn't successfully imported with current_modified_date.\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: #üìÉ If the file is not new and its modification date hasn't changed.\n",
    "          #üìÉ Update Skip file to Summary\n",
    "        summary_results[\"skipped_files_unchanged\"].append(f\"[KPI_Target] {file_basename}\")#üß©\n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "#üí° After iterating through all files in the directory.\n",
    "display_summary(\"KPI_Target\", error_count) #üß© Display a summary of the results (number of errors).\n",
    "process_and_save_log(log_df, log_entries, log_KPI_Target_path) #üß© Update the Excel log file with information\n",
    "logger.info(\"===== Processing of the KPI_Target data source is complete =====\") #üß© Log finish data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2db9eca-d565-4d03-8e46-757b9a88e681",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 1Ô∏è‚É£9Ô∏è‚É£[BKN]LogoutCountüíæ\n",
    "logger.info(\"===== Start LogoutCount Process =====\")  #üß© Log the start of the data processing.\n",
    "log_df = read_or_create_log(log_LogoutCount_path) #üß© Read the existing Excel log file or create a new one if it doesn't exist.\n",
    "log_entries, error_count = Default_variable() #üìÉ Initialize a list to store log results for the current run and an error counter.\n",
    "#üí° Iterate through all files in the specified folder.\n",
    "for filename in Folder_LogoutCount.glob(\"*.csv\"): #üß© Get files in the Folder directory.\n",
    "    #üìÉ Get the current modification timestamp and filename of the current file.\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Get modification timestamp, including microseconds.\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0) # Remove microseconds for easier comparison.\n",
    "    file_basename = filename.name # Get the base name of the file (e.g., \"data_20230101.csv\").\n",
    "    #üìÉ File is being processed.\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    #üìÉ Check if this file has been logged before.\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Filter the old log_df to find an entry for this filename.\n",
    "    is_new_file = log_entry.is_empty() # If no entry is found (log_entry is empty), it's a new file.\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1)  # Initialize the previous modification date with a very old date.\n",
    "    #üìÉ If this file was processed before (exists in the log).  \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Get the modification date recorded in the log from the previous processing.\n",
    "    #üìÉ Condition to process the file:\n",
    "    # 1. The file is new (not in the log).\n",
    "    # 2. Or, the file is old (in the log), but its current modification date (current_modified_date) is more recent than the modification date recorded in the log (previous_modified_date).\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # If it's an old file but has changed (ModifiedDate is newer).\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # If it's a completely new file.\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try: # Start a try-except block to handle potential errors during the import process.\n",
    "            if not is_new_file: # If this file already exists in the DB (because it's not new and its ModifiedDate changed) then delete the old data for this file from the df.\n",
    "                delete_data(engine, LogoutCount_TABLE_NAME, file_basename)  #üß© Call the function to delete data.\n",
    "            #üìÉ Read data from the excel file using Polars.\n",
    "            LogoutCount = (pl.read_csv(filename, infer_schema_length=None) #üß© Read the entire file to infer the schema.Add two new columns to the DataFrame: FileName and ModifiedDate.\n",
    "                   .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   )) #üß©\n",
    "            #üìÉ Validate if the columns in the DataFrame match the defined schema\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(LogoutCount, LogoutCount_schema, file_basename) #üß©\n",
    "            if has_critical_error: # If critical columns are missing.\n",
    "                raise ValueError(critical_schema_error_msg) # Raise an error to stop processing this file.\n",
    "            #üìÉ Adjust the column structure.\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            LogoutCount = LogoutCount.with_columns(pl.col('TimeDimension').str.strptime(pl.Date, format='%m/%d/%Y'),\n",
    "                                                   pl.col('KPI Value Formatted').cast(pl.Int64)) #üß©\n",
    "            LogoutCount = LogoutCount.select(LogoutCount_schema) #üß© Select columns in the correct order and names as per df_schema.\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(LogoutCount) #üß© Display information about the DataFrame (shape, columns, dtypes).\n",
    "            #üìÉ Start writing data to the database.\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, LogoutCount_TABLE_NAME, LogoutCount) #üß© Call the function to write the DataFrame to SQL.\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            #üìÉ Record to summary\n",
    "            if is_new_file:\n",
    "                summary_results[\"new_files_imported\"].append(f\"[LogoutCount] {file_basename}\")#üß©\n",
    "            else:\n",
    "                summary_results[\"updated_files_reimported\"].append(f\"[LogoutCount] {file_basename}\")#üß©       \n",
    "            #üìÉ Update log_entries for successfully import: Add information to the log list for this run (successful).\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            #üìÉ Handle exceptions if any error occurred in the try block.\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception # Catch various types of potential errors.\n",
    "        ) as e:\n",
    "            error_count += 1 # Increment the error counter.        \n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"  # Get a short error message (the first line).\n",
    "            #üìÉ Update Failed file to Summary\n",
    "            summary_results[\"failed_imports\"].append((f\"[LogoutCount] {file_basename}\", error_msg_short))#üß©\n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True) # Log the full error traceback.\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            # Add information to the log list for this run (failed).\n",
    "            # Note: ModifiedDate here is previous_modified_date, as this file wasn't successfully imported with current_modified_date.\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: #üìÉ If the file is not new and its modification date hasn't changed.\n",
    "          #üìÉ Update Skip file to Summary\n",
    "        summary_results[\"skipped_files_unchanged\"].append(f\"[LogoutCount] {file_basename}\")#üß© \n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "#üí° After iterating through all files in the directory.\n",
    "display_summary(\"LogoutCount\", error_count) #üß© Display a summary of the results (number of errors).\n",
    "process_and_save_log(log_df, log_entries, log_LogoutCount_path) #üß© Update the Excel log file with information\n",
    "logger.info(\"===== Processing of the LogoutCount data source is complete =====\") #üß© Log finish data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a62bcc-5a33-4d20-8daa-deb4009b42a1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 2Ô∏è‚É£0Ô∏è‚É£[BKN]WpDetailüíæ\n",
    "logger.info(\"===== Start WpDetail Process =====\")  #üß© Log the start of the data processing.\n",
    "log_df = read_or_create_log(log_WpDetail_path) #üß© Read the existing Excel log file or create a new one if it doesn't exist.\n",
    "log_entries, error_count = Default_variable() #üìÉ Initialize a list to store log results for the current run and an error counter.\n",
    "#üí° Iterate through all files in the specified folder.\n",
    "for filename in Folder_WpDetail.glob(\"*.xlsx\"): #üß© Get files in the Folder directory.\n",
    "    #üìÉ Get the current modification timestamp and filename of the current file.\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Get modification timestamp, including microseconds.\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0) # Remove microseconds for easier comparison.\n",
    "    file_basename = filename.name # Get the base name of the file (e.g., \"data_20230101.csv\").\n",
    "    #üìÉ File is being processed.\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    #üìÉ Check if this file has been logged before.\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Filter the old log_df to find an entry for this filename.\n",
    "    is_new_file = log_entry.is_empty() # If no entry is found (log_entry is empty), it's a new file.\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1)  # Initialize the previous modification date with a very old date.\n",
    "    #üìÉ If this file was processed before (exists in the log).  \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Get the modification date recorded in the log from the previous processing.\n",
    "    #üìÉ Condition to process the file:\n",
    "    # 1. The file is new (not in the log).\n",
    "    # 2. Or, the file is old (in the log), but its current modification date (current_modified_date) is more recent than the modification date recorded in the log (previous_modified_date).\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # If it's an old file but has changed (ModifiedDate is newer).\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # If it's a completely new file.\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try: # Start a try-except block to handle potential errors during the import process.\n",
    "            if not is_new_file: # If this file already exists in the DB (because it's not new and its ModifiedDate changed) then delete the old data for this file from the df.\n",
    "                delete_data(engine, WpDetail_TABLE_NAME, file_basename)  #üß© Call the function to delete data.\n",
    "            #üìÉ Read data from the excel file using Polars.\n",
    "            WpDetail = (pl.read_excel(filename, infer_schema_length=None, engine=\"calamine\", sheet_name=\"Sheet1\") #üß© Read the entire file to infer the schema.Add two new columns to the DataFrame: FileName and ModifiedDate.\n",
    "                   .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),\n",
    "                           pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   ))\n",
    "            time_cols = ['Start1', 'End1', 'Start2', 'End2']\n",
    "            expressions = []      \n",
    "            for col_name in time_cols:\n",
    "                # Bi·ªÉu th·ª©c n√†y s·∫Ω th·ª≠ nhi·ªÅu c√°ch ƒë·ªÉ chuy·ªÉn ƒë·ªïi v√† l·∫•y k·∫øt qu·∫£ h·ª£p l·ªá ƒë·∫ßu ti√™n\n",
    "                # 1. Th·ª≠ chuy·ªÉn ƒë·ªïi chu·ªói th·ªùi gian c√≥ ƒë·ªãnh d·∫°ng 12-gi·ªù (AM/PM)\n",
    "                # 2. N·∫øu th·∫•t b·∫°i, th·ª≠ √©p ki·ªÉu tr·ª±c ti·∫øp (d√†nh cho ƒë·ªãnh d·∫°ng 24h ho·∫∑c c√°c ki·ªÉu kh√°c)\n",
    "                expression = pl.coalesce(\n",
    "                    pl.col(col_name).str.strptime(pl.Time, format=\"%I:%M:%S %p\", strict=False),\n",
    "                    pl.col(col_name).cast(pl.Time, strict=False)\n",
    "                ).alias(col_name)\n",
    "                expressions.append(expression)      \n",
    "            # Apply expressions\n",
    "            WpDetail = WpDetail.with_columns(\n",
    "                pl.col('Date').cast(pl.Date),\n",
    "                *expressions\n",
    "            )                    \n",
    "            threshold = t(15, 0, 0) # Time threshold definition (15:00:00)\n",
    "            WpDetail = WpDetail.with_columns( # create Date_end\n",
    "                pl.when(pl.col(\"Start1\").cast(pl.Time) >= threshold)\n",
    "                    .then(pl.col(\"Date\").cast(pl.Date) + timedelta(days=1)) \n",
    "                    .otherwise(pl.col(\"Date\").cast(pl.Date)).alias(\"Date_end\"))\n",
    "            WpDetail = WpDetail.with_columns( # create DateTime_Start & DateTime_End\n",
    "                (pl.col(\"Date\").dt.strftime(\"%Y-%m-%d\") + \" \" + pl.col(\"Start1\").cast(str)).str.strptime(pl.Datetime, \"%Y-%m-%d %H:%M:%S\").alias(\"DateTime_Start\"),\n",
    "                (pl.col(\"Date_end\").dt.strftime(\"%Y-%m-%d\") + \" \" + pl.col(\"End1\").cast(str)).str.strptime(pl.Datetime, \"%Y-%m-%d %H:%M:%S\").alias(\"DateTime_End\"))\n",
    "            WpDetail = WpDetail.filter(pl.col(\"Start2\").is_not_null() & pl.col(\"End2\").is_not_null()) # Filter for non-null values\n",
    "            WpDetail = WpDetail.with_columns( # Create 'Date_Act_Start'\n",
    "                pl.when((pl.col(\"Start1\") >= t(15, 0, 0)) & (pl.col(\"Start2\") <= t(15, 0, 0)))\n",
    "                .then(pl.col(\"Date\") + pl.duration(days=1))  # Directly add a duration to the Date column\n",
    "                .otherwise(pl.col(\"Date\")).alias(\"Date_Act_Start\"))\n",
    "            WpDetail = WpDetail.with_columns( # Create 'DateTime_Act_Start' column\n",
    "                (pl.col(\"Date_Act_Start\").dt.strftime(\"%Y-%m-%d\") + \" \" + pl.col(\"Start2\").cast(str))\n",
    "                .str.strptime(pl.Datetime, \"%Y-%m-%d %H:%M:%S\").alias(\"DateTime_Act_Start\"))\n",
    "            WpDetail = WpDetail.with_columns( # Create 'Date_Act_End' column\n",
    "                pl.when((pl.col(\"Start1\") >= t(15, 0, 0)) & (pl.col(\"End2\") <= t(15, 0, 0)))\n",
    "                .then(pl.col(\"Date\") + pl.duration(days=1))  # Add duration directly to Date column\n",
    "                .otherwise(pl.col(\"Date\")).alias(\"Date_Act_End\"))\n",
    "            WpDetail = WpDetail.with_columns( # Create 'DateTime_Act_End' column\n",
    "                (pl.col(\"Date_Act_End\").dt.strftime(\"%Y-%m-%d\") + \" \" + pl.col(\"End2\").cast(str))\n",
    "                .str.strptime(pl.Datetime, \"%Y-%m-%d %H:%M:%S\").alias(\"DateTime_Act_End\"))\n",
    "            WpDetail = WpDetail.with_columns(( # Convert data and calculate Dur (Hrs)\n",
    "                (pl.col(\"DateTime_End\") - pl.col(\"DateTime_Start\")).dt.total_seconds() / 3600).alias(\"Dur\"))\n",
    "            WpDetail = WpDetail.with_columns(( # Convert data and calculate Act_Dur (Hrs)\n",
    "                (pl.col(\"DateTime_Act_End\") - pl.col(\"DateTime_Act_Start\")).dt.total_seconds() / 3600).alias(\"Act_Dur\"))\n",
    "            WpDetail = WpDetail.rename({'Date':'Date_Start','Start1':'Time_Start','End1':'Time_End','Schedule Act':'Action',\n",
    "                                        'Start2':'Time_Act_Start','End2':'Time_Act_End'}) #üß©\n",
    "            #üìÉ Validate if the columns in the DataFrame match the defined schema\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(WpDetail, WpDetail_schema, file_basename) #üß©\n",
    "            if has_critical_error: # If critical columns are missing.\n",
    "                raise ValueError(critical_schema_error_msg) # Raise an error to stop processing this file.\n",
    "            #üìÉ Adjust the column structure.\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            WpDetail = WpDetail.select(WpDetail_schema) #üß© Select columns in the correct order and names as per df_schema.\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(WpDetail) #üß© Display information about the DataFrame (shape, columns, dtypes).\n",
    "            #üìÉ Start writing data to the database.\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, WpDetail_TABLE_NAME, WpDetail) #üß© Call the function to write the DataFrame to SQL.\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            #üìÉ Record to summary\n",
    "            if is_new_file:\n",
    "                summary_results[\"new_files_imported\"].append(f\"[WpDetail] {file_basename}\")#üß©\n",
    "            else:\n",
    "                summary_results[\"updated_files_reimported\"].append(f\"[WpDetail] {file_basename}\")#üß© \n",
    "            #üìÉ Update log_entries for successfully import: Add information to the log list for this run (successful).\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            #üìÉ Handle exceptions if any error occurred in the try block.\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception # Catch various types of potential errors.\n",
    "        ) as e:\n",
    "            error_count += 1 # Increment the error counter.        \n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"  # Get a short error message (the first line).\n",
    "            #üìÉ Update Failed file to Summary\n",
    "            summary_results[\"failed_imports\"].append((f\"[WpDetail] {file_basename}\", error_msg_short))#üß©      \n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True) # Log the full error traceback.\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            # Add information to the log list for this run (failed).\n",
    "            # Note: ModifiedDate here is previous_modified_date, as this file wasn't successfully imported with current_modified_date.\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: #üìÉ If the file is not new and its modification date hasn't changed.\n",
    "          #üìÉ Update Skip file to Summary\n",
    "        summary_results[\"skipped_files_unchanged\"].append(f\"[WpDetail] {file_basename}\")#üß©      \n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "#üí° After iterating through all files in the directory.\n",
    "display_summary(\"WpDetail\", error_count) #üß© Display a summary of the results (number of errors).\n",
    "process_and_save_log(log_df, log_entries, log_WpDetail_path) #üß© Update the Excel log file with information\n",
    "logger.info(\"===== Processing of the WpDetail data source is complete =====\") #üß© Log finish data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7225ec7-4c62-4958-8cc5-2450e4cd3a3d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 2Ô∏è‚É£1Ô∏è‚É£[BKN]WpSummaryüíæ\n",
    "logger.info(\"===== Start WpSummary Process =====\")  #üß© Log the start of the data processing.\n",
    "log_df = read_or_create_log(log_WpSummary_path) #üß© Read the existing Excel log file or create a new one if it doesn't exist.\n",
    "log_entries, error_count = Default_variable() #üìÉ Initialize a list to store log results for the current run and an error counter.\n",
    "#üí° Iterate through all files in the specified folder.\n",
    "for filename in Folder_WpSummary.glob(\"*.xlsx\"): #üß© Get files in the Folder directory.\n",
    "    #üìÉ Get the current modification timestamp and filename of the current file.\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Get modification timestamp, including microseconds.\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0) # Remove microseconds for easier comparison.\n",
    "    file_basename = filename.name # Get the base name of the file (e.g., \"data_20230101.csv\").\n",
    "    #üìÉ File is being processed.\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    #üìÉ Check if this file has been logged before.\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Filter the old log_df to find an entry for this filename.\n",
    "    is_new_file = log_entry.is_empty() # If no entry is found (log_entry is empty), it's a new file.\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1)  # Initialize the previous modification date with a very old date.\n",
    "    #üìÉ If this file was processed before (exists in the log).  \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Get the modification date recorded in the log from the previous processing.\n",
    "    #üìÉ Condition to process the file:\n",
    "    # 1. The file is new (not in the log).\n",
    "    # 2. Or, the file is old (in the log), but its current modification date (current_modified_date) is more recent than the modification date recorded in the log (previous_modified_date).\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # If it's an old file but has changed (ModifiedDate is newer).\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # If it's a completely new file.\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try: # Start a try-except block to handle potential errors during the import process.\n",
    "            if not is_new_file: # If this file already exists in the DB (because it's not new and its ModifiedDate changed) then delete the old data for this file from the df.\n",
    "                delete_data(engine, WpSummary_TABLE_NAME, file_basename)  #üß© Call the function to delete data.\n",
    "            #üìÉ Read data from the excel file using Polars.\n",
    "            WpSummary = (pl.read_excel(filename, infer_schema_length=None, engine=\"calamine\", sheet_name=\"Sheet1\") #üß© Read the entire file to infer the schema.Add two new columns to the DataFrame: FileName and ModifiedDate.\n",
    "                    .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   )) #üß©\n",
    "            #üìÉ Validate if the columns in the DataFrame match the defined schema\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(WpSummary, WpSummary_schema, file_basename) #üß©\n",
    "            if has_critical_error: # If critical columns are missing.\n",
    "                raise ValueError(critical_schema_error_msg) # Raise an error to stop processing this file.\n",
    "            #üìÉ Adjust the column structure.\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            WpSummary = WpSummary.with_columns(pl.col(\"Date\").cast(pl.Date)) #üß©\n",
    "            WpSummary = WpSummary.with_columns(pl.col('Length', 'Percent').cast(pl.Float64)) #üß©\n",
    "            WpSummary = WpSummary.select(WpSummary_schema) #üß© Select columns in the correct order and names as per df_schema.\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(WpSummary) #üß© Display information about the DataFrame (shape, columns, dtypes).\n",
    "            #üìÉ Start writing data to the database.\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, WpSummary_TABLE_NAME, WpSummary) #üß© Call the function to write the DataFrame to SQL.\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            #üìÉ Record to summary\n",
    "            if is_new_file:\n",
    "                summary_results[\"new_files_imported\"].append(f\"[WpSummary] {file_basename}\")#üß©\n",
    "            else:\n",
    "                summary_results[\"updated_files_reimported\"].append(f\"[WpSummary] {file_basename}\")#üß©  \n",
    "            #üìÉ Update log_entries for successfully import: Add information to the log list for this run (successful).\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            #üìÉ Handle exceptions if any error occurred in the try block.\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception # Catch various types of potential errors.\n",
    "        ) as e:\n",
    "            error_count += 1 # Increment the error counter.        \n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"  # Get a short error message (the first line).\n",
    "            #üìÉ Update Failed file to Summary\n",
    "            summary_results[\"failed_imports\"].append((f\"[WpSummary] {file_basename}\", error_msg_short))#üß©          \n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True) # Log the full error traceback.\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            # Add information to the log list for this run (failed).\n",
    "            # Note: ModifiedDate here is previous_modified_date, as this file wasn't successfully imported with current_modified_date.\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: #üìÉ If the file is not new and its modification date hasn't changed.\n",
    "          #üìÉ Update Skip file to Summary\n",
    "        summary_results[\"skipped_files_unchanged\"].append(f\"[WpSummary] {file_basename}\")#üß©      \n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "#üí° After iterating through all files in the directory.\n",
    "display_summary(\"WpSummary\", error_count) #üß© Display a summary of the results (number of errors).\n",
    "process_and_save_log(log_df, log_entries, log_WpSummary_path) #üß© Update the Excel log file with information\n",
    "logger.info(\"===== Processing of the WpSummary data source is complete =====\") #üß© Log finish data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93abed96-af00-4a3c-a5da-264702c31887",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 2Ô∏è‚É£2Ô∏è‚É£[BKN]RegisteredOTüíæ\n",
    "logger.info(\"===== Start RegisteredOT Process =====\")  #üß© Log the start of the data processing.\n",
    "log_df = read_or_create_log(log_RegisteredOT_path) #üß© Read the existing Excel log file or create a new one if it doesn't exist.\n",
    "log_entries, error_count = Default_variable() #üìÉ Initialize a list to store log results for the current run and an error counter.\n",
    "#üí° Iterate through all files in the specified folder.\n",
    "for filename in Folder_RegisteredOT.glob(\"*.xlsx\"): #üß© Get files in the Folder directory.\n",
    "    #üìÉ Get the current modification timestamp and filename of the current file.\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Get modification timestamp, including microseconds.\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0) # Remove microseconds for easier comparison.\n",
    "    file_basename = filename.name # Get the base name of the file (e.g., \"data_20230101.csv\").\n",
    "    #üìÉ File is being processed.\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    #üìÉ Check if this file has been logged before.\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Filter the old log_df to find an entry for this filename.\n",
    "    is_new_file = log_entry.is_empty() # If no entry is found (log_entry is empty), it's a new file.\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1)  # Initialize the previous modification date with a very old date.\n",
    "    #üìÉ If this file was processed before (exists in the log).  \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Get the modification date recorded in the log from the previous processing.\n",
    "    #üìÉ Condition to process the file:\n",
    "    # 1. The file is new (not in the log).\n",
    "    # 2. Or, the file is old (in the log), but its current modification date (current_modified_date) is more recent than the modification date recorded in the log (previous_modified_date).\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # If it's an old file but has changed (ModifiedDate is newer).\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # If it's a completely new file.\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try: # Start a try-except block to handle potential errors during the import process.\n",
    "            if not is_new_file: # If this file already exists in the DB (because it's not new and its ModifiedDate changed) then delete the old data for this file from the df.\n",
    "                delete_data(engine, RegisteredOT_TABLE_NAME, file_basename)  #üß© Call the function to delete data.\n",
    "            #üìÉ Read data from the excel file using Polars.\n",
    "            RegisteredOT = (pl.read_excel(filename, infer_schema_length=None, engine=\"calamine\", sheet_name=\"Sheet1\") #üß© Read the entire file to infer the schema.Add two new columns to the DataFrame: FileName and ModifiedDate.\n",
    "                    .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   )) #üß©\n",
    "            #üìÉ Validate if the columns in the DataFrame match the defined schema\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(RegisteredOT, RegisteredOT_schema, file_basename) #üß©\n",
    "            if has_critical_error: # If critical columns are missing.\n",
    "                raise ValueError(critical_schema_error_msg) # Raise an error to stop processing this file.\n",
    "            #üìÉ Adjust the column structure.\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            RegisteredOT = RegisteredOT.with_columns(pl.col(\"Date\").cast(pl.Date)) #üß©\n",
    "            RegisteredOT = RegisteredOT.with_columns(pl.col(\"OT\").cast(pl.Float64)) #üß©\n",
    "            RegisteredOT = RegisteredOT.select(RegisteredOT_schema) #üß© Select columns in the correct order and names as per df_schema.\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(RegisteredOT) #üß© Display information about the DataFrame (shape, columns, dtypes).\n",
    "            #üìÉ Start writing data to the database.\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, RegisteredOT_TABLE_NAME, RegisteredOT) #üß© Call the function to write the DataFrame to SQL.\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            #üìÉ Record to summary\n",
    "            if is_new_file:\n",
    "                summary_results[\"new_files_imported\"].append(f\"[RegisteredOT] {file_basename}\")#üß©\n",
    "            else:\n",
    "                summary_results[\"updated_files_reimported\"].append(f\"[RegisteredOT] {file_basename}\")#üß©     \n",
    "            #üìÉ Update log_entries for successfully import: Add information to the log list for this run (successful).\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            #üìÉ Handle exceptions if any error occurred in the try block.\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception # Catch various types of potential errors.\n",
    "        ) as e:\n",
    "            error_count += 1 # Increment the error counter.        \n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"  # Get a short error message (the first line).\n",
    "            #üìÉ Update Failed file to Summary\n",
    "            summary_results[\"failed_imports\"].append((f\"[RegisteredOT] {file_basename}\", error_msg_short))#üß©\n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True) # Log the full error traceback.\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            # Add information to the log list for this run (failed).\n",
    "            # Note: ModifiedDate here is previous_modified_date, as this file wasn't successfully imported with current_modified_date.\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: #üìÉ If the file is not new and its modification date hasn't changed.\n",
    "          #üìÉ Update Skip file to Summary\n",
    "        summary_results[\"skipped_files_unchanged\"].append(f\"[RegisteredOT] {file_basename}\")#üß©\n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "#üí° After iterating through all files in the directory.\n",
    "display_summary(\"RegisteredOT\", error_count) #üß© Display a summary of the results (number of errors).\n",
    "process_and_save_log(log_df, log_entries, log_RegisteredOT_path) #üß© Update the Excel log file with information\n",
    "logger.info(\"===== Processing of the RegisteredOT data source is complete =====\") #üß© Log finish data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad897981-8686-4291-87b4-2785918d70be",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 2Ô∏è‚É£3Ô∏è‚É£[BKN]CSAT_TPüíæ\n",
    "logger.info(\"===== Start CSAT_TP Process =====\")  #üß© Log the start of the data processing.\n",
    "log_df = read_or_create_log(log_CSAT_TP_path) #üß© Read the existing Excel log file or create a new one if it doesn't exist.\n",
    "log_entries, error_count = Default_variable() #üìÉ Initialize a list to store log results for the current run and an error counter.\n",
    "#üí° Iterate through all files in the specified folder.\n",
    "for filename in Folder_CSAT_TP.glob(\"*.csv\"): #üß© Get files in the Folder directory.\n",
    "    #üìÉ Get the current modification timestamp and filename of the current file.\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Get modification timestamp, including microseconds.\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0) # Remove microseconds for easier comparison.\n",
    "    file_basename = filename.name # Get the base name of the file (e.g., \"data_20230101.csv\").\n",
    "    #üìÉ File is being processed.\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    #üìÉ Check if this file has been logged before.\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Filter the old log_df to find an entry for this filename.\n",
    "    is_new_file = log_entry.is_empty() # If no entry is found (log_entry is empty), it's a new file.\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1)  # Initialize the previous modification date with a very old date.\n",
    "    #üìÉ If this file was processed before (exists in the log).  \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Get the modification date recorded in the log from the previous processing.\n",
    "    #üìÉ Condition to process the file:\n",
    "    # 1. The file is new (not in the log).\n",
    "    # 2. Or, the file is old (in the log), but its current modification date (current_modified_date) is more recent than the modification date recorded in the log (previous_modified_date).\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # If it's an old file but has changed (ModifiedDate is newer).\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # If it's a completely new file.\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try: # Start a try-except block to handle potential errors during the import process.\n",
    "            if not is_new_file: # If this file already exists in the DB (because it's not new and its ModifiedDate changed) then delete the old data for this file from the df.\n",
    "                delete_data(engine, CSAT_TP_TABLE_NAME, file_basename)  #üß© Call the function to delete data.\n",
    "            #üìÉ Read data from the excel file using Polars.\n",
    "            CSAT_TP = (pl.read_csv(filename, infer_schema_length=None) #üß© Read the entire file to infer the schema.Add two new columns to the DataFrame: FileName and ModifiedDate.\n",
    "                   .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   ))       \n",
    "            CSAT_TP = (CSAT_TP.rename({'Date ': 'Date', '\"\"Comment\"\"': '\"Comment\"'})) #üß©\n",
    "            #üìÉ Validate if the columns in the DataFrame match the defined schema\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(CSAT_TP, CSAT_TP_schema, file_basename) #üß©\n",
    "            if has_critical_error: # If critical columns are missing.\n",
    "                raise ValueError(critical_schema_error_msg) # Raise an error to stop processing this file.\n",
    "            #üìÉ Adjust the column structure.\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            CSAT_TP = CSAT_TP.with_columns(\n",
    "                parse_date(pl.col(\"Sort by Dimension\")).alias(\"Sort by Dimension\"),\n",
    "                parse_date(pl.col(\"Date\")).alias(\"Date\"),\n",
    "                parse_date(pl.col(\"Max. Sort by Dimension\")).alias(\"Max. Sort by Dimension\"),\n",
    "                pl.col('Sort by Dimension (copy)').cast(pl.Float64)) #üß©\n",
    "            CSAT_TP = CSAT_TP.select(CSAT_TP_schema) #üß© Select columns in the correct order and names as per df_schema.\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(CSAT_TP) #üß© Display information about the DataFrame (shape, columns, dtypes).\n",
    "            #üìÉ Start writing data to the database.\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, CSAT_TP_TABLE_NAME, CSAT_TP) #üß© Call the function to write the DataFrame to SQL.\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            #üìÉ Record to summary\n",
    "            if is_new_file:\n",
    "                summary_results[\"new_files_imported\"].append(f\"[CSAT_TP] {file_basename}\")#üß©\n",
    "            else:\n",
    "                summary_results[\"updated_files_reimported\"].append(f\"[CSAT_TP] {file_basename}\")#üß©          \n",
    "            #üìÉ Update log_entries for successfully import: Add information to the log list for this run (successful).\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            #üìÉ Handle exceptions if any error occurred in the try block.\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception # Catch various types of potential errors.\n",
    "        ) as e:\n",
    "            error_count += 1 # Increment the error counter.        \n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"  # Get a short error message (the first line).\n",
    "            #üìÉ Update Failed file to Summary\n",
    "            summary_results[\"failed_imports\"].append((f\"[CSAT_TP] {file_basename}\", error_msg_short))#üß©         \n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True) # Log the full error traceback.\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            # Add information to the log list for this run (failed).\n",
    "            # Note: ModifiedDate here is previous_modified_date, as this file wasn't successfully imported with current_modified_date.\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: #üìÉ If the file is not new and its modification date hasn't changed.\n",
    "          #üìÉ Update Skip file to Summary\n",
    "        summary_results[\"skipped_files_unchanged\"].append(f\"[CSAT_TP] {file_basename}\")#üß© \n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "#üí° After iterating through all files in the directory.\n",
    "display_summary(\"CSAT_TP\", error_count) #üß© Display a summary of the results (number of errors).\n",
    "process_and_save_log(log_df, log_entries, log_CSAT_TP_path) #üß© Update the Excel log file with information\n",
    "logger.info(\"===== Processing of the CSAT_TP data source is complete =====\") #üß© Log finish data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a879047-417d-4d7a-b7f2-752a3982717d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 2Ô∏è‚É£4Ô∏è‚É£[BKN]CSAT_RSüíæ\n",
    "logger.info(\"===== Start CSAT_RS Process =====\")  #üß© Log the start of the data processing.\n",
    "log_df = read_or_create_log(log_CSAT_RS_path) #üß© Read the existing Excel log file or create a new one if it doesn't exist.\n",
    "log_entries, error_count = Default_variable() #üìÉ Initialize a list to store log results for the current run and an error counter.\n",
    "#üí° Iterate through all files in the specified folder.\n",
    "for filename in Folder_CSAT_RS.glob(\"*.csv\"): #üß© Get files in the Folder directory.\n",
    "    #üìÉ Get the current modification timestamp and filename of the current file.\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Get modification timestamp, including microseconds.\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0) # Remove microseconds for easier comparison.\n",
    "    file_basename = filename.name # Get the base name of the file (e.g., \"data_20230101.csv\").\n",
    "    #üìÉ File is being processed.\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    #üìÉ Check if this file has been logged before.\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Filter the old log_df to find an entry for this filename.\n",
    "    is_new_file = log_entry.is_empty() # If no entry is found (log_entry is empty), it's a new file.\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1)  # Initialize the previous modification date with a very old date.\n",
    "    #üìÉ If this file was processed before (exists in the log).  \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Get the modification date recorded in the log from the previous processing.\n",
    "    #üìÉ Condition to process the file:\n",
    "    # 1. The file is new (not in the log).\n",
    "    # 2. Or, the file is old (in the log), but its current modification date (current_modified_date) is more recent than the modification date recorded in the log (previous_modified_date).\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # If it's an old file but has changed (ModifiedDate is newer).\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # If it's a completely new file.\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try: # Start a try-except block to handle potential errors during the import process.\n",
    "            if not is_new_file: # If this file already exists in the DB (because it's not new and its ModifiedDate changed) then delete the old data for this file from the df.\n",
    "                delete_data(engine, CSAT_RS_TABLE_NAME, file_basename)  #üß© Call the function to delete data.\n",
    "            #üìÉ Read data from the excel file using Polars.\n",
    "            CSAT_RS = (pl.read_csv(filename, infer_schema_length=None) #üß© Read the entire file to infer the schema.Add two new columns to the DataFrame: FileName and ModifiedDate.\n",
    "                   .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   ))\n",
    "            CSAT_RS = (CSAT_RS.rename({'Date ': 'Date', '\"\"Comment\"\"': '\"Comment\"'})) #üß©\n",
    "            #üìÉ Validate if the columns in the DataFrame match the defined schema\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(CSAT_RS, CSAT_RS_schema, file_basename) #üß©\n",
    "            if has_critical_error: # If critical columns are missing.\n",
    "                raise ValueError(critical_schema_error_msg) # Raise an error to stop processing this file.\n",
    "            #üìÉ Adjust the column structure.\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            CSAT_RS = CSAT_RS.with_columns(\n",
    "                parse_date(pl.col(\"Sort by Dimension\")).alias(\"Sort by Dimension\"),\n",
    "                parse_date(pl.col(\"Max. Sort by Dimension\")).alias(\"Max. Sort by Dimension\"),\n",
    "                pl.col(\"Sort by Dimension (copy)\").cast(pl.Float64),).select(CSAT_RS_schema) #üß© Select columns in the correct order and names as per df_schema.\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(CSAT_RS) #üß© Display information about the DataFrame (shape, columns, dtypes).\n",
    "            #üìÉ Start writing data to the database.\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, CSAT_RS_TABLE_NAME, CSAT_RS) #üß© Call the function to write the DataFrame to SQL.\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            #üìÉ Record to summary\n",
    "            if is_new_file:\n",
    "                summary_results[\"new_files_imported\"].append(f\"[CSAT_RS] {file_basename}\")#üß©\n",
    "            else:\n",
    "                summary_results[\"updated_files_reimported\"].append(f\"[CSAT_RS] {file_basename}\")#üß©            \n",
    "            #üìÉ Update log_entries for successfully import: Add information to the log list for this run (successful).\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            #üìÉ Handle exceptions if any error occurred in the try block.\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception # Catch various types of potential errors.\n",
    "        ) as e:\n",
    "            error_count += 1 # Increment the error counter.        \n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"  # Get a short error message (the first line).\n",
    "            #üìÉ Update Failed file to Summary\n",
    "            summary_results[\"failed_imports\"].append((f\"[CSAT_RS] {file_basename}\", error_msg_short))#üß©    \n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True) # Log the full error traceback.\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            # Add information to the log list for this run (failed).\n",
    "            # Note: ModifiedDate here is previous_modified_date, as this file wasn't successfully imported with current_modified_date.\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: #üìÉ If the file is not new and its modification date hasn't changed.\n",
    "          #üìÉ Update Skip file to Summary\n",
    "        summary_results[\"skipped_files_unchanged\"].append(f\"[CSAT_RS] {file_basename}\")#üß©\n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "#üí° After iterating through all files in the directory.\n",
    "display_summary(\"CSAT_RS\", error_count) #üß© Display a summary of the results (number of errors).\n",
    "process_and_save_log(log_df, log_entries, log_CSAT_RS_path) #üß© Update the Excel log file with information\n",
    "logger.info(\"===== Processing of the CSAT_RS data source is complete =====\") #üß© Log finish data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024eb106-5577-4a29-af1f-bfaec9f90b7c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 2Ô∏è‚É£5Ô∏è‚É£[BKN]PSATüíæ\n",
    "logger.info(\"===== Start PSAT Process =====\")  #üß© Log the start of the data processing.\n",
    "log_df = read_or_create_log(log_PSAT_path) #üß© Read the existing Excel log file or create a new one if it doesn't exist.\n",
    "log_entries, error_count = Default_variable() #üìÉ Initialize a list to store log results for the current run and an error counter.\n",
    "#üí° Iterate through all files in the specified folder.\n",
    "for filename in Folder_PSAT.glob(\"*.csv\"): #üß© Get files in the Folder directory.\n",
    "    #üìÉ Get the current modification timestamp and filename of the current file.\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Get modification timestamp, including microseconds.\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0) # Remove microseconds for easier comparison.\n",
    "    file_basename = filename.name # Get the base name of the file (e.g., \"data_20230101.csv\").\n",
    "    #üìÉ File is being processed.\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    #üìÉ Check if this file has been logged before.\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Filter the old log_df to find an entry for this filename.\n",
    "    is_new_file = log_entry.is_empty() # If no entry is found (log_entry is empty), it's a new file.\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1)  # Initialize the previous modification date with a very old date.\n",
    "    #üìÉ If this file was processed before (exists in the log).  \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Get the modification date recorded in the log from the previous processing.\n",
    "    #üìÉ Condition to process the file:\n",
    "    # 1. The file is new (not in the log).\n",
    "    # 2. Or, the file is old (in the log), but its current modification date (current_modified_date) is more recent than the modification date recorded in the log (previous_modified_date).\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # If it's an old file but has changed (ModifiedDate is newer).\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # If it's a completely new file.\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try: # Start a try-except block to handle potential errors during the import process.\n",
    "            if not is_new_file: # If this file already exists in the DB (because it's not new and its ModifiedDate changed) then delete the old data for this file from the df.\n",
    "                delete_data(engine, PSAT_TABLE_NAME, file_basename)  #üß© Call the function to delete data.\n",
    "            #üìÉ Read data from the excel file using Polars.\n",
    "            PSAT = (pl.read_csv(filename, infer_schema_length=None) #üß© Read the entire file to infer the schema.Add two new columns to the DataFrame: FileName and ModifiedDate.\n",
    "                   .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   )) \n",
    "            PSAT = (PSAT.rename({'\"\"Comment\"\"': '\"Comment\"'})) #üß©\n",
    "            #üìÉ Validate if the columns in the DataFrame match the defined schema\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(PSAT, PSAT_schema, file_basename) #üß©\n",
    "            if has_critical_error: # If critical columns are missing.\n",
    "                raise ValueError(critical_schema_error_msg) # Raise an error to stop processing this file.\n",
    "            #üìÉ Adjust the column structure.\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            PSAT = PSAT.with_columns(\n",
    "                parse_date(pl.col(\"Sorted By Dimension\")).alias(\"Sorted By Dimension\"),\n",
    "                parse_date(pl.col(\"Date\")).alias(\"Date\"),\n",
    "                pl.col(\"Sorted BY Dimension (copy)\").cast(pl.Float64),).select(PSAT_schema) #üß© Select columns in the correct order and names as per df_schema.\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(PSAT) #üß© Display information about the DataFrame (shape, columns, dtypes).\n",
    "            #üìÉ Start writing data to the database.\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, PSAT_TABLE_NAME, PSAT) #üß© Call the function to write the DataFrame to SQL.\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            #üìÉ Record to summary\n",
    "            if is_new_file:\n",
    "                summary_results[\"new_files_imported\"].append(f\"[PSAT] {file_basename}\")#üß©\n",
    "            else:\n",
    "                summary_results[\"updated_files_reimported\"].append(f\"[PSAT] {file_basename}\")#üß©\n",
    "            #üìÉ Update log_entries for successfully import: Add information to the log list for this run (successful).\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            #üìÉ Handle exceptions if any error occurred in the try block.\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception # Catch various types of potential errors.\n",
    "        ) as e:\n",
    "            error_count += 1 # Increment the error counter.        \n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"  # Get a short error message (the first line).\n",
    "            #üìÉ Update Failed file to Summary\n",
    "            summary_results[\"failed_imports\"].append((f\"[PSAT] {file_basename}\", error_msg_short))#üß©\n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True) # Log the full error traceback.\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            # Add information to the log list for this run (failed).\n",
    "            # Note: ModifiedDate here is previous_modified_date, as this file wasn't successfully imported with current_modified_date.\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: #üìÉ If the file is not new and its modification date hasn't changed.\n",
    "          #üìÉ Update Skip file to Summary\n",
    "        summary_results[\"skipped_files_unchanged\"].append(f\"[PSAT] {file_basename}\")#üß©   \n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "#üí° After iterating through all files in the directory.\n",
    "display_summary(\"PSAT\", error_count) #üß© Display a summary of the results (number of errors).\n",
    "process_and_save_log(log_df, log_entries, log_PSAT_path) #üß© Update the Excel log file with information\n",
    "logger.info(\"===== Processing of the PSAT data source is complete =====\") #üß© Log finish data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ba7d3e-51a2-492c-a1f1-65ebc3a6ea6c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 2Ô∏è‚É£6Ô∏è‚É£[BKN]IEX_Hrsüíæ\n",
    "logger.info(\"===== Start IEX_Hrs Process =====\")  #üß© Log the start of the data processing.\n",
    "log_df = read_or_create_log(log_IEX_Hrs_path) #üß© Read the existing Excel log file or create a new one if it doesn't exist.\n",
    "log_entries, error_count = Default_variable() #üìÉ Initialize a list to store log results for the current run and an error counter.\n",
    "#üí° Iterate through all files in the specified folder.\n",
    "for filename in Folder_IEX_Hrs.glob(\"*.xlsx\"): #üß© Get files in the Folder directory.\n",
    "    #üìÉ Get the current modification timestamp and filename of the current file.\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Get modification timestamp, including microseconds.\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0) # Remove microseconds for easier comparison.\n",
    "    file_basename = filename.name # Get the base name of the file (e.g., \"data_20230101.csv\").\n",
    "    #üìÉ File is being processed.\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    #üìÉ Check if this file has been logged before.\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Filter the old log_df to find an entry for this filename.\n",
    "    is_new_file = log_entry.is_empty() # If no entry is found (log_entry is empty), it's a new file.\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1)  # Initialize the previous modification date with a very old date.\n",
    "    #üìÉ If this file was processed before (exists in the log).  \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Get the modification date recorded in the log from the previous processing.\n",
    "    #üìÉ Condition to process the file:\n",
    "    # 1. The file is new (not in the log).\n",
    "    # 2. Or, the file is old (in the log), but its current modification date (current_modified_date) is more recent than the modification date recorded in the log (previous_modified_date).\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # If it's an old file but has changed (ModifiedDate is newer).\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # If it's a completely new file.\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try: # Start a try-except block to handle potential errors during the import process.\n",
    "            if not is_new_file: # If this file already exists in the DB (because it's not new and its ModifiedDate changed) then delete the old data for this file from the df.\n",
    "                delete_data(engine, IEX_Hrs_TABLE_NAME, file_basename)  #üß© Call the function to delete data.\n",
    "            #üìÉ Read data from the excel file using Polars.\n",
    "            IEX_Hrs = (pl.read_excel(filename, infer_schema_length=None, engine=\"calamine\", sheet_name=\"Sheet1\") #üß© Read the entire file to infer the schema.Add two new columns to the DataFrame: FileName and ModifiedDate.\n",
    "                    .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   )) #üß©\n",
    "            #üìÉ Validate if the columns in the DataFrame match the defined schema\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(IEX_Hrs, IEX_Hrs_schema, file_basename) #üß©\n",
    "            if has_critical_error: # If critical columns are missing.\n",
    "                raise ValueError(critical_schema_error_msg) # Raise an error to stop processing this file.\n",
    "            #üìÉ Adjust the column structure.\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            IEX_Hrs = IEX_Hrs.with_columns(pl.col(\"VNT\", \"CET\").cast(pl.Datetime)) #üß©\n",
    "            IEX_Hrs = IEX_Hrs.with_columns(pl.col(\"HC\", \"Hour\").cast(pl.Float64)) #üß©\n",
    "            IEX_Hrs = IEX_Hrs.select(IEX_Hrs_schema) #üß© Select columns in the correct order and names as per df_schema.\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(IEX_Hrs) #üß© Display information about the DataFrame (shape, columns, dtypes).\n",
    "            #üìÉ Start writing data to the database.\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, IEX_Hrs_TABLE_NAME, IEX_Hrs) #üß© Call the function to write the DataFrame to SQL.\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            #üìÉ Record to summary\n",
    "            if is_new_file:\n",
    "                summary_results[\"new_files_imported\"].append(f\"[IEX_Hrs] {file_basename}\")#üß©\n",
    "            else:\n",
    "                summary_results[\"updated_files_reimported\"].append(f\"[IEX_Hrs] {file_basename}\")#üß©   \n",
    "            #üìÉ Update log_entries for successfully import: Add information to the log list for this run (successful).\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            #üìÉ Handle exceptions if any error occurred in the try block.\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception # Catch various types of potential errors.\n",
    "        ) as e:\n",
    "            error_count += 1 # Increment the error counter.        \n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"  # Get a short error message (the first line).\n",
    "            #üìÉ Update Failed file to Summary\n",
    "            summary_results[\"failed_imports\"].append((f\"[IEX_Hrs] {file_basename}\", error_msg_short))#üß©\n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True) # Log the full error traceback.\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            # Add information to the log list for this run (failed).\n",
    "            # Note: ModifiedDate here is previous_modified_date, as this file wasn't successfully imported with current_modified_date.\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: #üìÉ If the file is not new and its modification date hasn't changed.\n",
    "          #üìÉ Update Skip file to Summary\n",
    "        summary_results[\"skipped_files_unchanged\"].append(f\"[IEX_Hrs] {file_basename}\")#üß©\n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "#üí° After iterating through all files in the directory.\n",
    "display_summary(\"IEX_Hrs\", error_count) #üß© Display a summary of the results (number of errors).\n",
    "process_and_save_log(log_df, log_entries, log_IEX_Hrs_path) #üß© Update the Excel log file with information\n",
    "logger.info(\"===== Processing of the IEX_Hrs data source is complete =====\") #üß© Log finish data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf813105-f648-43e6-9cc2-b480d42d6cca",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 2Ô∏è‚É£7Ô∏è‚É£[BKN]IntervalReqüíæ\n",
    "logger.info(\"===== Start IntervalReq Process =====\")  #üß© Log the start of the data processing.\n",
    "log_df = read_or_create_log(log_IntervalReq_path) #üß© Read the existing Excel log file or create a new one if it doesn't exist.\n",
    "log_entries, error_count = Default_variable() #üìÉ Initialize a list to store log results for the current run and an error counter.\n",
    "#üí° Iterate through all files in the specified folder.\n",
    "for filename in Folder_IntervalReq.glob(\"*.xlsx\"): #üß© Get files in the Folder directory.\n",
    "    #üìÉ Get the current modification timestamp and filename of the current file.\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Get modification timestamp, including microseconds.\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0) # Remove microseconds for easier comparison.\n",
    "    file_basename = filename.name # Get the base name of the file (e.g., \"data_20230101.csv\").\n",
    "    #üìÉ File is being processed.\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    #üìÉ Check if this file has been logged before.\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Filter the old log_df to find an entry for this filename.\n",
    "    is_new_file = log_entry.is_empty() # If no entry is found (log_entry is empty), it's a new file.\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1)  # Initialize the previous modification date with a very old date.\n",
    "    #üìÉ If this file was processed before (exists in the log).  \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Get the modification date recorded in the log from the previous processing.\n",
    "    #üìÉ Condition to process the file:\n",
    "    # 1. The file is new (not in the log).\n",
    "    # 2. Or, the file is old (in the log), but its current modification date (current_modified_date) is more recent than the modification date recorded in the log (previous_modified_date).\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # If it's an old file but has changed (ModifiedDate is newer).\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # If it's a completely new file.\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try: # Start a try-except block to handle potential errors during the import process.\n",
    "            if not is_new_file: # If this file already exists in the DB (because it's not new and its ModifiedDate changed) then delete the old data for this file from the df.\n",
    "                delete_data(engine, IntervalReq_TABLE_NAME, file_basename)  #üß© Call the function to delete data.\n",
    "            #üìÉ Read data from the excel file using Polars.\n",
    "            IntervalReq = (pl.read_excel(filename, infer_schema_length=None, engine=\"calamine\", sheet_name=\"Sheet1\") #üß© Read the entire file to infer the schema.Add two new columns to the DataFrame: FileName and ModifiedDate.\n",
    "                    .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   )) \n",
    "            IntervalReq = IntervalReq.with_columns(pl.col('Attribute').dt.strftime('%Y-%m-%d'))\n",
    "            IntervalReq = IntervalReq.with_columns(pl.col('Forecast').dt.strftime('%H:%M:%S'))\n",
    "            IntervalReq = (\n",
    "                IntervalReq.with_columns(\n",
    "                    (pl.col(\"Value\") / 95 * 100).alias(\"delivery_requirement\"))\n",
    "                .with_columns(\n",
    "                    pl.col(\"delivery_requirement\").round(2).alias(\"Delivery_Req\")))\n",
    "            IntervalReq = IntervalReq.with_columns(\n",
    "                pl.format(\"{} {}\", pl.col(\"Attribute\"), pl.col(\"Forecast\")).alias(\"Datetime_CET\"))\n",
    "            IntervalReq = IntervalReq.with_columns(pl.col('Datetime_CET'\n",
    "                                ).str.strptime(pl.Datetime, format='%Y-%m-%d %H:%M:%S'\n",
    "                                ).dt.replace_time_zone(\"Europe/Berlin\", ambiguous=\"earliest\"\n",
    "                                ).dt.convert_time_zone(\"Asia/Bangkok\").alias(\"Datetime_VN\").dt.strftime('%Y-%m-%d %H:%M:%S')) #üß©\n",
    "            IntervalReq = IntervalReq.with_columns(pl.col('Datetime_CET'\n",
    "                                ).str.strptime(pl.Datetime, format='%Y-%m-%d %H:%M:%S').dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "                                ).with_columns(pl.col(\"Datetime_CET\").str.to_datetime(\"%Y-%m-%d %H:%M:%S\").cast(pl.Datetime)\n",
    "                                ).with_columns(pl.col(\"Datetime_VN\").str.to_datetime(\"%Y-%m-%d %H:%M:%S\").cast(pl.Datetime)) #üß©\n",
    "            #üìÉ Validate if the columns in the DataFrame match the defined schema\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(IntervalReq, IntervalReq_schema, file_basename) #üß©\n",
    "            if has_critical_error: # If critical columns are missing.\n",
    "                raise ValueError(critical_schema_error_msg) # Raise an error to stop processing this file.\n",
    "            #üìÉ Adjust the column structure.\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            IntervalReq = IntervalReq.select(IntervalReq_schema) #üß© Select columns in the correct order and names as per df_schema.\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(IntervalReq) #üß© Display information about the DataFrame (shape, columns, dtypes).\n",
    "            #üìÉ Start writing data to the database.\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, IntervalReq_TABLE_NAME, IntervalReq) #üß© Call the function to write the DataFrame to SQL.\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            #üìÉ Record to summary\n",
    "            if is_new_file:\n",
    "                summary_results[\"new_files_imported\"].append(f\"[IntervalReq] {file_basename}\")#üß©\n",
    "            else:\n",
    "                summary_results[\"updated_files_reimported\"].append(f\"[IntervalReq] {file_basename}\")#üß©\n",
    "            #üìÉ Update log_entries for successfully import: Add information to the log list for this run (successful).\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            #üìÉ Handle exceptions if any error occurred in the try block.\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception # Catch various types of potential errors.\n",
    "        ) as e:\n",
    "            error_count += 1 # Increment the error counter.        \n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"  # Get a short error message (the first line).\n",
    "            #üìÉ Update Failed file to Summary\n",
    "            summary_results[\"failed_imports\"].append((f\"[IntervalReq] {file_basename}\", error_msg_short))#üß©      \n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True) # Log the full error traceback.\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            # Add information to the log list for this run (failed).\n",
    "            # Note: ModifiedDate here is previous_modified_date, as this file wasn't successfully imported with current_modified_date.\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: #üìÉ If the file is not new and its modification date hasn't changed.\n",
    "          #üìÉ Update Skip file to Summary\n",
    "        summary_results[\"skipped_files_unchanged\"].append(f\"[IntervalReq] {file_basename}\")#üß©        \n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "#üí° After iterating through all files in the directory.\n",
    "display_summary(\"IntervalReq\", error_count) #üß© Display a summary of the results (number of errors).\n",
    "process_and_save_log(log_df, log_entries, log_IntervalReq_path) #üß© Update the Excel log file with information\n",
    "logger.info(\"===== Processing of the IntervalReq data source is complete =====\") #üß© Log finish data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f61cda-d080-487b-8c46-b5157e7eb784",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 2Ô∏è‚É£8Ô∏è‚É£[BKN]ExceptionReqüíæ\n",
    "logger.info(\"===== Start ExceptionReq Process =====\")  #üß© Log the start of the data processing.\n",
    "log_df = read_or_create_log(log_ExceptionReq_path) #üß© Read the existing Excel log file or create a new one if it doesn't exist.\n",
    "log_entries, error_count = Default_variable() #üìÉ Initialize a list to store log results for the current run and an error counter.\n",
    "#üí° Iterate through all files in the specified folder.\n",
    "for filename in Folder_ExceptionReq.glob(\"*.xlsx\"): #üß© Get files in the Folder directory.\n",
    "    #üìÉ Get the current modification timestamp and filename of the current file.\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Get modification timestamp, including microseconds.\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0) # Remove microseconds for easier comparison.\n",
    "    file_basename = filename.name # Get the base name of the file (e.g., \"data_20230101.csv\").\n",
    "    #üìÉ File is being processed.\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    #üìÉ Check if this file has been logged before.\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Filter the old log_df to find an entry for this filename.\n",
    "    is_new_file = log_entry.is_empty() # If no entry is found (log_entry is empty), it's a new file.\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1)  # Initialize the previous modification date with a very old date.\n",
    "    #üìÉ If this file was processed before (exists in the log).  \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Get the modification date recorded in the log from the previous processing.\n",
    "    #üìÉ Condition to process the file:\n",
    "    # 1. The file is new (not in the log).\n",
    "    # 2. Or, the file is old (in the log), but its current modification date (current_modified_date) is more recent than the modification date recorded in the log (previous_modified_date).\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # If it's an old file but has changed (ModifiedDate is newer).\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # If it's a completely new file.\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try: # Start a try-except block to handle potential errors during the import process.\n",
    "            if not is_new_file: # If this file already exists in the DB (because it's not new and its ModifiedDate changed) then delete the old data for this file from the df.\n",
    "                delete_data(engine, ExceptionReq_TABLE_NAME, file_basename)  #üß© Call the function to delete data.\n",
    "            #üìÉ Read data from the excel file using Polars.\n",
    "            ExceptionReq = (pl.read_excel(filename, infer_schema_length=None, engine=\"calamine\", sheet_name=\"Sheet1\") #üß© Read the entire file to infer the schema.Add two new columns to the DataFrame: FileName and ModifiedDate.\n",
    "                    .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   )) #üß©\n",
    "            #üìÉ Validate if the columns in the DataFrame match the defined schema\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(ExceptionReq, ExceptionReq_schema, file_basename) #üß©\n",
    "            if has_critical_error: # If critical columns are missing.\n",
    "                raise ValueError(critical_schema_error_msg) # Raise an error to stop processing this file.\n",
    "            #üìÉ Adjust the column structure.\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            ExceptionReq = ExceptionReq.with_columns(pl.col(\"Date (MM/DD/YYYY)\").cast(pl.Date)) #üß©\n",
    "            ExceptionReq = ExceptionReq.with_columns(pl.col(\"Exception request (Minute)\").cast(pl.Float64)) #üß©\n",
    "            ExceptionReq = ExceptionReq.select(ExceptionReq_schema) #üß© Select columns in the correct order and names as per df_schema.\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(ExceptionReq) #üß© Display information about the DataFrame (shape, columns, dtypes).\n",
    "            #üìÉ Start writing data to the database.\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, ExceptionReq_TABLE_NAME, ExceptionReq) #üß© Call the function to write the DataFrame to SQL.\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            #üìÉ Record to summary\n",
    "            if is_new_file:\n",
    "                summary_results[\"new_files_imported\"].append(f\"[ExceptionReq] {file_basename}\")#üß©\n",
    "            else:\n",
    "                summary_results[\"updated_files_reimported\"].append(f\"[ExceptionReq] {file_basename}\")#üß©    \n",
    "            #üìÉ Update log_entries for successfully import: Add information to the log list for this run (successful).\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            #üìÉ Handle exceptions if any error occurred in the try block.\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception # Catch various types of potential errors.\n",
    "        ) as e:\n",
    "            error_count += 1 # Increment the error counter.        \n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"  # Get a short error message (the first line).\n",
    "            #üìÉ Update Failed file to Summary\n",
    "            summary_results[\"failed_imports\"].append((f\"[ExceptionReq] {file_basename}\", error_msg_short))#üß©      \n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True) # Log the full error traceback.\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            # Add information to the log list for this run (failed).\n",
    "            # Note: ModifiedDate here is previous_modified_date, as this file wasn't successfully imported with current_modified_date.\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: #üìÉ If the file is not new and its modification date hasn't changed.\n",
    "          #üìÉ Update Skip file to Summary\n",
    "        summary_results[\"skipped_files_unchanged\"].append(f\"[ExceptionReq] {file_basename}\")#üß©      \n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "#üí° After iterating through all files in the directory.\n",
    "display_summary(\"ExceptionReq\", error_count) #üß© Display a summary of the results (number of errors).\n",
    "process_and_save_log(log_df, log_entries, log_ExceptionReq_path) #üß© Update the Excel log file with information\n",
    "logger.info(\"===== Processing of the ExceptionReq data source is complete =====\") #üß© Log finish data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45693229-6f62-4fcf-84f0-c10426e63947",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 2Ô∏è‚É£9Ô∏è‚É£[BKN]LTTransfersüíæ\n",
    "logger.info(\"===== Start LTTransfers Process =====\")  #üß© Log the start of the data processing.\n",
    "log_df = read_or_create_log(log_LTTransfers_path) #üß© Read the existing Excel log file or create a new one if it doesn't exist.\n",
    "log_entries, error_count = Default_variable() #üìÉ Initialize a list to store log results for the current run and an error counter.\n",
    "#üí° Iterate through all files in the specified folder.\n",
    "for filename in Folder_LTTransfers.glob(\"*.xlsx\"): #üß© Get files in the Folder directory.\n",
    "    #üìÉ Get the current modification timestamp and filename of the current file.\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Get modification timestamp, including microseconds.\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0) # Remove microseconds for easier comparison.\n",
    "    file_basename = filename.name # Get the base name of the file (e.g., \"data_20230101.csv\").\n",
    "    #üìÉ File is being processed.\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    #üìÉ Check if this file has been logged before.\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Filter the old log_df to find an entry for this filename.\n",
    "    is_new_file = log_entry.is_empty() # If no entry is found (log_entry is empty), it's a new file.\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1)  # Initialize the previous modification date with a very old date.\n",
    "    #üìÉ If this file was processed before (exists in the log).  \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Get the modification date recorded in the log from the previous processing.\n",
    "    #üìÉ Condition to process the file:\n",
    "    # 1. The file is new (not in the log).\n",
    "    # 2. Or, the file is old (in the log), but its current modification date (current_modified_date) is more recent than the modification date recorded in the log (previous_modified_date).\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # If it's an old file but has changed (ModifiedDate is newer).\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # If it's a completely new file.\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try: # Start a try-except block to handle potential errors during the import process.\n",
    "            if not is_new_file: # If this file already exists in the DB (because it's not new and its ModifiedDate changed) then delete the old data for this file from the df.\n",
    "                delete_data(engine, LTTransfers_TABLE_NAME, file_basename)  #üß© Call the function to delete data.\n",
    "            #üìÉ Read data from the excel file using Polars.\n",
    "            LTTransfers = (pl.read_excel(filename, infer_schema_length=None, engine=\"calamine\", sheet_name=\"Sheet1\") #üß© Read the entire file to infer the schema.Add two new columns to the DataFrame: FileName and ModifiedDate.\n",
    "                    .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   )) #üß©\n",
    "            #üìÉ Validate if the columns in the DataFrame match the defined schema\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(LTTransfers, LTTransfers_schema, file_basename) #üß©\n",
    "            if has_critical_error: # If critical columns are missing.\n",
    "                raise ValueError(critical_schema_error_msg) # Raise an error to stop processing this file.\n",
    "            #üìÉ Adjust the column structure.\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            LTTransfers = LTTransfers.with_columns(pl.col(\"LWD\").cast(pl.Date)) #üß©\n",
    "            LTTransfers = LTTransfers.select(LTTransfers_schema) #üß© Select columns in the correct order and names as per df_schema.\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(LTTransfers) #üß© Display information about the DataFrame (shape, columns, dtypes).\n",
    "            #üìÉ Start writing data to the database.\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, LTTransfers_TABLE_NAME, LTTransfers) #üß© Call the function to write the DataFrame to SQL.\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            #üìÉ Record to summary\n",
    "            if is_new_file:\n",
    "                summary_results[\"new_files_imported\"].append(f\"[LTTransfers] {file_basename}\")#üß©\n",
    "            else:\n",
    "                summary_results[\"updated_files_reimported\"].append(f\"[LTTransfers] {file_basename}\")#üß©     \n",
    "            #üìÉ Update log_entries for successfully import: Add information to the log list for this run (successful).\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            #üìÉ Handle exceptions if any error occurred in the try block.\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception # Catch various types of potential errors.\n",
    "        ) as e:\n",
    "            error_count += 1 # Increment the error counter.        \n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"  # Get a short error message (the first line).\n",
    "            #üìÉ Update Failed file to Summary\n",
    "            summary_results[\"failed_imports\"].append((f\"[LTTransfers] {file_basename}\", error_msg_short))#üß©    \n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True) # Log the full error traceback.\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            # Add information to the log list for this run (failed).\n",
    "            # Note: ModifiedDate here is previous_modified_date, as this file wasn't successfully imported with current_modified_date.\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: #üìÉ If the file is not new and its modification date hasn't changed.\n",
    "          #üìÉ Update Skip file to Summary\n",
    "        summary_results[\"skipped_files_unchanged\"].append(f\"[LTTransfers] {file_basename}\")#üß©\n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "#üí° After iterating through all files in the directory.\n",
    "display_summary(\"LTTransfers\", error_count) #üß© Display a summary of the results (number of errors).\n",
    "process_and_save_log(log_df, log_entries, log_LTTransfers_path) #üß© Update the Excel log file with information\n",
    "logger.info(\"===== Processing of the LTTransfers data source is complete =====\") #üß© Log finish data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5f7614-c566-4ed9-ab46-024d2b61023e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 3Ô∏è‚É£0Ô∏è‚É£[BKN]DailyReqüíæ\n",
    "logger.info(\"===== Start DailyReq Process =====\")  #üß© Log the start of the data processing.\n",
    "log_df = read_or_create_log(log_DailyReq_path) #üß© Read the existing Excel log file or create a new one if it doesn't exist.\n",
    "log_entries, error_count = Default_variable() #üìÉ Initialize a list to store log results for the current run and an error counter.\n",
    "#üí° Iterate through all files in the specified folder.\n",
    "for filename in Folder_DailyReq.glob(\"*.xlsx\"): #üß© Get files in the Folder directory.\n",
    "    #üìÉ Get the current modification timestamp and filename of the current file.\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Get modification timestamp, including microseconds.\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0) # Remove microseconds for easier comparison.\n",
    "    file_basename = filename.name # Get the base name of the file (e.g., \"data_20230101.csv\").\n",
    "    #üìÉ File is being processed.\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    #üìÉ Check if this file has been logged before.\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Filter the old log_df to find an entry for this filename.\n",
    "    is_new_file = log_entry.is_empty() # If no entry is found (log_entry is empty), it's a new file.\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1)  # Initialize the previous modification date with a very old date.\n",
    "    #üìÉ If this file was processed before (exists in the log).  \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Get the modification date recorded in the log from the previous processing.\n",
    "    #üìÉ Condition to process the file:\n",
    "    # 1. The file is new (not in the log).\n",
    "    # 2. Or, the file is old (in the log), but its current modification date (current_modified_date) is more recent than the modification date recorded in the log (previous_modified_date).\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # If it's an old file but has changed (ModifiedDate is newer).\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # If it's a completely new file.\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try: # Start a try-except block to handle potential errors during the import process.\n",
    "            if not is_new_file: # If this file already exists in the DB (because it's not new and its ModifiedDate changed) then delete the old data for this file from the df.\n",
    "                delete_data(engine, DailyReq_TABLE_NAME, file_basename)  #üß© Call the function to delete data.\n",
    "            #üìÉ Read data from the excel file using Polars.\n",
    "            DailyReq = (pl.read_excel(filename, infer_schema_length=None, engine=\"calamine\", sheet_name=\"Sheet1\") #üß© Read the entire file to infer the schema.Add two new columns to the DataFrame: FileName and ModifiedDate.\n",
    "                    .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   )) #üß©\n",
    "            #üìÉ Validate if the columns in the DataFrame match the defined schema\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(DailyReq, DailyReq_schema, file_basename) #üß©\n",
    "            if has_critical_error: # If critical columns are missing.\n",
    "                raise ValueError(critical_schema_error_msg) # Raise an error to stop processing this file.\n",
    "            #üìÉ Adjust the column structure.\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            DailyReq = DailyReq.with_columns(pl.col(\"Date\").cast(pl.Date)) #üß©\n",
    "            DailyReq = DailyReq.with_columns(pl.col(\"Daily Requirement\", \"Prod Requirement\").cast(pl.Float64)) #üß©\n",
    "            DailyReq = DailyReq.select(DailyReq_schema) #üß© Select columns in the correct order and names as per df_schema.\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(DailyReq) #üß© Display information about the DataFrame (shape, columns, dtypes).\n",
    "            #üìÉ Start writing data to the database.\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, DailyReq_TABLE_NAME, DailyReq) #üß© Call the function to write the DataFrame to SQL.\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            #üìÉ Record to summary\n",
    "            if is_new_file:\n",
    "                summary_results[\"new_files_imported\"].append(f\"[DailyReq] {file_basename}\")#üß©\n",
    "            else:\n",
    "                summary_results[\"updated_files_reimported\"].append(f\"[DailyReq] {file_basename}\")#üß©     \n",
    "            #üìÉ Update log_entries for successfully import: Add information to the log list for this run (successful).\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            #üìÉ Handle exceptions if any error occurred in the try block.\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception # Catch various types of potential errors.\n",
    "        ) as e:\n",
    "            error_count += 1 # Increment the error counter.        \n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"  # Get a short error message (the first line).\n",
    "            #üìÉ Update Failed file to Summary\n",
    "            summary_results[\"failed_imports\"].append((f\"[DailyReq] {file_basename}\", error_msg_short))#üß©            \n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True) # Log the full error traceback.\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            # Add information to the log list for this run (failed).\n",
    "            # Note: ModifiedDate here is previous_modified_date, as this file wasn't successfully imported with current_modified_date.\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: #üìÉ If the file is not new and its modification date hasn't changed.\n",
    "          #üìÉ Update Skip file to Summary\n",
    "        summary_results[\"skipped_files_unchanged\"].append(f\"[DailyReq] {file_basename}\")#üß©  \n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "#üí° After iterating through all files in the directory.\n",
    "display_summary(\"DailyReq\", error_count) #üß© Display a summary of the results (number of errors).\n",
    "process_and_save_log(log_df, log_entries, log_DailyReq_path) #üß© Update the Excel log file with information\n",
    "logger.info(\"===== Processing of the DailyReq data source is complete =====\") #üß© Log finish data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1604a80-20fb-48ec-8b4f-b0896f67fabb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 3Ô∏è‚É£1Ô∏è‚É£[BKN]ProjectedShrinküíæ\n",
    "logger.info(\"===== Start ProjectedShrink Process =====\")  #üß© Log the start of the data processing.\n",
    "log_df = read_or_create_log(log_ProjectedShrink_path) #üß© Read the existing Excel log file or create a new one if it doesn't exist.\n",
    "log_entries, error_count = Default_variable() #üìÉ Initialize a list to store log results for the current run and an error counter.\n",
    "#üí° Iterate through all files in the specified folder.\n",
    "for filename in Folder_ProjectedShrink.glob(\"*.xlsx\"): #üß© Get files in the Folder directory.\n",
    "    #üìÉ Get the current modification timestamp and filename of the current file.\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Get modification timestamp, including microseconds.\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0) # Remove microseconds for easier comparison.\n",
    "    file_basename = filename.name # Get the base name of the file (e.g., \"data_20230101.csv\").\n",
    "    #üìÉ File is being processed.\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    #üìÉ Check if this file has been logged before.\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Filter the old log_df to find an entry for this filename.\n",
    "    is_new_file = log_entry.is_empty() # If no entry is found (log_entry is empty), it's a new file.\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1)  # Initialize the previous modification date with a very old date.\n",
    "    #üìÉ If this file was processed before (exists in the log).  \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Get the modification date recorded in the log from the previous processing.\n",
    "    #üìÉ Condition to process the file:\n",
    "    # 1. The file is new (not in the log).\n",
    "    # 2. Or, the file is old (in the log), but its current modification date (current_modified_date) is more recent than the modification date recorded in the log (previous_modified_date).\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # If it's an old file but has changed (ModifiedDate is newer).\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # If it's a completely new file.\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try: # Start a try-except block to handle potential errors during the import process.\n",
    "            if not is_new_file: # If this file already exists in the DB (because it's not new and its ModifiedDate changed) then delete the old data for this file from the df.\n",
    "                delete_data(engine, ProjectedShrink_TABLE_NAME, file_basename)  #üß© Call the function to delete data.\n",
    "            #üìÉ Read data from the excel file using Polars.\n",
    "            ProjectedShrink = (pl.read_excel(filename, infer_schema_length=None, engine=\"calamine\", sheet_name=\"Sheet1\") #üß© Read the entire file to infer the schema.Add two new columns to the DataFrame: FileName and ModifiedDate.\n",
    "                    .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   )) #üß©\n",
    "            #üìÉ Validate if the columns in the DataFrame match the defined schema\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(ProjectedShrink, ProjectedShrink_schema, file_basename) #üß©\n",
    "            if has_critical_error: # If critical columns are missing.\n",
    "                raise ValueError(critical_schema_error_msg) # Raise an error to stop processing this file.\n",
    "            #üìÉ Adjust the column structure.\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            ProjectedShrink = ProjectedShrink.with_columns(pl.col(\"Week\").cast(pl.Int64)) #üß©\n",
    "            ProjectedShrink = ProjectedShrink.with_columns(pl.col(\"Ratio\").cast(pl.Float64)) #üß©\n",
    "            ProjectedShrink = ProjectedShrink.select(ProjectedShrink_schema) #üß© Select columns in the correct order and names as per df_schema.\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(ProjectedShrink) #üß© Display information about the DataFrame (shape, columns, dtypes).\n",
    "            #üìÉ Start writing data to the database.\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, ProjectedShrink_TABLE_NAME, ProjectedShrink) #üß© Call the function to write the DataFrame to SQL.\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            #üìÉ Record to summary\n",
    "            if is_new_file:\n",
    "                summary_results[\"new_files_imported\"].append(f\"[ProjectedShrink] {file_basename}\")#üß©\n",
    "            else:\n",
    "                summary_results[\"updated_files_reimported\"].append(f\"[ProjectedShrink] {file_basename}\")#üß©\n",
    "            #üìÉ Update log_entries for successfully import: Add information to the log list for this run (successful).\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            #üìÉ Handle exceptions if any error occurred in the try block.\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception # Catch various types of potential errors.\n",
    "        ) as e:\n",
    "            error_count += 1 # Increment the error counter.        \n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"  # Get a short error message (the first line).\n",
    "            #üìÉ Update Failed file to Summary\n",
    "            summary_results[\"failed_imports\"].append((f\"[ProjectedShrink] {file_basename}\", error_msg_short))#üß©\n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True) # Log the full error traceback.\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            # Add information to the log list for this run (failed).\n",
    "            # Note: ModifiedDate here is previous_modified_date, as this file wasn't successfully imported with current_modified_date.\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: #üìÉ If the file is not new and its modification date hasn't changed.\n",
    "          #üìÉ Update Skip file to Summary\n",
    "        summary_results[\"skipped_files_unchanged\"].append(f\"[ProjectedShrink] {file_basename}\")#üß©\n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "#üí° After iterating through all files in the directory.\n",
    "display_summary(\"ProjectedShrink\", error_count) #üß© Display a summary of the results (number of errors).\n",
    "process_and_save_log(log_df, log_entries, log_ProjectedShrink_path) #üß© Update the Excel log file with information\n",
    "logger.info(\"===== Processing of the ProjectedShrink data source is complete =====\") #üß© Log finish data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f608a80-bc03-4fbb-a3b5-9e32153153ec",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 3Ô∏è‚É£2Ô∏è‚É£[BKN]OTReqüíæ\n",
    "logger.info(\"===== Start OTReq Process =====\")  #üß© Log the start of the data processing.\n",
    "log_df = read_or_create_log(log_OTReq_path) #üß© Read the existing Excel log file or create a new one if it doesn't exist.\n",
    "log_entries, error_count = Default_variable() #üìÉ Initialize a list to store log results for the current run and an error counter.\n",
    "#üí° Iterate through all files in the specified folder.\n",
    "for filename in Folder_OTReq.glob(\"*.xlsx\"): #üß© Get files in the Folder directory.\n",
    "    #üìÉ Get the current modification timestamp and filename of the current file.\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Get modification timestamp, including microseconds.\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0) # Remove microseconds for easier comparison.\n",
    "    file_basename = filename.name # Get the base name of the file (e.g., \"data_20230101.csv\").\n",
    "    #üìÉ File is being processed.\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    #üìÉ Check if this file has been logged before.\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Filter the old log_df to find an entry for this filename.\n",
    "    is_new_file = log_entry.is_empty() # If no entry is found (log_entry is empty), it's a new file.\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1)  # Initialize the previous modification date with a very old date.\n",
    "    #üìÉ If this file was processed before (exists in the log).  \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Get the modification date recorded in the log from the previous processing.\n",
    "    #üìÉ Condition to process the file:\n",
    "    # 1. The file is new (not in the log).\n",
    "    # 2. Or, the file is old (in the log), but its current modification date (current_modified_date) is more recent than the modification date recorded in the log (previous_modified_date).\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # If it's an old file but has changed (ModifiedDate is newer).\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # If it's a completely new file.\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try: # Start a try-except block to handle potential errors during the import process.\n",
    "            if not is_new_file: # If this file already exists in the DB (because it's not new and its ModifiedDate changed) then delete the old data for this file from the df.\n",
    "                delete_data(engine, OTReq_TABLE_NAME, file_basename)  #üß© Call the function to delete data.\n",
    "            #üìÉ Read data from the excel file using Polars.\n",
    "            OTReq = (pl.read_excel(filename, infer_schema_length=None, engine=\"calamine\", sheet_name=\"Sheet1\") #üß© Read the entire file to infer the schema.Add two new columns to the DataFrame: FileName and ModifiedDate.\n",
    "                    .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   )) # Import Schemaüß©\n",
    "            #üìÉ Validate if the columns in the DataFrame match the defined schema\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(OTReq, OTReq_schema, file_basename) #üß©\n",
    "            if has_critical_error: # If critical columns are missing.\n",
    "                raise ValueError(critical_schema_error_msg) # Raise an error to stop processing this file.\n",
    "            #üìÉ Adjust the column structure.\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            OTReq = OTReq.with_columns(pl.col(\"Date\").cast(pl.Date)) #üß©\n",
    "            OTReq = OTReq.with_columns(pl.col(\"OT Hour\").cast(pl.Float64)) #üß©\n",
    "            OTReq = OTReq.select(OTReq_schema) #üß© Select columns in the correct order and names as per df_schema.\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(OTReq) #üß© Display information about the DataFrame (shape, columns, dtypes).\n",
    "            #üìÉ Start writing data to the database.\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, OTReq_TABLE_NAME, OTReq) #üß© Call the function to write the DataFrame to SQL.\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            #üìÉ Record to summary\n",
    "            if is_new_file:\n",
    "                summary_results[\"new_files_imported\"].append(f\"[OTReq] {file_basename}\")#üß©\n",
    "            else:\n",
    "                summary_results[\"updated_files_reimported\"].append(f\"[OTReq] {file_basename}\")#üß©   \n",
    "            #üìÉ Update log_entries for successfully import: Add information to the log list for this run (successful).\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            #üìÉ Handle exceptions if any error occurred in the try block.\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception # Catch various types of potential errors.\n",
    "        ) as e:\n",
    "            error_count += 1 # Increment the error counter.        \n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"  # Get a short error message (the first line).\n",
    "            #üìÉ Update Failed file to Summary\n",
    "            summary_results[\"failed_imports\"].append((f\"[OTReq] {file_basename}\", error_msg_short))#üß©\n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True) # Log the full error traceback.\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            # Add information to the log list for this run (failed).\n",
    "            # Note: ModifiedDate here is previous_modified_date, as this file wasn't successfully imported with current_modified_date.\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: #üìÉ If the file is not new and its modification date hasn't changed.\n",
    "          #üìÉ Update Skip file to Summary\n",
    "        summary_results[\"skipped_files_unchanged\"].append(f\"[OTReq] {file_basename}\")#üß©   \n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "#üí° After iterating through all files in the directory.\n",
    "display_summary(\"OTReq\", error_count) #üß© Display a summary of the results (number of errors).\n",
    "process_and_save_log(log_df, log_entries, log_OTReq_path) #üß© Update the Excel log file with information\n",
    "logger.info(\"===== Processing of the OTReq data source is complete =====\") #üß© Log finish data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c711a47-cecf-449d-b40c-8df925114031",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 3Ô∏è‚É£3Ô∏è‚É£[BKN]CapHCüíæ\n",
    "logger.info(\"===== Start CapHC Process =====\")  #üß© Log the start of the data processing.\n",
    "log_df = read_or_create_log(log_CapHC_path) #üß© Read the existing Excel log file or create a new one if it doesn't exist.\n",
    "log_entries, error_count = Default_variable() #üìÉ Initialize a list to store log results for the current run and an error counter.\n",
    "#üí° Iterate through all files in the specified folder.\n",
    "for filename in Folder_CapHC.glob(\"*.xlsx\"): #üß© Get files in the Folder directory.\n",
    "    #üìÉ Get the current modification timestamp and filename of the current file.\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Get modification timestamp, including microseconds.\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0) # Remove microseconds for easier comparison.\n",
    "    file_basename = filename.name # Get the base name of the file (e.g., \"data_20230101.csv\").\n",
    "    #üìÉ File is being processed.\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    #üìÉ Check if this file has been logged before.\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Filter the old log_df to find an entry for this filename.\n",
    "    is_new_file = log_entry.is_empty() # If no entry is found (log_entry is empty), it's a new file.\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1)  # Initialize the previous modification date with a very old date.\n",
    "    #üìÉ If this file was processed before (exists in the log).  \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Get the modification date recorded in the log from the previous processing.\n",
    "    #üìÉ Condition to process the file:\n",
    "    # 1. The file is new (not in the log).\n",
    "    # 2. Or, the file is old (in the log), but its current modification date (current_modified_date) is more recent than the modification date recorded in the log (previous_modified_date).\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # If it's an old file but has changed (ModifiedDate is newer).\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # If it's a completely new file.\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try: # Start a try-except block to handle potential errors during the import process.\n",
    "            if not is_new_file: # If this file already exists in the DB (because it's not new and its ModifiedDate changed) then delete the old data for this file from the df.\n",
    "                delete_data(engine, CapHC_TABLE_NAME, file_basename)  #üß© Call the function to delete data.\n",
    "            #üìÉ Read data from the excel file using Polars.\n",
    "            CapHC = (pl.read_excel(filename, infer_schema_length=None, engine=\"calamine\", sheet_name=\"Sheet1\") #üß© Read the entire file to infer the schema.Add two new columns to the DataFrame: FileName and ModifiedDate.\n",
    "                    .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   )) #üß©\n",
    "            #üìÉ Validate if the columns in the DataFrame match the defined schema\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(CapHC, CapHC_schema, file_basename) #üß©\n",
    "            if has_critical_error: # If critical columns are missing.\n",
    "                raise ValueError(critical_schema_error_msg) # Raise an error to stop processing this file.\n",
    "            #üìÉ Adjust the column structure.\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            CapHC = CapHC.with_columns(pl.col(\"Date\").cast(pl.Date)) #üß©\n",
    "            CapHC = CapHC.with_columns(pl.col(\"Client Requirement (Hours)\").cast(pl.Float64)) #üß©\n",
    "            CapHC = CapHC.select(CapHC_schema) #üß© Select columns in the correct order and names as per df_schema.\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(CapHC) #üß© Display information about the DataFrame (shape, columns, dtypes).\n",
    "            #üìÉ Start writing data to the database.\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, CapHC_TABLE_NAME, CapHC) #üß© Call the function to write the DataFrame to SQL.\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            #üìÉ Record to summary\n",
    "            if is_new_file:\n",
    "                summary_results[\"new_files_imported\"].append(f\"[CapHC] {file_basename}\")#üß©\n",
    "            else:\n",
    "                summary_results[\"updated_files_reimported\"].append(f\"[CapHC] {file_basename}\")#üß©    \n",
    "            #üìÉ Update log_entries for successfully import: Add information to the log list for this run (successful).\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            #üìÉ Handle exceptions if any error occurred in the try block.\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception # Catch various types of potential errors.\n",
    "        ) as e:\n",
    "            error_count += 1 # Increment the error counter.        \n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"  # Get a short error message (the first line).\n",
    "            #üìÉ Update Failed file to Summary\n",
    "            summary_results[\"failed_imports\"].append((f\"[CapHC] {file_basename}\", error_msg_short))#üß©\n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True) # Log the full error traceback.\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            # Add information to the log list for this run (failed).\n",
    "            # Note: ModifiedDate here is previous_modified_date, as this file wasn't successfully imported with current_modified_date.\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: #üìÉ If the file is not new and its modification date hasn't changed.\n",
    "          #üìÉ Update Skip file to Summary\n",
    "        summary_results[\"skipped_files_unchanged\"].append(f\"[CapHC] {file_basename}\")#üß©      \n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "#üí° After iterating through all files in the directory.\n",
    "display_summary(\"CapHC\", error_count) #üß© Display a summary of the results (number of errors).\n",
    "process_and_save_log(log_df, log_entries, log_CapHC_path) #üß© Update the Excel log file with information\n",
    "logger.info(\"===== Processing of the CapHC data source is complete =====\") #üß© Log finish data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ae8a55-befb-476b-b843-647f3192e582",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 3Ô∏è‚É£4Ô∏è‚É£[BKN]ProjectedHCüíæ\n",
    "logger.info(\"===== Start ProjectedHC Process =====\")  #üß© Log the start of the data processing.\n",
    "log_df = read_or_create_log(log_ProjectedHC_path) #üß© Read the existing Excel log file or create a new one if it doesn't exist.\n",
    "log_entries, error_count = Default_variable() #üìÉ Initialize a list to store log results for the current run and an error counter.\n",
    "#üí° Iterate through all files in the specified folder.\n",
    "for filename in Folder_ProjectedHC.glob(\"*.xlsm\"): #üß© Get files in the Folder directory.\n",
    "    #üìÉ Get the current modification timestamp and filename of the current file.\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Get modification timestamp, including microseconds.\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0) # Remove microseconds for easier comparison.\n",
    "    file_basename = filename.name # Get the base name of the file (e.g., \"data_20230101.csv\").\n",
    "    #üìÉ File is being processed.\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    #üìÉ Check if this file has been logged before.\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Filter the old log_df to find an entry for this filename.\n",
    "    is_new_file = log_entry.is_empty() # If no entry is found (log_entry is empty), it's a new file.\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1)  # Initialize the previous modification date with a very old date.\n",
    "    #üìÉ If this file was processed before (exists in the log).  \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Get the modification date recorded in the log from the previous processing.\n",
    "    #üìÉ Condition to process the file:\n",
    "    # 1. The file is new (not in the log).\n",
    "    # 2. Or, the file is old (in the log), but its current modification date (current_modified_date) is more recent than the modification date recorded in the log (previous_modified_date).\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # If it's an old file but has changed (ModifiedDate is newer).\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # If it's a completely new file.\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try: # Start a try-except block to handle potential errors during the import process.\n",
    "            if not is_new_file: # If this file already exists in the DB (because it's not new and its ModifiedDate changed) then delete the old data for this file from the df.\n",
    "                delete_data(engine, ProjectedHC_TABLE_NAME, file_basename)  #üß© Call the function to delete data.\n",
    "            #üìÉ Read data from the excel file using Polars.\n",
    "            ProjectedHC = (pl.read_excel(filename, infer_schema_length=None, engine=\"calamine\", sheet_name=\"rawDup\") #üß© Read the entire file to infer the schema.Add two new columns to the DataFrame: FileName and ModifiedDate.\n",
    "                    .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   )) #üß©\n",
    "            #üìÉ Validate if the columns in the DataFrame match the defined schema\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(ProjectedHC, ProjectedHC_schema, file_basename) #üß©\n",
    "            if has_critical_error: # If critical columns are missing.\n",
    "                raise ValueError(critical_schema_error_msg) # Raise an error to stop processing this file.\n",
    "            #üìÉ Adjust the column structure.\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            ProjectedHC = ProjectedHC.with_columns(pl.col(\"Date\").cast(pl.Date)) #üß©\n",
    "            ProjectedHC = ProjectedHC.with_columns(pl.col('FTE Required', 'Projected HC', 'Plan Leave', 'Actual Projected HC', '%OO', '%IO', \n",
    "                                                          'Projected HC with Shrink', 'OT', 'Leave allow for Shrink', '% Deli').cast(pl.Float64)) #üß©\n",
    "            ProjectedHC = ProjectedHC.select(ProjectedHC_schema) #üß© Select columns in the correct order and names as per df_schema.\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(ProjectedHC) #üß© Display information about the DataFrame (shape, columns, dtypes).\n",
    "            #üìÉ Start writing data to the database.\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, ProjectedHC_TABLE_NAME, ProjectedHC) #üß© Call the function to write the DataFrame to SQL.\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            #üìÉ Record to summary\n",
    "            if is_new_file:\n",
    "                summary_results[\"new_files_imported\"].append(f\"[ProjectedHC] {file_basename}\")#üß©\n",
    "            else:\n",
    "                summary_results[\"updated_files_reimported\"].append(f\"[ProjectedHC] {file_basename}\")#üß©    \n",
    "            #üìÉ Update log_entries for successfully import: Add information to the log list for this run (successful).\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            #üìÉ Handle exceptions if any error occurred in the try block.\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception # Catch various types of potential errors.\n",
    "        ) as e:\n",
    "            error_count += 1 # Increment the error counter.        \n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"  # Get a short error message (the first line).\n",
    "            #üìÉ Update Failed file to Summary\n",
    "            summary_results[\"failed_imports\"].append((f\"[ProjectedHC] {file_basename}\", error_msg_short))#üß©\n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True) # Log the full error traceback.\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            # Add information to the log list for this run (failed).\n",
    "            # Note: ModifiedDate here is previous_modified_date, as this file wasn't successfully imported with current_modified_date.\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: #üìÉ If the file is not new and its modification date hasn't changed.\n",
    "\n",
    "          #üìÉ Update Skip file to Summary\n",
    "        summary_results[\"skipped_files_unchanged\"].append(f\"[ProjectedHC] {file_basename}\")#üß©\n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "#üí° After iterating through all files in the directory.\n",
    "display_summary(\"ProjectedHC\", error_count) #üß© Display a summary of the results (number of errors).\n",
    "process_and_save_log(log_df, log_entries, log_ProjectedHC_path) #üß© Update the Excel log file with information\n",
    "logger.info(\"===== Processing of the ProjectedHC data source is complete =====\") #üß© Log finish data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8eae43-5bab-4311-84af-4a9d96d0bbc9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 3Ô∏è‚É£5Ô∏è‚É£[BKN]RampHCüíæ\n",
    "logger.info(\"===== Start RampHC Process =====\")  #üß© Log the start of the data processing.\n",
    "log_df = read_or_create_log(log_RampHC_path) #üß© Read the existing Excel log file or create a new one if it doesn't exist.\n",
    "log_entries, error_count = Default_variable() #üìÉ Initialize a list to store log results for the current run and an error counter.\n",
    "#üí° Iterate through all files in the specified folder.\n",
    "for filename in Folder_RampHC.glob(\"*.xlsx\"): #üß© Get files in the Folder directory.\n",
    "    #üìÉ Get the current modification timestamp and filename of the current file.\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Get modification timestamp, including microseconds.\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0) # Remove microseconds for easier comparison.\n",
    "    file_basename = filename.name # Get the base name of the file (e.g., \"data_20230101.csv\").\n",
    "    #üìÉ File is being processed.\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    #üìÉ Check if this file has been logged before.\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Filter the old log_df to find an entry for this filename.\n",
    "    is_new_file = log_entry.is_empty() # If no entry is found (log_entry is empty), it's a new file.\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1)  # Initialize the previous modification date with a very old date.\n",
    "    #üìÉ If this file was processed before (exists in the log).  \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Get the modification date recorded in the log from the previous processing.\n",
    "    #üìÉ Condition to process the file:\n",
    "    # 1. The file is new (not in the log).\n",
    "    # 2. Or, the file is old (in the log), but its current modification date (current_modified_date) is more recent than the modification date recorded in the log (previous_modified_date).\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # If it's an old file but has changed (ModifiedDate is newer).\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # If it's a completely new file.\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try: # Start a try-except block to handle potential errors during the import process.\n",
    "            if not is_new_file: # If this file already exists in the DB (because it's not new and its ModifiedDate changed) then delete the old data for this file from the df.\n",
    "                delete_data(engine, RampHC_TABLE_NAME, file_basename)  #üß© Call the function to delete data.\n",
    "            #üìÉ Read data from the excel file using Polars.\n",
    "            RampHC = (pl.read_excel(filename, infer_schema_length=None, engine=\"calamine\", sheet_name=\"Sheet1\") #üß© Read the entire file to infer the schema.Add two new columns to the DataFrame: FileName and ModifiedDate.\n",
    "                    .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   )) #üß©\n",
    "            #üìÉ Validate if the columns in the DataFrame match the defined schema\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(RampHC, RampHC_schema, file_basename) #üß©\n",
    "            if has_critical_error: # If critical columns are missing.\n",
    "                raise ValueError(critical_schema_error_msg) # Raise an error to stop processing this file.\n",
    "            #üìÉ Adjust the column structure.\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            RampHC = RampHC.with_columns(pl.col(\"Date\").cast(pl.Date)) #üß©\n",
    "            RampHC = RampHC.select(RampHC_schema) #üß© Select columns in the correct order and names as per df_schema.\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(RampHC) #üß© Display information about the DataFrame (shape, columns, dtypes).\n",
    "            #üìÉ Start writing data to the database.\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, RampHC_TABLE_NAME, RampHC) #üß© Call the function to write the DataFrame to SQL.\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            #üìÉ Record to summary\n",
    "            if is_new_file:\n",
    "                summary_results[\"new_files_imported\"].append(f\"[RampHC] {file_basename}\")#üß©\n",
    "            else:\n",
    "                summary_results[\"updated_files_reimported\"].append(f\"[RampHC] {file_basename}\")#üß©       \n",
    "            #üìÉ Update log_entries for successfully import: Add information to the log list for this run (successful).\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            #üìÉ Handle exceptions if any error occurred in the try block.\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception # Catch various types of potential errors.\n",
    "        ) as e:\n",
    "            error_count += 1 # Increment the error counter.        \n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"  # Get a short error message (the first line).\n",
    "            #üìÉ Update Failed file to Summary\n",
    "            summary_results[\"failed_imports\"].append((f\"[RampHC] {file_basename}\", error_msg_short))#üß©      \n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True) # Log the full error traceback.\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            # Add information to the log list for this run (failed).\n",
    "            # Note: ModifiedDate here is previous_modified_date, as this file wasn't successfully imported with current_modified_date.\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: #üìÉ If the file is not new and its modification date hasn't changed.\n",
    "          #üìÉ Update Skip file to Summary\n",
    "        summary_results[\"skipped_files_unchanged\"].append(f\"[RampHC] {file_basename}\")#üß©     \n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "#üí° After iterating through all files in the directory.\n",
    "display_summary(\"RampHC\", error_count) #üß© Display a summary of the results (number of errors).\n",
    "process_and_save_log(log_df, log_entries, log_RampHC_path) #üß© Update the Excel log file with information\n",
    "logger.info(\"===== Processing of the RampHC data source is complete =====\") #üß© Log finish data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219c5c6d-4b1a-4687-a373-891083b1bdf3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 3Ô∏è‚É£6Ô∏è‚É£[BKN]SEATüíæ\n",
    "logger.info(\"===== Start SEAT Process =====\")  #üß© Log the start of the data processing.\n",
    "log_df = read_or_create_log(log_SEAT_path) #üß© Read the existing Excel log file or create a new one if it doesn't exist.\n",
    "log_entries, error_count = Default_variable() #üìÉ Initialize a list to store log results for the current run and an error counter.\n",
    "all_seat_files = list(Folder_SEAT.glob(\"*.xlsx\")) + list(Folder_SEAT2.glob(\"*.xlsx\")) #üß© Combine folder\n",
    "#üí° Iterate through all files in the specified folder.\n",
    "for filename in all_seat_files: #üìÉ Get files in the Folder directory.\n",
    "    #üìÉ Get the current modification timestamp and filename of the current file.\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Get modification timestamp, including microseconds.\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0) # Remove microseconds for easier comparison.\n",
    "    file_basename = filename.name # Get the base name of the file (e.g., \"data_20230101.csv\").\n",
    "    #üìÉ File is being processed.\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    #üìÉ Check if this file has been logged before.\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Filter the old log_df to find an entry for this filename.\n",
    "    is_new_file = log_entry.is_empty() # If no entry is found (log_entry is empty), it's a new file.\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1)  # Initialize the previous modification date with a very old date.\n",
    "    #üìÉ If this file was processed before (exists in the log).  \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Get the modification date recorded in the log from the previous processing.\n",
    "    #üìÉ Condition to process the file:\n",
    "    # 1. The file is new (not in the log).\n",
    "    # 2. Or, the file is old (in the log), but its current modification date (current_modified_date) is more recent than the modification date recorded in the log (previous_modified_date).\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # If it's an old file but has changed (ModifiedDate is newer).\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # If it's a completely new file.\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try: # Start a try-except block to handle potential errors during the import process.\n",
    "            if not is_new_file: # If this file already exists in the DB (because it's not new and its ModifiedDate changed) then delete the old data for this file from the df.\n",
    "                delete_data(engine, SEAT_TABLE_NAME, file_basename)  #üß© Call the function to delete data.\n",
    "            #üìÉ Read data from the excel file using Polars.\n",
    "            SEAT = (pl.read_excel(filename, infer_schema_length=None, engine=\"calamine\", sheet_name=\"SEAT MAP\") #üß© Read the entire file to infer the schema.Add two new columns to the DataFrame: FileName and ModifiedDate.\n",
    "                    .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   )) #üß©\n",
    "            #üìÉ Validate if the columns in the DataFrame match the defined schema\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(SEAT, SEAT_schema, file_basename) #üß©\n",
    "            if has_critical_error: # If critical columns are missing.\n",
    "                raise ValueError(critical_schema_error_msg) # Raise an error to stop processing this file.\n",
    "            #üìÉ Adjust the column structure.\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            SEAT = SEAT.with_columns(pl.col(\"Date\").cast(pl.Date)) #üß©\n",
    "            SEAT = SEAT.select(SEAT_schema) #üß© Select columns in the correct order and names as per df_schema.\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(SEAT) #üß© Display information about the DataFrame (shape, columns, dtypes).\n",
    "            #üìÉ Start writing data to the database.\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, SEAT_TABLE_NAME, SEAT) #üß© Call the function to write the DataFrame to SQL.\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            #üìÉ Record to summary\n",
    "            if is_new_file:\n",
    "                summary_results[\"new_files_imported\"].append(f\"[SEAT] {file_basename}\")#üß©\n",
    "            else:\n",
    "                summary_results[\"updated_files_reimported\"].append(f\"[SEAT] {file_basename}\")#üß©    \n",
    "            #üìÉ Update log_entries for successfully import: Add information to the log list for this run (successful).\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            #üìÉ Handle exceptions if any error occurred in the try block.\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception # Catch various types of potential errors.\n",
    "        ) as e:\n",
    "            error_count += 1 # Increment the error counter.        \n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"  # Get a short error message (the first line).\n",
    "            #üìÉ Update Failed file to Summary\n",
    "            summary_results[\"failed_imports\"].append((f\"[SEAT] {file_basename}\", error_msg_short))#üß©   \n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True) # Log the full error traceback.\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            # Add information to the log list for this run (failed).\n",
    "            # Note: ModifiedDate here is previous_modified_date, as this file wasn't successfully imported with current_modified_date.\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: #üìÉ If the file is not new and its modification date hasn't changed.\n",
    "          #üìÉ Update Skip file to Summary\n",
    "        summary_results[\"skipped_files_unchanged\"].append(f\"[SEAT] {file_basename}\")#üß©\n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "#üí° After iterating through all files in the directory.\n",
    "display_summary(\"SEAT\", error_count) #üß© Display a summary of the results (number of errors).\n",
    "process_and_save_log(log_df, log_entries, log_SEAT_path) #üß© Update the Excel log file with information\n",
    "logger.info(\"===== Processing of the SEAT data source is complete =====\") #üß© Log finish data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f59e116-c7a7-4b05-9877-fdd1de65d46f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 3Ô∏è‚É£7Ô∏è‚É£[BKN]SEAT_AVAILüíæ\n",
    "logger.info(\"===== Start SEAT_AVAIL Process =====\")  #üß© Log the start of the data processing.\n",
    "log_df = read_or_create_log(log_SEAT_AVAIL_path) #üß© Read the existing Excel log file or create a new one if it doesn't exist.\n",
    "log_entries, error_count = Default_variable() #üìÉ Initialize a list to store log results for the current run and an error counter.\n",
    "all_seat_files = list(Folder_SEAT_AVAIL.glob(\"*.xlsx\")) + list(Folder_SEAT_AVAIL2.glob(\"*.xlsx\")) #üß© Combine folder\n",
    "#üí° Iterate through all files in the specified folder.\n",
    "for filename in all_seat_files: #üìÉ Get files in the Folder directory.\n",
    "    #üìÉ Get the current modification timestamp and filename of the current file.\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Get modification timestamp, including microseconds.\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0) # Remove microseconds for easier comparison.\n",
    "    file_basename = filename.name # Get the base name of the file (e.g., \"data_20230101.csv\").\n",
    "    #üìÉ File is being processed.\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    #üìÉ Check if this file has been logged before.\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Filter the old log_df to find an entry for this filename.\n",
    "    is_new_file = log_entry.is_empty() # If no entry is found (log_entry is empty), it's a new file.\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1)  # Initialize the previous modification date with a very old date.\n",
    "    #üìÉ If this file was processed before (exists in the log).  \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Get the modification date recorded in the log from the previous processing.\n",
    "    #üìÉ Condition to process the file:\n",
    "    # 1. The file is new (not in the log).\n",
    "    # 2. Or, the file is old (in the log), but its current modification date (current_modified_date) is more recent than the modification date recorded in the log (previous_modified_date).\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # If it's an old file but has changed (ModifiedDate is newer).\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # If it's a completely new file.\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try: # Start a try-except block to handle potential errors during the import process.\n",
    "            if not is_new_file: # If this file already exists in the DB (because it's not new and its ModifiedDate changed) then delete the old data for this file from the df.\n",
    "                delete_data(engine, SEAT_AVAIL_TABLE_NAME, file_basename)  #üß© Call the function to delete data.\n",
    "            #üìÉ Read data from the excel file using Polars.\n",
    "            SEAT_AVAIL = (pl.read_excel(filename, infer_schema_length=None, engine=\"calamine\", sheet_name=\"SEAT AVAILABLE\") #üß© Read the entire file to infer the schema.Add two new columns to the DataFrame: FileName and ModifiedDate.\n",
    "                    .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   )) #üß©\n",
    "            #üìÉ Validate if the columns in the DataFrame match the defined schema\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(SEAT_AVAIL, SEAT_AVAIL_schema, file_basename) #üß©\n",
    "            if has_critical_error: # If critical columns are missing.\n",
    "                raise ValueError(critical_schema_error_msg) # Raise an error to stop processing this file.\n",
    "            #üìÉ Adjust the column structure.\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            SEAT_AVAIL = SEAT_AVAIL.with_columns(pl.col(\"Date\").cast(pl.Date)) #üß©\n",
    "            SEAT_AVAIL = SEAT_AVAIL.select(SEAT_AVAIL_schema) #üß© Select columns in the correct order and names as per df_schema.\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(SEAT_AVAIL) #üß© Display information about the DataFrame (shape, columns, dtypes).\n",
    "            #üìÉ Start writing data to the database.\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, SEAT_AVAIL_TABLE_NAME, SEAT_AVAIL) #üß© Call the function to write the DataFrame to SQL.\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            #üìÉ Record to summary\n",
    "            if is_new_file:\n",
    "                summary_results[\"new_files_imported\"].append(f\"[SEAT_AVAIL] {file_basename}\")#üß©\n",
    "            else:\n",
    "                summary_results[\"updated_files_reimported\"].append(f\"[SEAT_AVAIL] {file_basename}\")#üß©   \n",
    "            #üìÉ Update log_entries for successfully import: Add information to the log list for this run (successful).\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            #üìÉ Handle exceptions if any error occurred in the try block.\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception # Catch various types of potential errors.\n",
    "        ) as e:\n",
    "            error_count += 1 # Increment the error counter.        \n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"  # Get a short error message (the first line).\n",
    "            #üìÉ Update Failed file to Summary\n",
    "            summary_results[\"failed_imports\"].append((f\"[SEAT_AVAIL] {file_basename}\", error_msg_short))#üß©     \n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True) # Log the full error traceback.\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            # Add information to the log list for this run (failed).\n",
    "            # Note: ModifiedDate here is previous_modified_date, as this file wasn't successfully imported with current_modified_date.\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: #üìÉ If the file is not new and its modification date hasn't changed.\n",
    "          #üìÉ Update Skip file to Summary\n",
    "        summary_results[\"skipped_files_unchanged\"].append(f\"[SEAT_AVAIL] {file_basename}\")#üß©\n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "#üí° After iterating through all files in the directory.\n",
    "display_summary(\"SEAT_AVAIL\", error_count) #üß© Display a summary of the results (number of errors).\n",
    "process_and_save_log(log_df, log_entries, log_SEAT_AVAIL_path) #üß© Update the Excel log file with information\n",
    "logger.info(\"===== Processing of the SEAT_AVAIL data source is complete =====\") #üß© Log finish data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd43ad3-1dcc-46e4-b5b0-0cf4c80bbb56",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 3Ô∏è‚É£8Ô∏è‚É£[BKN]CSAT_Compüíæ\n",
    "logger.info(\"===== Start CSAT_Comp Process =====\")  #üß© Log the start of the data processing.\n",
    "log_df = read_or_create_log(log_CSAT_Comp_path) #üß© Read the existing Excel log file or create a new one if it doesn't exist.\n",
    "log_entries, error_count = Default_variable() #üìÉ Initialize a list to store log results for the current run and an error counter.\n",
    "#üí° Iterate through all files in the specified folder.\n",
    "for filename in Folder_CSAT_Comp.glob(\"*.xlsx\"): #üß© Get files in the Folder directory.\n",
    "    #üìÉ Get the current modification timestamp and filename of the current file.\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Get modification timestamp, including microseconds.\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0) # Remove microseconds for easier comparison.\n",
    "    file_basename = filename.name # Get the base name of the file (e.g., \"data_20230101.csv\").\n",
    "    #üìÉ File is being processed.\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    #üìÉ Check if this file has been logged before.\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Filter the old log_df to find an entry for this filename.\n",
    "    is_new_file = log_entry.is_empty() # If no entry is found (log_entry is empty), it's a new file.\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1)  # Initialize the previous modification date with a very old date.\n",
    "    #üìÉ If this file was processed before (exists in the log).  \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Get the modification date recorded in the log from the previous processing.\n",
    "    #üìÉ Condition to process the file:\n",
    "    # 1. The file is new (not in the log).\n",
    "    # 2. Or, the file is old (in the log), but its current modification date (current_modified_date) is more recent than the modification date recorded in the log (previous_modified_date).\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # If it's an old file but has changed (ModifiedDate is newer).\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # If it's a completely new file.\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try: # Start a try-except block to handle potential errors during the import process.\n",
    "            if not is_new_file: # If this file already exists in the DB (because it's not new and its ModifiedDate changed) then delete the old data for this file from the df.\n",
    "                delete_data(engine, CSAT_Comp_TABLE_NAME, file_basename)  #üß© Call the function to delete data.\n",
    "            #üìÉ Read data from the excel file using Polars.\n",
    "            CSAT_Comp = (pl.read_excel(filename, infer_schema_length=None, engine=\"calamine\", sheet_name=\"Table1\") #üß© Read the entire file to infer the schema.Add two new columns to the DataFrame: FileName and ModifiedDate.\n",
    "                    .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   )) #üß©\n",
    "            #üìÉ Validate if the columns in the DataFrame match the defined schema\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(CSAT_Comp, CSAT_Comp_schema, file_basename) #üß©\n",
    "            if has_critical_error: # If critical columns are missing.\n",
    "                raise ValueError(critical_schema_error_msg) # Raise an error to stop processing this file.\n",
    "            #üìÉ Adjust the column structure.\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            CSAT_Comp = CSAT_Comp.with_columns(pl.col(\"Date\").cast(pl.Date)) #üß©\n",
    "            CSAT_Comp = CSAT_Comp.select(CSAT_Comp_schema) #üß© Select columns in the correct order and names as per df_schema.\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(CSAT_Comp) #üß© Display information about the DataFrame (shape, columns, dtypes).\n",
    "            #üìÉ Start writing data to the database.\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, CSAT_Comp_TABLE_NAME, CSAT_Comp) #üß© Call the function to write the DataFrame to SQL.\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            #üìÉ Record to summary\n",
    "            if is_new_file:\n",
    "                summary_results[\"new_files_imported\"].append(f\"[CSAT_Comp] {file_basename}\")#üß©\n",
    "            else:\n",
    "                summary_results[\"updated_files_reimported\"].append(f\"[CSAT_Comp] {file_basename}\")#üß©   \n",
    "            #üìÉ Update log_entries for successfully import: Add information to the log list for this run (successful).\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            #üìÉ Handle exceptions if any error occurred in the try block.\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception # Catch various types of potential errors.\n",
    "        ) as e:\n",
    "            error_count += 1 # Increment the error counter.        \n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"  # Get a short error message (the first line).\n",
    "            #üìÉ Update Failed file to Summary\n",
    "            summary_results[\"failed_imports\"].append((f\"[CSAT_Comp] {file_basename}\", error_msg_short))#üß©       \n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True) # Log the full error traceback.\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            # Add information to the log list for this run (failed).\n",
    "            # Note: ModifiedDate here is previous_modified_date, as this file wasn't successfully imported with current_modified_date.\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: #üìÉ If the file is not new and its modification date hasn't changed.\n",
    "          #üìÉ Update Skip file to Summary\n",
    "        summary_results[\"skipped_files_unchanged\"].append(f\"[CSAT_Comp] {file_basename}\")#üß©        \n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "#üí° After iterating through all files in the directory.\n",
    "display_summary(\"CSAT_Comp\", error_count) #üß© Display a summary of the results (number of errors).\n",
    "process_and_save_log(log_df, log_entries, log_CSAT_Comp_path) #üß© Update the Excel log file with information\n",
    "logger.info(\"===== Processing of the CSAT_Comp data source is complete =====\") #üß© Log finish data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb453290-27af-4555-8b15-f96aa9752628",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Notify & Close DBüìÉ\n",
    "# Summary\n",
    "summary_report = generate_summary_report(summary_results)\n",
    "print(summary_report)\n",
    "# Recort Summary\n",
    "logger.info(\"===== FINAL SUMMARY REPORT =====\")\n",
    "logger.info(summary_report)\n",
    "logger.info(\"================================\")\n",
    "try:\n",
    "    with open(user_credential / r'DataBase//DataRaw//BKN//import_status_log.txt', 'w', encoding='utf-8') as file:\n",
    "        file.write(summary_report)\n",
    "except IOError as e:\n",
    "    print(f\"Error creating file: {e}\")\n",
    "engine.dispose()\n",
    "print(\"Database connection closed.\")\n",
    "logger.info(\"Database connection closed.\")\n",
    "# Close logging\n",
    "logger.info(\"Close logging system...\")\n",
    "logging.shutdown()\n",
    "print(\"logging shutdown.\")\n",
    "%reset -f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
