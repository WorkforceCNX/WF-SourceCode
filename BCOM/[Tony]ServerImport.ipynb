{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4990a7ea-4935-475f-bb03-d6657f954fa1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Library listü§ñ\n",
    "import glob, logging, warnings, polars as pl, datetime, os, zipfile, xml.dom.minidom\n",
    "from datetime import datetime as dt, time as t, timedelta\n",
    "import pandas as pd, numpy as np, sqlalchemy as sa, xlsxwriter\n",
    "from sqlalchemy import create_engine, text\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "from polars.exceptions import ColumnNotFoundError, PanicException\n",
    "from pathlib import Path\n",
    "from IPython.display import HTML\n",
    "from tabulate import tabulate\n",
    "# -----------------------------------------------------------------------------------------------#\n",
    "# --- Logging configurationüìú ---\n",
    "log_directory = Path(os.environ['USERPROFILE']) / r'Concentrix Corporation//CNXVN - WFM Team - Documents//DataBase//DataFrame//BKN//ScriptLogs//'\n",
    "log_directory.mkdir(parents=True, exist_ok=True) \n",
    "log_filename = log_directory / f\"import_log_{dt.now():%Y%m%d_%H%M%S}.log\"\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # (DEBUG, INFO, WARNING, ERROR, CRITICAL)\n",
    "    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_filename, encoding='utf-8'), \n",
    "    ],force=True)\n",
    "# Create logger object\n",
    "logger = logging.getLogger('ServerImportScript')\n",
    "# -----------------------------------------------------------------------------------------------#\n",
    "# Source collectionüì•\n",
    "user_credential = Path(os.environ['USERPROFILE']) / r'Concentrix Corporation//CNXVN - WFM Team - Documents//'\n",
    "\n",
    "# 0Ô∏è‚É£1Ô∏è‚É£[BKN]AHT2üóÉÔ∏è\n",
    "AHT2_TABLE_NAME = \"BCOM.AHT2\"\n",
    "Folder_AHT2 = user_credential / r'DataBase//DataRaw//BKN//AHT2//'\n",
    "log_AHT2_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//AHT2_log.xlsx'\n",
    "AHT2_schema = ['FileName', 'ModifiedDate', 'Date', 'Agent Name Display', 'Answered Language Name', 'Measure Names', 'Measure Values']\n",
    "# 0Ô∏è‚É£2Ô∏è‚É£[BKN]ROSTERüóÉÔ∏è\n",
    "ROSTER_TABLE_NAME = \"BCOM.ROSTER\"\n",
    "Folder_ROSTER = user_credential / r'DataBase//DataRaw//BKN//ROSTER//'\n",
    "log_ROSTER_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//ROSTER_log.xlsx'\n",
    "ROSTER_schema = ['FileName', 'ModifiedDate', 'Emp ID', 'Name', 'Attribute', 'Value', 'LOB', \n",
    "                 'team_leader', 'week_shift', 'week_off', 'OM', 'DPE']\n",
    "# 0Ô∏è‚É£3Ô∏è‚É£[BKN]EPSüóÉÔ∏è\n",
    "EPS_TABLE_NAME = \"BCOM.EPS\"\n",
    "Folder_EPS = user_credential / r'DataBase//DataRaw//BKN//EPS//'\n",
    "log_EPS_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//EPS_log.xlsx'\n",
    "EPS_schema = ['FileName', 'ModifiedDate', 'sitecode', 'manager_username', 'Username', 'Date', 'Session Login', \n",
    "              'Session Logout', 'Session Time', 'BPE Code', 'Total Time', 'SessionLogin_VN', 'SessionLogout_VN',\n",
    "              'NightTime', 'DayTime', 'Night_BPE', 'Day_BPE']\n",
    "# 0Ô∏è‚É£4Ô∏è‚É£[BKN]CPIüóÉÔ∏è\n",
    "CPI_TABLE_NAME = \"BCOM.CPI\"\n",
    "Folder_CPI = user_credential / r'DataBase//DataRaw//BKN//CPI//'\n",
    "log_CPI_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//CPI_log.xlsx'\n",
    "CPI_schema = ['FileName', 'ModifiedDate', 'Date', 'Staff Name', 'Hour Interval Selected', 'Channel', \n",
    "              'Item Label', 'Item ID', \"'Item ID'\", 'Time Alert', 'Nr. Contacts', 'Item Link', 'Time']\n",
    "# 0Ô∏è‚É£5Ô∏è‚É£[GLB]RAMCOüóÉÔ∏è\n",
    "RAMCO_TABLE_NAME = \"GLB.RAMCO\"\n",
    "Folder_RAMCO = user_credential / r'DataBase//DataRaw//GLOBAL//RAMCO//'\n",
    "log_RAMCO_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//RAMCO_log.xlsx'\n",
    "RAMCO_schema = ['FileName', 'ModifiedDate', 'EID', 'Employee_Name', 'Employee_type', 'Date', 'Code']\n",
    "# 0Ô∏è‚É£6Ô∏è‚É£[GLB]OT_RAMCOüóÉÔ∏è\n",
    "OT_RAMCO_TABLE_NAME = \"GLB.OT_RAMCO\"\n",
    "Folder_OT_RAMCO = user_credential / r'DataBase//DataRaw//GLOBAL//OT_RAMCO//'\n",
    "log_OT_RAMCO_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//OT_RAMCO_log.xlsx'\n",
    "OT_RAMCO_schema = ['FileName', 'ModifiedDate', 'employee_code', 'employee_name', 'Employee Type', 'OT Type', 'Date', 'Status', 'Hours']\n",
    "# 0Ô∏è‚É£7Ô∏è‚É£[GLB]PremHdaysüóÉÔ∏è\n",
    "PremHdays_TABLE_NAME = \"GLB.PremHdays\"\n",
    "Folder_PremHdays = user_credential / r'DataBase//DataRaw//GLOBAL//HOLIDAY_MAPPING//'\n",
    "log_PremHdays_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//PremHdays_log.xlsx'\n",
    "PremHdays_schema = ['FileName', 'ModifiedDate', 'Date', 'Holiday']\n",
    "# 0Ô∏è‚É£8Ô∏è‚É£[GLB]NormHdaysüóÉÔ∏è\n",
    "NormHdays_TABLE_NAME = \"GLB.NormHdays\"\n",
    "Folder_NormHdays = user_credential / r'DataBase//DataRaw//GLOBAL//HOLIDAY_MAPPING_NONBILLABLE//'\n",
    "log_NormHdays_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//NormHdays_log.xlsx'\n",
    "NormHdays_schema = ['FileName', 'ModifiedDate', 'Solar Day', 'Lunar Day', 'Holiday']\n",
    "# 0Ô∏è‚É£9Ô∏è‚É£[GLB]EmpMasterüóÉÔ∏è\n",
    "EmpMaster_TABLE_NAME = \"GLB.EmpMaster\"\n",
    "Folder_EmpMaster = user_credential / r'DataBase//DataRaw//GLOBAL//WDD//'\n",
    "log_EmpMaster_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//EmpMaster_log.xlsx'\n",
    "EmpMaster_schema = ['FileName', 'ModifiedDate', 'EMPLOYEE_NUMBER', 'PREVIOUS_PAYROLL_ID', 'FIRST_NAME', 'MIDDLE_NAME', 'LAST_NAME', \n",
    "                    'FULL_NAME', 'Work Related Status', 'Work Related (Extended Status)', 'Service Type', 'WAH & Hybrid Platform', \t\n",
    "                    'ORIGINAL_DATE_OF_HIRE', 'LEGAL_EMPLOYER_HIRE_DATE', 'Continuous Service Date', 'Fixed Term Hire End Date', \n",
    "                    'Contract End Date', 'PERSON_TYPE', 'WORKER_CATEGORY', 'Time Type', 'Employee Type', 'Last Promotion Date', \n",
    "                    'Assignment Category', 'Email - Work', 'BUSINESS_UNIT', 'Job Code', 'Job Title', 'Business Title', 'Cost Center - ID', \n",
    "                    'Cost Center - Name', 'LOCATION_CODE', 'LOCATION_NAME', 'CNX BU', 'Concentrix LOB', 'Process', 'COMPANY', \n",
    "                    'MANAGEMENT_LEVEL', 'Job Level', 'Compensation Grade', 'JOB_FUNCTION_DESCRIPTION', 'JOB_FAMILY', 'MSA', 'MSA Client', \n",
    "                    'MSA Program', 'ACTIVITY ID', 'SUPERVISOR_ID', 'SUPERVISOR_FULL_NAME', 'SUPERVISOR_EMAIL_ID', 'MANAGER_02_ID', \n",
    "                    'MANAGER_02_FULL_NAME', 'MANAGER_02_EMAIL_ID', 'COMP_CODE', 'CITY', 'Location', 'Country', 'Employee Status', 'Work Shift']\n",
    "# 1Ô∏è‚É£0Ô∏è‚É£[GLB]TerminationüóÉÔ∏è\n",
    "Termination_TABLE_NAME = \"GLB.Termination\"\n",
    "Folder_Termination = user_credential / r'DataBase//DataRaw//GLOBAL//WDD//'\n",
    "log_Termination_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//Termination_log.xlsx'\n",
    "Termination_schema = ['FileName', 'ModifiedDate', 'EMPLOYEE_ID', 'PREVIOUS_PAYROLL_ID', 'FIRST_NAME', 'MIDDLE_NAME', 'LAST_NAME', 'FULL_NAME', \n",
    "                      'EMAIL_ADDRESS', 'HIRE_DATE', 'ORIGINAL_HIRE_DATE', 'END EMPLOYMENT DATE', 'Contract End Date', 'Termination Date', \n",
    "                      'Termination Date (DD/MM/YY)', 'Eligible for Rehire', 'LWD', 'MOST RECENT TERMINATION - DATE INITIATED', \n",
    "                      'MOST RECENT TERMINATION - DATE COMPLETED', 'MOST RECENT TERMINATION - EFFECTIVE DATE', 'MOST RECENT TERMINATION - REASON', \n",
    "                      'Action date', 'DATE INITIATED', 'COMPELETED DATE AND TIME', 'TERMINATION DATE 2', 'Is Initiated through Resignation', \n",
    "                      'Termination Reason', 'Resignation Reason', 'Secondary Termination Reasons', 'Resignation Notice served', 'PERSON_TYPE', \n",
    "                      'Time Type', 'Employee Type', 'Worker Type', 'Assignment Category', 'WORKER_CATEGORY', 'BUSINESS_UNIT', 'Cost Center', \n",
    "                      'Cost Center - ID', 'JOB_CODE', 'JOB_TITLE', 'BUSINESS_TITLE', 'LOCATION_NAME', 'LOCATION_CODE', 'COUNTRY', 'COMPANY', \n",
    "                      'MANAGEMENT LEVEL', 'JOB LEVEL', 'JOB_FAMILY', 'JOB_FUNCTION', 'JOB_ROLE', 'MSA', 'CNX BU', 'Concentrix LOB', 'Process', \n",
    "                      'Client Name ( Process )', 'Compensation Grade', 'SUPERVISOR_ID', 'SUPERVISOR_FULL_NAME', 'SUPERVISOR_EMAIL_ID', 'COMP_CODE', \n",
    "                      'CITY', 'LOCATION_DESCRIPTION', 'EMPLOYEE STATUS', 'Continuous Service Date', 'Work Related Status', \n",
    "                      'Work Related (Extended Status)', 'Activity', 'MSA Legacy Project ID']\n",
    "# 1Ô∏è‚É£1Ô∏è‚É£[GLB]ResignationüóÉÔ∏è\n",
    "Resignation_TABLE_NAME = \"GLB.Resignation\"\n",
    "Folder_Resignation = user_credential / r'DataBase//DataRaw//GLOBAL//WDD//'\n",
    "log_Resignation_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//Resignation_log.xlsx'\n",
    "Resignation_schema = ['FileName', 'ModifiedDate', 'Employee ID', 'Full Name', 'Job Family', 'MSA Client', 'Country', 'Location', 'Action', \n",
    "                      'Action Date', 'Date and Time Initiated', 'Status', 'Primary Reason', 'Secondary Reasons', 'Notification Date', 'Awaiting Persons', \n",
    "                      'Resignation Primary Reason', 'Hire Date', 'Proposed Termination Date', 'Notice Served', 'Sup ID', 'Supervisor Name', \n",
    "                      'Employee Status', 'Activity', 'MSA Legacy Project ID', 'Initiated By']\n",
    "# 1Ô∏è‚É£2Ô∏è‚É£[BKN]CPI_PEGAüóÉÔ∏è\n",
    "CPI_PEGA_TABLE_NAME = \"BCOM.CPI_PEGA\"\n",
    "Folder_CPI_PEGA = user_credential / r'DataBase//DataRaw//BKN//CPI_PEGA//'\n",
    "log_CPI_PEGA_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//CPI_PEGA_log.xlsx'\n",
    "CPI_PEGA_schema = ['FileName', 'ModifiedDate', 'Staff Name', 'Operator Def', 'Service Case Type New', 'Channel Def',\t\n",
    "                   'Lang Def', 'Reason For No Service Case', 'Topic Def New', 'Subtopics', 'Case Id', 'Reservation Id Def',\n",
    "                   'Day of Date', 'Blank', '# Swivels', 'Count of ServiceCase or Interaction']\n",
    "# 1Ô∏è‚É£3Ô∏è‚É£[BKN]StaffüóÉÔ∏è\n",
    "Staff_TABLE_NAME = \"BCOM.Staff\"\n",
    "Folder_Staff = user_credential / r'DataBase//DataRaw//BKN//AGENTS//'\n",
    "log_Staff_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//Staff_log.xlsx'\n",
    "Staff_schema = ['FileName', 'ModifiedDate', 'Employee_ID', 'GEO', 'Site_ID', 'Employee_Last_Name', 'Employee_First_Name', 'Status', 'Wave #', \n",
    "                'Role', 'Booking Login ID', 'Language Start Date', 'TED Name', 'CUIC Name', 'EnterpriseName', 'Hire_Date', 'PST_Start_Date',\n",
    "                'Production_Start_Date', 'LWD', 'Termination_Date', 'Designation', 'cnx_email', 'Booking Email', 'WAH Category', 'Full name',\n",
    "                'IEX', 'serial_number', 'BKN_ID', 'Extension Number']\n",
    "# 1Ô∏è‚É£4Ô∏è‚É£[BKN]ConTracküóÉÔ∏è\n",
    "ConTrack_TABLE_NAME = \"BCOM.ConTrack\"\n",
    "Folder_ConTrack = Path(os.environ['USERPROFILE']) / r'OneDrive - Concentrix Corporation//DataBase//DataRaw//BKN//ContactTracker//'\n",
    "log_ConTrack_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//ConTrack_log.xlsx'\n",
    "ConTrack_schema = ['FileName', 'ModifiedDate', 'Id', 'Start time', 'Completion time', 'Email', 'Name', 'Reservation Number', 'Contact Types',\n",
    "                   'Contact Parties', 'Unbabel Tool Used?', 'Backlog Case', 'How many days since guest contacted? (ex: 30)', 'Topics',\n",
    "                   'Resolutions', 'Reason If Skipped', 'CRM used', 'Outbound to Senior', 'Outbound Status','Reason (Name - Site of Senior)',\n",
    "                   'Note', 'Reason for cannot make OB call to Guest', 'Is it possible to make Outbound call to Guest?¬†']\n",
    "# 1Ô∏è‚É£5Ô∏è‚É£[BKN]QualityüóÉÔ∏è\n",
    "Quality_TABLE_NAME = \"BCOM.Quality\"\n",
    "Folder_Quality = user_credential / r'DataBase//DataRaw//BKN//QUALITY//'\n",
    "log_Quality_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//Quality_log.xlsx'\n",
    "Quality_schema = ['FileName', 'ModifiedDate', 'eps_name', 'eval_id', 'eval_date', 'agent_username', 'evaluator_username', 'result',\t\n",
    "                  'final_question_grouping', 'template_group', 'eval_template_name', 'sections', 'sitecode', 'score_n', 'score_question_weight',\n",
    "                  'eval_language', 'eval_reference', 'tix_final_topic', 'tix_final_subtopic', 'csat_language_code', 'csat_satisfied']\n",
    "# 1Ô∏è‚É£6Ô∏è‚É£[BKN]RONAüóÉÔ∏è\n",
    "RONA_TABLE_NAME = \"BCOM.RONA\"\n",
    "Folder_RONA = user_credential / r'DataBase//DataRaw//BKN//RONA//'\n",
    "log_RONA_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//RONA_log.xlsx'\n",
    "RONA_schema = ['FileName', 'ModifiedDate', 'Agent', 'DateTime', 'RONA']\n",
    "# 1Ô∏è‚É£7Ô∏è‚É£[BKN]CUICüóÉÔ∏è\n",
    "CUIC_TABLE_NAME = \"BCOM.CUIC\"\n",
    "Folder_CUIC = user_credential / r'DataBase//DataRaw//BKN//CUIC//'\n",
    "log_CUIC_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//CUIC_log.xlsx'\n",
    "CUIC_schema = ['FileName', 'ModifiedDate', 'FullName', 'LoginName', 'Interval', 'AgentAvailTime', 'AgentLoggedOnTime']\n",
    "# 1Ô∏è‚É£8Ô∏è‚É£[BKN]KPI_TargetüóÉÔ∏è\n",
    "KPI_Target_TABLE_NAME = \"BCOM.KPI_Target\"\n",
    "Folder_KPI_Target = user_credential / r'DataBase//DataRaw//BKN//KPI_TARGET//'\n",
    "log_KPI_Target_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//KPI_Target_log.xlsx'\n",
    "KPI_Target_schema = ['FileName', 'ModifiedDate', 'LOB', 'LOB Group', 'Week', 'Tenure days', 'Overall CPH tar', 'Phone CPH tar', 'Non Phone CPH tar',\t\n",
    "                     'Quality - Customer Impact tar', 'Quality - Business Impact tar', 'Quality - Compliance Impact tar', 'Quality - Overall tar', 'AHT Phone tar',\t\n",
    "                     'AHT Non-phone tar', 'AHT Overall tar', 'Hold (phone) tar', 'AACW (phone) tar', 'Avg Talk Time tar', 'Phone CSAT tar', 'Non phone CSAT tar',\t\n",
    "                     'Overall CSAT tar', 'PSAT tar', 'PSAT Vietnamese tar', 'PSAT English (American) tar', 'PSAT English (Great Britain) tar', 'CSAT Reso tar']\n",
    "# 1Ô∏è‚É£9Ô∏è‚É£[BKN]LogoutCountüóÉÔ∏è\n",
    "LogoutCount_TABLE_NAME = \"BCOM.LogoutCount\"\n",
    "Folder_LogoutCount = user_credential / r'DataBase//DataRaw//BKN//LOGOUT_COUNT//'\n",
    "log_LogoutCount_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//LogoutCount_log.xlsx'\n",
    "LogoutCount_schema = ['FileName', 'ModifiedDate', 'Aggregation', 'TimeDimension', 'KPI Value Formatted']\n",
    "# 2Ô∏è‚É£0Ô∏è‚É£[BKN]WpDetailüóÉÔ∏è\n",
    "WpDetail_TABLE_NAME = \"BCOM.WpDetail\"\n",
    "Folder_WpDetail = user_credential / r'DataBase//DataRaw//BKN//WORKPLAN//WORKPLAN QUERY//'\n",
    "log_WpDetail_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//WpDetail_log.xlsx'\n",
    "WpDetail_schema = ['FileName', 'ModifiedDate', 'LOB', 'ID', 'DateTime_Start', 'DateTime_End', 'Date_Start', 'Date_end', 'Time_Start', 'Time_End', \n",
    "                   'Dur', 'Action', 'DateTime_Act_Start', 'DateTime_Act_End', 'Date_Act_Start', 'Date_Act_End', 'Time_Act_Start', 'Time_Act_End', 'Act_Dur']\n",
    "# 2Ô∏è‚É£1Ô∏è‚É£[BKN]WpSummaryüóÉÔ∏è\n",
    "WpSummary_TABLE_NAME = \"BCOM.WpSummary\"\n",
    "Folder_WpSummary = user_credential / r'DataBase//DataRaw//BKN//WORKPLAN_SUMMARY//WORKPLAN_SUMMARY_QUERY//'\n",
    "log_WpSummary_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//WpSummary_log.xlsx'\n",
    "WpSummary_schema = ['FileName', 'ModifiedDate', 'LOB', 'Date', 'Agent ID', 'Agent Name', 'Scheduled Activity', 'Length', 'Percent']\n",
    "# 2Ô∏è‚É£2Ô∏è‚É£[BKN]RegisteredOTüóÉÔ∏è\n",
    "RegisteredOT_TABLE_NAME = \"BCOM.RegisteredOT\"\n",
    "Folder_RegisteredOT = user_credential / r'DataBase//DataRaw//BKN//OVERTIME//'\n",
    "log_RegisteredOT_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//RegisteredOT_log.xlsx'\n",
    "RegisteredOT_schema = ['FileName', 'ModifiedDate', 'Emp ID', 'Name', 'Date', 'Value', 'OT', 'LOB','Type']\n",
    "# 2Ô∏è‚É£3Ô∏è‚É£[BKN]CSAT_TPüóÉÔ∏è\n",
    "CSAT_TP_TABLE_NAME = \"BCOM.CSAT_TP\"\n",
    "Folder_CSAT_TP = user_credential / r'DataBase//DataRaw//BKN//CSAT//'\n",
    "log_CSAT_TP_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//CSAT_TP_log.xlsx'\n",
    "CSAT_TP_schema = ['FileName', 'ModifiedDate', 'Sort by Dimension', 'Survey Id', 'Reservation', 'Team', 'Channel', 'Staff', 'Type', 'Date',\n",
    "                  'Topic of the first Ticket', 'Language', 'Csat 2.0 Score', 'Has Comment', '\"Comment\"', 'Reservation Link', 'View comment',\n",
    "                  'Sort by Dimension (copy)', 'Max. Sort by Dimension']\n",
    "# 2Ô∏è‚É£4Ô∏è‚É£[BKN]CSAT_RSüóÉÔ∏è\n",
    "CSAT_RS_TABLE_NAME = \"BCOM.CSAT_RS\"\n",
    "Folder_CSAT_RS = user_credential / r'DataBase//DataRaw//BKN//CSAT_RESO//'\n",
    "log_CSAT_RS_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//CSAT_RS_log.xlsx'\n",
    "CSAT_RS_schema = ['FileName', 'ModifiedDate', 'Sort by Dimension', 'Survey Id', 'Reservation', 'Team', 'Channel', 'Staff', 'Type', 'Date',\n",
    "                  'Topic of the first Ticket', 'Language', 'Csat 2.0 Score', 'Has Comment', '\"Comment\"', 'Reservation Link', 'View comment',\n",
    "                  'Sort by Dimension (copy)', 'Max. Sort by Dimension']\n",
    "# 2Ô∏è‚É£5Ô∏è‚É£[BKN]PSATüóÉÔ∏è\n",
    "PSAT_TABLE_NAME = \"BCOM.PSAT\"\n",
    "Folder_PSAT = user_credential / r'DataBase//DataRaw//BKN//PSAT//'\n",
    "log_PSAT_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//PSAT_log.xlsx'\n",
    "PSAT_schema = ['FileName', 'ModifiedDate', 'Sorted By Dimension', 'Survey Id', 'Date', 'Staff Name', 'Language', 'Final Topics',\n",
    "               'How satisfied were you with our service?', 'How difficult did we make it or you to solve your issue?', 'Agent understood my question',\n",
    "               'Agent did everything possible to help me', 'Did we fully resolve your issue?', 'Channel', 'Hotel Id', '\"Comment\"',\n",
    "               'Has Comment', 'Sorted BY Dimension (copy)']\n",
    "# 2Ô∏è‚É£6Ô∏è‚É£[BKN]IEX_HrsüóÉÔ∏è\n",
    "IEX_Hrs_TABLE_NAME = \"BCOM.IEX_Hrs\"\n",
    "Folder_IEX_Hrs = user_credential / r'DataBase//DataRaw//BKN//IEXHOURS//Overview Interval Query//'\n",
    "log_IEX_Hrs_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//IEX_Hrs_log.xlsx'\n",
    "IEX_Hrs_schema = ['FileName', 'ModifiedDate', 'LOB', 'VNT', 'CET', 'HC', 'Hour']\n",
    "# 2Ô∏è‚É£7Ô∏è‚É£[BKN]IntervalReqüóÉÔ∏è\n",
    "IntervalReq_TABLE_NAME = \"BCOM.IntervalReq\"\n",
    "Folder_IntervalReq = user_credential / r'DataBase//DataRaw//BKN//INTERVAL_REQUIREMENT//'\n",
    "log_IntervalReq_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//IntervalReq_log.xlsx'\n",
    "IntervalReq_schema = ['FileName', 'ModifiedDate', 'LOB', 'Datetime_CET', 'Datetime_VN', 'Value', 'Delivery_Req']\n",
    "# 2Ô∏è‚É£8Ô∏è‚É£[BKN]ExceptionReqüóÉÔ∏è\n",
    "ExceptionReq_TABLE_NAME = \"BCOM.ExceptionReq\"\n",
    "Folder_ExceptionReq = user_credential / r'DataBase//DataRaw//BKN//EXCEPTION_REQ//'\n",
    "log_ExceptionReq_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//ExceptionReq_log.xlsx'\n",
    "ExceptionReq_schema = ['FileName', 'ModifiedDate', 'Emp ID', 'Date (MM/DD/YYYY)', 'Exception request (Minute)', 'Reason', 'TL', 'OM']\n",
    "# 2Ô∏è‚É£9Ô∏è‚É£[BKN]LTTransfersüóÉÔ∏è\n",
    "LTTransfers_TABLE_NAME = \"BCOM.LTTransfers\"\n",
    "Folder_LTTransfers = user_credential / r'DataBase//DataRaw//BKN//HC_TRANSFER//'\n",
    "log_LTTransfers_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//LTTransfers_log.xlsx'\n",
    "LTTransfers_schema = ['FileName', 'ModifiedDate', 'EID', 'Full Name', 'Employee Status', 'LWD', 'Remarks']\n",
    "# 3Ô∏è‚É£0Ô∏è‚É£[BKN]DailyReqüóÉÔ∏è\n",
    "DailyReq_TABLE_NAME = \"BCOM.DailyReq\"\n",
    "Folder_DailyReq = user_credential / r'DataBase//DataRaw//BKN//REQUIREMENT_HOURS//'\n",
    "log_DailyReq_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//DailyReq_log.xlsx'\n",
    "DailyReq_schema = ['FileName', 'ModifiedDate', 'LOB', 'Date', 'Daily Requirement', 'Prod Requirement']\n",
    "# 3Ô∏è‚É£1Ô∏è‚É£[BKN]ProjectedShrinküóÉÔ∏è\n",
    "ProjectedShrink_TABLE_NAME = \"BCOM.ProjectedShrink\"\n",
    "Folder_ProjectedShrink = user_credential / r'DataBase//DataRaw//BKN//SHRINKAGE_TARGET//'\n",
    "log_ProjectedShrink_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//ProjectedShrink_log.xlsx'\n",
    "ProjectedShrink_schema = ['FileName', 'ModifiedDate', 'LOB', 'Week', 'Ratio']\n",
    "# 3Ô∏è‚É£2Ô∏è‚É£[BKN]OTReqüóÉÔ∏è\n",
    "OTReq_TABLE_NAME = \"BCOM.OTReq\"\n",
    "Folder_OTReq = user_credential / r'DataBase//DataRaw//BKN//OT_REQ//'\n",
    "log_OTReq_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//OTReq_log.xlsx'\n",
    "OTReq_schema = ['FileName', 'ModifiedDate', 'Date', 'LOB', 'OT Hour', 'Type']\n",
    "# 3Ô∏è‚É£3Ô∏è‚É£[BKN]CapHCüóÉÔ∏è\n",
    "CapHC_TABLE_NAME = \"BCOM.CapHC\"\n",
    "Folder_CapHC = user_credential / r'DataBase//DataRaw//BKN//CAPACITY_HC//'\n",
    "log_CapHC_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//CapHC_log.xlsx'\n",
    "CapHC_schema = ['FileName', 'ModifiedDate', 'LOB', 'Date', 'Client Requirement (Hours)']\n",
    "# 3Ô∏è‚É£4Ô∏è‚É£[BKN]ProjectedHCüóÉÔ∏è\n",
    "ProjectedHC_TABLE_NAME = \"BCOM.ProjectedHC\"\n",
    "Folder_ProjectedHC = user_credential / r'DataBase//DataRaw//BKN//PROJECTED_HEADCOUNT//'\n",
    "log_ProjectedHC_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//ProjectedHC_log.xlsx'\n",
    "ProjectedHC_schema = ['FileName', 'ModifiedDate', 'Date', 'LOB', 'FTE Required', 'Projected HC', 'Plan Leave', \n",
    "                      'Actual Projected HC', '%OO', '%IO', 'Projected HC with Shrink', 'OT', 'Leave allow for Shrink', '% Deli']\n",
    "# 3Ô∏è‚É£5Ô∏è‚É£[BKN]RampHCüóÉÔ∏è\n",
    "RampHC_TABLE_NAME = \"BCOM.RampHC\"\n",
    "Folder_RampHC = user_credential / r'DataBase//DataRaw//BKN//RAMPUP_HC//'\n",
    "log_RampHC_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//RampHC_log.xlsx'\n",
    "RampHC_schema = ['FileName', 'ModifiedDate', 'Date', 'LOB', 'Headcount', 'Hours']\n",
    "# -----------------------------------------------------------------------------------------------#\n",
    "# Database_Connecterüß¨\n",
    "\n",
    "server_name = \"PHMANVMDEV01V\"\n",
    "server_ip = \"10.5.11.60\"\n",
    "database = \"wfm_vn_dev\"\n",
    "user = \"usr_wfmvn_dev\"\n",
    "password = \"12guWU2OdEj5kEspl9Rlfoglf\"\n",
    "# SQL Server Authentication üîó\n",
    "connection_string = f\"mssql+pyodbc://{user}:{password}@{server_ip}/{database}?driver=ODBC+Driver+17+for+SQL+Server\"\n",
    "# Windows Authentication üîó\n",
    "# connection_string = f\"mssql+pyodbc://{server_name}/{database}?driver=ODBC+Driver+17+for+SQL+Server&Trusted_Connection=yes\"\n",
    "try:\n",
    "    engine = create_engine(connection_string, fast_executemany=True)\n",
    "    logger.info(f\"‚úÖ Successfully connected to DB: {database} server: {server_ip}\")\n",
    "except Exception as e:\n",
    "    logger.exception(\"‚ùå DB Connection error\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52637df5-a0d3-4131-9c4b-ab715459f181",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function Definitionüõ†Ô∏è\n",
    "\n",
    "# Log Color viewüí°\n",
    "def print_colored(text, color):\n",
    "    display(HTML(f'<span style=\"color: {color};\">{text}</span>'))\n",
    "\n",
    "# Check existing log fileüí°\n",
    "def read_or_create_log(log_path):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore') # Ignor WarningüìÉ\n",
    "        try:\n",
    "            logger.debug(f\"Reading log file: {log_path}\")\n",
    "            log_df = pl.read_excel(log_path)\n",
    "            log_df = log_df.with_columns([pl.col(\"ModifiedDate\").dt.cast_time_unit(\"ms\")], strict=False)\n",
    "            logger.info(f\"Success read log file: {log_path}\")\n",
    "        except FileNotFoundError: # Create new log if can't find logüìÉ\n",
    "            logger.warning(f\"Log file not found: {log_path}. Create new log.\")\n",
    "            log_df = pl.DataFrame(\n",
    "                {\n",
    "                    \"FileName\": pl.Series([], dtype=pl.Utf8),\n",
    "                    \"ModifiedDate\": pl.Series([], dtype=pl.Datetime),\n",
    "                    \"Error\": pl.Series([], dtype=pl.Utf8),})\n",
    "        except Exception as e: # Create new log if can't open logüìÉ\n",
    "            logger.exception(f\"Error reading log file: {log_path}\")\n",
    "            print(f\"Error reading log file: {e}\")\n",
    "            log_df = pl.DataFrame(\n",
    "                {\n",
    "                    \"FileName\": pl.Series([], dtype=pl.Utf8),\n",
    "                    \"ModifiedDate\": pl.Series([], dtype=pl.Datetime),\n",
    "                    \"Error\": pl.Series([], dtype=pl.Utf8),})\n",
    "        return log_df\n",
    "        \n",
    "# Update log_dfüí°\n",
    "def process_and_save_log(log_df, log_entries, log_path):\n",
    "    if log_entries:\n",
    "        new_log_df = pl.DataFrame(log_entries)\n",
    "        log_df = log_df.with_columns(pl.col('ModifiedDate').dt.cast_time_unit(\"ms\"))\n",
    "        log_df = (pl.concat([log_df, new_log_df], how=\"diagonal_relaxed\") # Combine and remove duplicate New_Log and Old_LogüìÉ\n",
    "                  .sort(\"ModifiedDate\", descending=[False])\n",
    "                  .unique(subset=[\"FileName\"], keep=\"last\")\n",
    "                  .sort(\"FileName\", descending=[False])\n",
    "                  .select([\"FileName\", \"ModifiedDate\", \"Error\"]))\n",
    "        try:\n",
    "            log_df.write_excel(log_path, worksheet=\"ImportLog\", autofit=True)\n",
    "            print(f\"Import log saved to: {log_path}\")\n",
    "            logger.info(f\"Import log saved to: {log_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error writing log file: {e}\")\n",
    "            logger.error(f\"Error writing log file: {log_path} - {e}\")\n",
    "\n",
    "# write_dataüí°\n",
    "def write_data(engine, table_name, df): # write to databaseüìÉ\n",
    "     df.write_database(table_name=table_name, connection=engine, if_table_exists=\"append\")\n",
    "    \n",
    "# delete_dataüí°\n",
    "def delete_data(engine, table_name, filename):\n",
    "    try:\n",
    "        with engine.connect() as connection:\n",
    "            print_colored(f\"Prepare to delete old data for '{filename}' in '{table_name}'\", \"DarkTurquoise\")\n",
    "            logger.warning(f\"Prepare to delete old data for '{filename}' in '{table_name}'\")\n",
    "            delete_query = text(f\"DELETE FROM {table_name} WHERE [FileName] = :filename\")\n",
    "            connection.execute(delete_query, {\"filename\": filename})\n",
    "            connection.commit()\n",
    "            print_colored(f\"Old data deleted successfullyüßπ\", \"DarkTurquoise\")\n",
    "            logger.info(f\"'{filename}' data deleted successfully in '{table_name}' üßπ.\")\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Error while delete data for '{filename}' in '{table_name}'\")\n",
    "        print_colored(f\"Error while delete data for '{filename}' in '{table_name}'\", \"DarkTurquoise\")\n",
    "        raise \n",
    "        \n",
    "# Check Timeüí°\n",
    "def is_time_between(begin_time, end_time, check_time=None):\n",
    "    check_time = check_time or datetime.utcnow().time() # If check time is not given, default to current UTC timeüìÉ\n",
    "    if begin_time < end_time:\n",
    "        return check_time >= begin_time and check_time <= end_time\n",
    "    else: # crosses midnightüìÉ\n",
    "        return check_time >= begin_time or check_time <= end_time\n",
    "def time_difference(time1, time2):\n",
    "    seconds1 = time1.hour * 3600 + time1.minute * 60 + time1.second # Convert times to secondsüìÉ\n",
    "    seconds2 = time2.hour * 3600 + time2.minute * 60 + time2.second\n",
    "    diff_seconds = seconds1 - seconds2\n",
    "    return diff_seconds\n",
    "\n",
    "# Final Summaryüí°\n",
    "def display_summary(source_name: str, error_count: int) -> None:\n",
    "    \"\"\"Final Notice.\"\"\"\n",
    "    if error_count > 0:\n",
    "        print_colored(f\"Finished processing all files ({error_count} have errorsüõ†Ô∏è).\", \"OrangeRed\")\n",
    "        logger.warning(f\"Finished processing all files ({error_count} have errorsüõ†Ô∏è).\")\n",
    "    else:\n",
    "        print_colored(f\"Finished processing all files (no errorsüéâ).\", \"PaleVioletRed\")\n",
    "        logger.info(f\"Finished processing [{source_name}] (no errorsüéâ).\")\n",
    "\n",
    "# Default_variableüí°\n",
    "def Default_variable():\n",
    "    log_entries = []\n",
    "    error_count = 0\n",
    "    return log_entries, error_count\n",
    "\n",
    "# parse_dateüí°\n",
    "def parse_date(col: pl.Expr) -> pl.Expr:\n",
    "    return pl.coalesce(\n",
    "        col.str.strptime(pl.Date, format=\"%m/%d/%Y\", strict=False),\n",
    "        col.str.strptime(pl.Date, format=\"%Y-%m-%d\", strict=False),\n",
    "        col.str.strptime(pl.Date, format=\"%d %B %Y\", strict=False),\n",
    "        col.str.strptime(pl.Date, format=\"%B %d, %Y\", strict=False),\n",
    "        col.str.strptime(pl.Date, format=\"%d-%b-%y\", strict=False),\n",
    "    )\n",
    "\n",
    "# validate_schemaüí°\n",
    "def validate_schema(df: pl.DataFrame, expected_schema: list[str], filename: str) -> tuple[bool, str | None]:\n",
    "    # Start validation\n",
    "    start_msg = f\"üîç Starting schema validation for file: {filename}\"\n",
    "    logger.info(start_msg)\n",
    "    print_colored(start_msg, \"DodgerBlue\")\n",
    "    actual_columns = df.columns\n",
    "    expected_set = set(expected_schema)\n",
    "    actual_set = set(actual_columns)\n",
    "    missing_columns = expected_set - actual_set\n",
    "    extra_columns = actual_set - expected_set\n",
    "    has_critical_error = False\n",
    "    critical_error_message = None\n",
    "    has_warnings = False\n",
    "    # 1. Schema error (Missing columns)\n",
    "    if missing_columns:\n",
    "        has_critical_error = True\n",
    "        critical_error_message = f\"Schema error in the file: '{filename}'. Missing columns: {sorted(list(missing_columns))}\"\n",
    "        logger.error(critical_error_message)\n",
    "        print_colored(f\"‚ùóÔ∏è {critical_error_message}\", \"OrangeRed\")\n",
    "    # 2. warning extra columns\n",
    "    if extra_columns:\n",
    "        has_warnings = True\n",
    "        warning_message = f\"warning schema for file '{filename}'. Extra columns: {sorted(list(extra_columns))}. These columns will be excluded from the import process.\"\n",
    "        logger.warning(warning_message)\n",
    "        print_colored(f\"‚ö†Ô∏è {warning_message}\", \"Gold\")\n",
    "    # 3. Final results announcement\n",
    "    if not has_critical_error and not has_warnings:\n",
    "        final_msg = f\"‚úÖ Completely valid schema for the file: {filename}.\"\n",
    "        logger.info(final_msg)\n",
    "        print_colored(final_msg, \"MediumSeaGreen\")\n",
    "    elif not has_critical_error and has_warnings:\n",
    "        final_msg = f\"‚ö†Ô∏è File schema check: {filename} Passed (No missing columns, extra columns warned)\"\n",
    "        logger.info(final_msg)\n",
    "        print_colored(final_msg, \"MediumSeaGreen\") # V·∫´n d√πng m√†u xanh l√°\n",
    "    elif has_critical_error:\n",
    "        final_msg = f\"‚ùå Schema validation failed due to missing column(s) for file: {filename}.\"\n",
    "        logger.warning(final_msg) # Log ·ªü m·ª©c warning ho·∫∑c error t√πy √Ω\n",
    "        print_colored(final_msg, \"OrangeRed\")\n",
    "    return has_critical_error, critical_error_message\n",
    "    \n",
    "# DF Infoüí°\n",
    "def info_polars(df: pl.DataFrame):\n",
    "    print_colored(f\"‚öôÔ∏èFinal structure\", \"Olive\")\n",
    "    logger.info(f\"‚öôÔ∏èFinal structure\")\n",
    "    shape = df.shape\n",
    "    print(f\"Shape: {shape}\")\n",
    "    print(\"Data columns:\")  \n",
    "    table_data = []\n",
    "    for i, name in enumerate(df.columns):\n",
    "        dtype = df.dtypes[i]\n",
    "        non_null_count = df.select(pl.col(name).is_not_null().sum()).item()\n",
    "        table_data.append([i, name, non_null_count, dtype])  \n",
    "    headers = [\"#\", \"Column\", \"Non-Null Count\", \"Dtype\"]\n",
    "    print(tabulate(table_data, headers=headers, tablefmt=\"grid\"))\n",
    "    logger.info(tabulate(table_data, headers=headers, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95153293-c891-489f-9553-117b58dabf17",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 0Ô∏è‚É£1Ô∏è‚É£[BKN]AHT2üíæ\n",
    "logger.info(\"===== Start AHT2 Process =====\") #üß©\n",
    "log_df = read_or_create_log(log_AHT2_path) #üß©\n",
    "log_entries, error_count = Default_variable()\n",
    "# Loop Folderüí°\n",
    "for filename in Folder_AHT2.glob(\"*.csv\"): #üß©\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Add ModifiedDateüìÉ\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0)\n",
    "    file_basename = filename.name # Add FileNameüìÉ\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Compare FileName with LogüìÉ\n",
    "    is_new_file = log_entry.is_empty()\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1) \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Compare ModifiedDate with LogüìÉ\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # Old_File with modifieddate changedüìÉ\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # Is_New_fileüìÉ\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try:\n",
    "            if not is_new_file: #Then Delete and ImportüìÉ\n",
    "                delete_data(engine, AHT2_TABLE_NAME, file_basename) #üß©\n",
    "            # Process import to Databaseüí°\n",
    "            AHT2 = (pl.read_csv(filename, infer_schema_length=None) #üß©\n",
    "                   .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   )) #üß©        \n",
    "            # SCHEMA checküìÉ\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(AHT2, AHT2_schema, file_basename) #üß©\n",
    "            if has_critical_error:\n",
    "                raise ValueError(critical_schema_error_msg)\n",
    "            # column structureüìÉ\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            AHT2 = AHT2.with_columns(parse_date(pl.col(\"Date\")).alias(\"Date\"),).select(AHT2_schema) #üß©\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(AHT2) #üß©\n",
    "            #ImportüìÉ\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, AHT2_TABLE_NAME, AHT2) #üß©\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            # Uppdate log_entries for successfully importüí°\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "        # Uppdate log_entries for fail importüí°\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception\n",
    "        ) as e:\n",
    "            error_count += 1\n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"\n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True)\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: # Old_File with modifieddate unchangedüìÉ\n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "display_summary(\"AHT2\", error_count) #üß©\n",
    "process_and_save_log(log_df, log_entries, log_AHT2_path) #üß©\n",
    "logger.info(\"===== Processing of the AHT2 data source is complete =====\") #üß©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ee02f0-2bdc-4e98-8893-cba402f08557",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 0Ô∏è‚É£2Ô∏è‚É£[BKN]ROSTERüíæ\n",
    "logger.info(\"===== Start ROSTER Process =====\") #üß©\n",
    "log_df = read_or_create_log(log_ROSTER_path) #üß©\n",
    "log_entries, error_count = Default_variable()\n",
    "# Loop Folderüí°\n",
    "for filename in Folder_ROSTER.glob(\"*.xlsx\"): #üß©\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Add ModifiedDateüìÉ\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0)\n",
    "    file_basename = filename.name # Add FileNameüìÉ\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Compare FileName with LogüìÉ\n",
    "    is_new_file = log_entry.is_empty()\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1) \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Compare ModifiedDate with LogüìÉ\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # Old_File with modifieddate changedüìÉ\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # Is_New_fileüìÉ\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try:\n",
    "            if not is_new_file: #Then Delete and ImportüìÉ\n",
    "                delete_data(engine, ROSTER_TABLE_NAME, file_basename) #üß©\n",
    "            # Process import to Databaseüí°\n",
    "            ROSTER = (pl.read_excel(filename, infer_schema_length=None, engine=\"calamine\", sheet_name=\"Sheet1\") #üß©\n",
    "                    .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   )) #üß©\n",
    "            # SCHEMA checküìÉ\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(ROSTER, ROSTER_schema, file_basename) #üß©\n",
    "            if has_critical_error:\n",
    "                raise ValueError(critical_schema_error_msg)\n",
    "            # column structureüìÉ\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            ROSTER = ROSTER.with_columns(pl.col(\"Attribute\").cast(pl.Date)).select(ROSTER_schema) #üß© \n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(ROSTER) #üß©\n",
    "            #ImportüìÉ\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, ROSTER_TABLE_NAME, ROSTER) #üß©\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            # Uppdate log_entries for successfully importüí°\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None}) \n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            # Uppdate log_entries for fail importüí°\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception\n",
    "        ) as e:\n",
    "            error_count += 1\n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"\n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True)\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: # Old_File with modifieddate unchangedüìÉ\n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "display_summary(\"ROSTER\", error_count) #üß©\n",
    "process_and_save_log(log_df, log_entries, log_ROSTER_path) #üß©\n",
    "logger.info(\"===== Processing of the ROSTER data source is complete =====\") #üß©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f852dc5-8889-4844-b7e1-4bf3bfc6de1b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 0Ô∏è‚É£3Ô∏è‚É£[BKN]EPSüíæ\n",
    "logger.info(\"===== Start EPS Process =====\") #üß©\n",
    "log_df = read_or_create_log(log_EPS_path) #üß©\n",
    "log_entries, error_count = Default_variable()\n",
    "# Loop Folderüí°\n",
    "for filename in Folder_EPS.glob(\"*.csv\"): #üß©\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Add ModifiedDateüìÉ\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0)\n",
    "    file_basename = filename.name # Add FileNameüìÉ\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Compare FileName with LogüìÉ\n",
    "    is_new_file = log_entry.is_empty()\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1) \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Compare ModifiedDate with LogüìÉ\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # Old_File with modifieddate changedüìÉ\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # Is_New_fileüìÉ\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try:\n",
    "            if not is_new_file: #Then Delete and ImportüìÉ\n",
    "                delete_data(engine, EPS_TABLE_NAME, file_basename) #üß©\n",
    "            # Process import to Databaseüí°\n",
    "            EPS = (pl.read_csv(filename, infer_schema_length=0, encoding='latin-1') #üß©\n",
    "                   .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   ))\n",
    "            try:\n",
    "                EPS = EPS.rename({'√Ø¬ª¬øsitecode': 'sitecode'})\n",
    "            except pl.exceptions.ColumnNotFoundError:\n",
    "                pass           \n",
    "            EPS = EPS.with_columns(pl.col('Session Login', 'Session Logout').str.strptime(pl.Datetime, format='%m/%d/%Y %H:%M'))\n",
    "            EPS = EPS.with_columns(pl.col('Session Login' #Setup Login VNüìÉ\n",
    "                                ).dt.replace_time_zone(\"Europe/Berlin\", ambiguous=\"earliest\"\n",
    "                                ).dt.convert_time_zone(\"Asia/Bangkok\").alias(\"SessionLogin_VN\"))\n",
    "            EPS = EPS.with_columns(pl.col('Session Logout' #Setup Logout VNüìÉ\n",
    "                                ).dt.replace_time_zone(\"Europe/Berlin\", ambiguous=\"earliest\"\n",
    "                                ).dt.convert_time_zone(\"Asia/Bangkok\").alias(\"SessionLogout_VN\"))\n",
    "            EPS = EPS.with_columns(pl.col('SessionLogin_VN', 'SessionLogout_VN').dt.strftime('%Y-%m-%d %H:%M:%S')) \n",
    "            EPS = EPS.to_pandas()\n",
    "            time_difference(t(23,0,0),t(7,0,0))\n",
    "            EPS['SessionLogin_VN'] =  pd.to_datetime(EPS['SessionLogin_VN'])\n",
    "            EPS['SessionLogout_VN'] =  pd.to_datetime(EPS['SessionLogout_VN'])\n",
    "            EPS['Hour Difference'] = (EPS['SessionLogout_VN'] - EPS['SessionLogin_VN']).dt.total_seconds()\n",
    "            EPS['is_night_login'] = [is_time_between(t(23,0),t(7,0),i) for i in EPS['SessionLogin_VN'].dt.time]\n",
    "            EPS['is_night_logout'] = [is_time_between(t(23,0),t(7,0),i) for i in EPS['SessionLogout_VN'].dt.time]\n",
    "            EPS['Log In Adjusted'] =  [time_difference(t(23,0),i) if a == False and b == True else 0 for i,a,b in zip(EPS['SessionLogin_VN'].dt.time,EPS['is_night_login'],EPS['is_night_logout'])]\n",
    "            EPS['Log Out Adjusted'] =  [time_difference(i,t(7,0)) if a == True and b == False else 0 for i,a,b in zip(EPS['SessionLogout_VN'].dt.time,EPS['is_night_login'],EPS['is_night_logout'])]\n",
    "            EPS['NightTime'] = [i-j-k if a != False or b != False else 0 for i,j,k,a,b in zip(EPS['Hour Difference'],EPS['Log In Adjusted'],EPS['Log Out Adjusted'],EPS['is_night_login'],EPS['is_night_logout'])]\n",
    "            EPS['Hour Difference'] = [time_difference(i,j) for i,j in zip(EPS['SessionLogout_VN'].dt.time,EPS['SessionLogin_VN'].dt.time)]\n",
    "            EPS['is_day_login'] = [is_time_between(t(7,0),t(23,0),i) for i in EPS['SessionLogin_VN'].dt.time]\n",
    "            EPS['is_day_logout'] = [is_time_between(t(7,0),t(23,0),i) for i in EPS['SessionLogout_VN'].dt.time]\n",
    "            EPS['Log In Adjusted'] =  [time_difference(t(7,0),i) if a == False and b == True else 0 for i,a,b in zip(EPS['SessionLogin_VN'].dt.time,EPS['is_day_login'],EPS['is_day_logout'])]\n",
    "            EPS['Log Out Adjusted'] =  [time_difference(i,t(23,0)) if a == True and b == False else 0 for i,a,b in zip(EPS['SessionLogout_VN'].dt.time,EPS['is_day_login'],EPS['is_day_logout'])]\n",
    "            EPS['DayTime'] = [i-j-k if a != False or b != False else 0 for i,j,k,a,b in zip(EPS['Hour Difference'],EPS['Log In Adjusted'],EPS['Log Out Adjusted'],EPS['is_day_login'],EPS['is_day_logout'])]\n",
    "            EPS = pl.from_pandas(EPS)\n",
    "            EPS = EPS.with_columns(pl.col(\"Total Time\").cast(pl.Int64))\n",
    "            EPS = EPS.with_columns(pl.when(pl.col(\"Total Time\") - pl.col(\"NightTime\") < 0).then(pl.col(\"Total Time\")).otherwise(pl.col(\"NightTime\")).alias(\"Night_BPE\"))\n",
    "            EPS = EPS.with_columns(pl.when(pl.col(\"Total Time\") - pl.col(\"NightTime\") < 0).then(0).otherwise(pl.col(\"Total Time\") - pl.col(\"Night_BPE\")).alias(\"Day_BPE\"))\n",
    "            # SCHEMA checküìÉ\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(EPS, EPS_schema, file_basename) #üß©\n",
    "            if has_critical_error:\n",
    "                raise ValueError(critical_schema_error_msg)\n",
    "            # column structureüìÉ\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")    \n",
    "            EPS = (EPS.select(EPS_schema)) #üß©\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(EPS) #üß©\n",
    "            #ImportüìÉ\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, EPS_TABLE_NAME, EPS) #üß©\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            # Uppdate log_entries for successfully importüí°\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            # Uppdate log_entries for fail importüí°\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception\n",
    "        ) as e:\n",
    "            error_count += 1\n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"\n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True)\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: # Old_File with modifieddate unchangedüìÉ\n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "display_summary(\"EPS\", error_count) #üß©\n",
    "process_and_save_log(log_df, log_entries, log_EPS_path) #üß©\n",
    "logger.info(\"===== Processing of the EPS data source is complete =====\") #üß©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19724984-b665-414e-8b02-a3ad8108c1fc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 0Ô∏è‚É£4Ô∏è‚É£[BKN]CPIüíæ\n",
    "logger.info(\"===== Start CPI Process =====\") #üß©\n",
    "log_df = read_or_create_log(log_CPI_path) #üß©\n",
    "log_entries, error_count = Default_variable()\n",
    "# Loop Folderüí°\n",
    "for filename in Folder_CPI.glob(\"*.csv\"): #üß©\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Add ModifiedDateüìÉ\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0)\n",
    "    file_basename = filename.name # Add FileNameüìÉ\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Compare FileName with LogüìÉ\n",
    "    is_new_file = log_entry.is_empty()\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1) \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Compare ModifiedDate with LogüìÉ\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # Old_File with modifieddate changedüìÉ\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # Is_New_fileüìÉ\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try:\n",
    "            if not is_new_file: #Then Delete and ImportüìÉ\n",
    "                delete_data(engine, CPI_TABLE_NAME, file_basename) #üß©\n",
    "            # Process import to Databaseüí°\n",
    "            CPI = (pl.read_csv(filename, infer_schema_length=None) #üß©\n",
    "                   .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   ))\n",
    "            try:\n",
    "                CPI = CPI.with_columns(pl.col('Nr. Contacts').cast(pl.Int64))\n",
    "            except:\n",
    "                CPI = CPI.with_columns(pl.col(\"Nr. Contacts\").str.replace_all(r\"[()]\", \"\").cast(pl.Int64).alias(\"Nr. Contacts\"))\n",
    "            CPI = CPI.with_columns(\n",
    "                pl.when(pl.col(\"Nr. Contacts\").is_null()).then(1).otherwise(pl.col(\"Nr. Contacts\").abs()).alias(\"Nr. Contacts\"))\n",
    "            if \"Date\" not in CPI.columns: # Extract Date from filename if not already presentüìÉ\n",
    "                try:\n",
    "                    CPI = CPI.with_columns( \n",
    "                        pl.col(\"FileName\")\n",
    "                        .str.replace(\".csv\", \"\", literal=True)\n",
    "                        .str.to_datetime(\"%Y%m%d\", strict=False)\n",
    "                        .cast(pl.Date)\n",
    "                        .alias(\"Date\")\n",
    "                    )\n",
    "                except pl.exceptions.ParseError:\n",
    "                    print(f\"Unable to extract date from filename: {file_basename}\")\n",
    "                    CPI = CPI.with_columns(pl.lit(None).cast(pl.Date).alias(\"Date\"))\n",
    "            CPI = CPI.rename({'Time ': 'Time'}) #üß©\n",
    "            # SCHEMA checküìÉ\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(CPI, CPI_schema, file_basename) #üß©\n",
    "            if has_critical_error:\n",
    "                raise ValueError(critical_schema_error_msg)\n",
    "            # column structureüìÉ\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            CPI = CPI.with_columns(parse_date(pl.col(\"Date\")).alias(\"Date\"),).select(CPI_schema) #üß©\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(CPI) #üß©\n",
    "            #ImportüìÉ\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, CPI_TABLE_NAME, CPI) #üß©\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            # Uppdate log_entries for successfully importüí°\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            # Uppdate log_entries for fail importüí°\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception\n",
    "        ) as e:\n",
    "            error_count += 1\n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"\n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True)\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: # Old_File with modifieddate unchangedüìÉ\n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "display_summary(\"CPI\", error_count) #üß©\n",
    "process_and_save_log(log_df, log_entries, log_CPI_path) #üß©\n",
    "logger.info(\"===== Processing of the CPI data source is complete =====\") #üß©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4a35d7-ced3-4579-8991-f0cb3177e090",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 0Ô∏è‚É£5Ô∏è‚É£[GLB]RAMCOüíæ\n",
    "logger.info(\"===== Start RAMCO Process =====\") #üß©\n",
    "log_df = read_or_create_log(log_RAMCO_path) #üß©\n",
    "log_entries, error_count = Default_variable()\n",
    "# Loop Folderüí°\n",
    "for filename in Folder_RAMCO.glob(\"*.csv\"): #üß©\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Add ModifiedDateüìÉ\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0)\n",
    "    file_basename = filename.name # Add FileNameüìÉ\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Compare FileName with LogüìÉ\n",
    "    is_new_file = log_entry.is_empty()\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1) \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Compare ModifiedDate with LogüìÉ\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # Old_File with modifieddate changedüìÉ\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # Is_New_fileüìÉ\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try:\n",
    "            if not is_new_file: #Then Delete and ImportüìÉ\n",
    "                delete_data(engine, RAMCO_TABLE_NAME, file_basename) #üß©\n",
    "            # Process import to Databaseüí°\n",
    "            RAMCO = (pl.read_csv(filename, infer_schema_length=0, encoding='latin-1') #üß©\n",
    "                       .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")))\n",
    "            RAMCO = RAMCO.with_columns(\n",
    "                pl.col('Attribute').str.strptime(pl.Date, format='%m/%d/%Y'),\n",
    "                pl.col('EID').cast(pl.Int64)).rename({'Attribute': 'Date', 'Value': 'Code'}) #üß©\n",
    "            # SCHEMA checküìÉ\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(RAMCO, RAMCO_schema, file_basename) #üß©\n",
    "            if has_critical_error:\n",
    "                raise ValueError(critical_schema_error_msg)\n",
    "            # column structureüìÉ\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            RAMCO = RAMCO.select(RAMCO_schema) #üß©\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(RAMCO) #üß©\n",
    "            #ImportüìÉ\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, RAMCO_TABLE_NAME, RAMCO) #üß©\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            # Uppdate log_entries for successfully importüí°\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            # Uppdate log_entries for fail importüí°\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception\n",
    "        ) as e:\n",
    "            error_count += 1\n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"\n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True)\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: # Old_File with modifieddate unchangedüìÉ\n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "display_summary(\"RAMCO\", error_count) #üß©\n",
    "process_and_save_log(log_df, log_entries, log_RAMCO_path) #üß©\n",
    "logger.info(\"===== Processing of the RAMCO data source is complete =====\") #üß©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067593bc-a572-472f-808c-07aa0c6e080f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 0Ô∏è‚É£6Ô∏è‚É£[GLB]OT_RAMCOüíæ\n",
    "logger.info(\"===== Start OT_ROSTER Process =====\") #üß©\n",
    "log_df = read_or_create_log(log_OT_RAMCO_path) #üß©\n",
    "log_entries, error_count = Default_variable()\n",
    "# Loop Folderüí°\n",
    "for filename in Folder_OT_RAMCO.glob(\"*.xlsx\"): #üß©\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Add ModifiedDateüìÉ\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0)\n",
    "    file_basename = filename.name # Add FileNameüìÉ\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Compare FileName with LogüìÉ\n",
    "    is_new_file = log_entry.is_empty()\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1) \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Compare ModifiedDate with LogüìÉ\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # Old_File with modifieddate changedüìÉ\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # Is_New_fileüìÉ\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try:\n",
    "            if not is_new_file: #Then Delete and ImportüìÉ\n",
    "                delete_data(engine, OT_RAMCO_TABLE_NAME, file_basename) #üß©\n",
    "            # Process import to Databaseüí°\n",
    "            OT_RAMCO = (pl.read_excel(filename, infer_schema_length=None, engine=\"calamine\", sheet_name=\"Sheet1\") #üß©\n",
    "                   .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   ).rename({'Attribute': 'Date'})) #üß©\n",
    "            # SCHEMA checküìÉ\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(OT_RAMCO, OT_RAMCO_schema, file_basename) #üß©\n",
    "            if has_critical_error:\n",
    "                raise ValueError(critical_schema_error_msg)\n",
    "            # column structureüìÉ\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            OT_RAMCO = OT_RAMCO.select(OT_RAMCO_schema) #üß©\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(OT_RAMCO) #üß©\n",
    "            #ImportüìÉ\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, OT_RAMCO_TABLE_NAME, OT_RAMCO) #üß©\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            # Uppdate log_entries for successfully importüí°\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            # Uppdate log_entries for fail importüí°\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception\n",
    "        ) as e:\n",
    "            error_count += 1\n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"\n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True)\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: # Old_File with modifieddate unchangedüìÉ\n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "display_summary(\"OT_RAMCO\", error_count) #üß©\n",
    "process_and_save_log(log_df, log_entries, log_OT_RAMCO_path) #üß©\n",
    "logger.info(\"===== Processing of the OT_RAMCO data source is complete =====\") #üß©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57aab6b-a5dc-4fce-a47b-3593c4ff2c7c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 0Ô∏è‚É£7Ô∏è‚É£[GLB]PremHdaysüíæ\n",
    "logger.info(\"===== Start PremHdays Process =====\") #üß©\n",
    "log_df = read_or_create_log(log_PremHdays_path) #üß©\n",
    "log_entries, error_count = Default_variable()\n",
    "# Loop Folderüí°\n",
    "for filename in Folder_PremHdays.glob(\"*.csv\"): #üß©\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Add ModifiedDateüìÉ\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0)\n",
    "    file_basename = filename.name # Add FileNameüìÉ\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Compare FileName with LogüìÉ\n",
    "    is_new_file = log_entry.is_empty()\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1) \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Compare ModifiedDate with LogüìÉ\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # Old_File with modifieddate changedüìÉ\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # Is_New_fileüìÉ\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try:\n",
    "            if not is_new_file: #Then Delete and ImportüìÉ\n",
    "                delete_data(engine, PremHdays_TABLE_NAME, file_basename) #üß©\n",
    "            # Process import to Databaseüí°\n",
    "            PremHdays = (pl.read_csv(filename, infer_schema_length=None) #üß©\n",
    "                    .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   )) # Import Schemaüß©\n",
    "            # SCHEMA checküìÉ\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(PremHdays, PremHdays_schema, file_basename) #üß©\n",
    "            if has_critical_error:\n",
    "                raise ValueError(critical_schema_error_msg)\n",
    "            # column structureüìÉ\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            PremHdays = PremHdays.with_columns(pl.col('Date').str.strptime(pl.Date, format='%m/%d/%Y')) #üß©\n",
    "            PremHdays = PremHdays.select(PremHdays_schema) #üß©\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(PremHdays) #üß©\n",
    "            #ImportüìÉ\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, PremHdays_TABLE_NAME, PremHdays) #üß©\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            # Uppdate log_entries for successfully importüí°\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            # Uppdate log_entries for fail importüí°\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception\n",
    "        ) as e:\n",
    "            error_count += 1\n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"\n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True)\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: # Old_File with modifieddate unchangedüìÉ\n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "display_summary(\"PremHdays\", error_count) #üß©\n",
    "process_and_save_log(log_df, log_entries, log_PremHdays_path) #üß©\n",
    "logger.info(\"===== Processing of the PremHdays data source is complete =====\") #üß©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ba2009-31b0-4ec4-a511-2cddfa75be08",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 0Ô∏è‚É£8Ô∏è‚É£[GLB]NormHdaysüíæ\n",
    "logger.info(\"===== Start NormHdays Process =====\") #üß©\n",
    "log_df = read_or_create_log(log_NormHdays_path) #üß©\n",
    "log_entries, error_count = Default_variable()\n",
    "# Loop Folderüí°\n",
    "for filename in Folder_NormHdays.glob(\"*.csv\"): #üß©\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Add ModifiedDateüìÉ\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0)\n",
    "    file_basename = filename.name # Add FileNameüìÉ\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Compare FileName with LogüìÉ\n",
    "    is_new_file = log_entry.is_empty()\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1) \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Compare ModifiedDate with LogüìÉ\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # Old_File with modifieddate changedüìÉ\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # Is_New_fileüìÉ\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try:\n",
    "            if not is_new_file: #Then Delete and ImportüìÉ\n",
    "                delete_data(engine, NormHdays_TABLE_NAME, file_basename) #üß©\n",
    "            # Process import to Databaseüí°\n",
    "            NormHdays = (pl.read_csv(filename, infer_schema_length=None) #üß©\n",
    "                    .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   )) #üß©\n",
    "            # SCHEMA checküìÉ\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(NormHdays, NormHdays_schema, file_basename) #üß©\n",
    "            if has_critical_error:\n",
    "                raise ValueError(critical_schema_error_msg)\n",
    "            # column structureüìÉ\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            NormHdays = NormHdays.with_columns(pl.col('Solar Day', 'Lunar Day').str.strptime(pl.Date, format='%m/%d/%Y')) #üß©\n",
    "            NormHdays = NormHdays.select(NormHdays_schema) #üß©\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(NormHdays) #üß©\n",
    "            #ImportüìÉ\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, NormHdays_TABLE_NAME, NormHdays) #üß©\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            # Uppdate log_entries for successfully importüí°\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            # Uppdate log_entries for fail importüí°\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception\n",
    "        ) as e:\n",
    "            error_count += 1\n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"\n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True)\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: # Old_File with modifieddate unchangedüìÉ\n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "display_summary(\"NormHdays\", error_count) #üß©\n",
    "process_and_save_log(log_df, log_entries, log_NormHdays_path) #üß©\n",
    "logger.info(\"===== Processing of the NormHdays data source is complete =====\") #üß©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cef690e-9148-4a5a-8159-5500e1b67826",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 0Ô∏è‚É£9Ô∏è‚É£[GLB]EmpMasterüíæ\n",
    "logger.info(\"===== Start EmpMaster Process =====\") #üß©\n",
    "log_df = read_or_create_log(log_EmpMaster_path) #üß©\n",
    "log_entries, error_count = Default_variable()\n",
    "# Loop Folderüí°\n",
    "for filename in Folder_EmpMaster.glob(\"*.xlsx\"): #üß©\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Add ModifiedDateüìÉ\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0)\n",
    "    file_basename = filename.name # Add FileNameüìÉ\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Compare FileName with LogüìÉ\n",
    "    is_new_file = log_entry.is_empty()\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1) \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Compare ModifiedDate with LogüìÉ\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # Old_File with modifieddate changedüìÉ\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # Is_New_fileüìÉ\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try:\n",
    "            if not is_new_file: #Then Delete and ImportüìÉ\n",
    "                delete_data(engine, EmpMaster_TABLE_NAME, file_basename) #üß©\n",
    "            # Process import to Databaseüí°\n",
    "            EmpMaster = (pl.read_excel(filename, infer_schema_length=None, engine=\"calamine\", sheet_name=\"Employee Master\") #üß©\n",
    "                   .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   )) #üß©\n",
    "            # SCHEMA checküìÉ\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(EmpMaster, EmpMaster_schema, file_basename) #üß©\n",
    "            if has_critical_error:\n",
    "                raise ValueError(critical_schema_error_msg)\n",
    "            # column structureüìÉ\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            EmpMaster = EmpMaster.with_columns(pl.col(\"ORIGINAL_DATE_OF_HIRE\", \"LEGAL_EMPLOYER_HIRE_DATE\", \"Continuous Service Date\", \\\n",
    "                                                      \"Fixed Term Hire End Date\", \"Contract End Date\", \"Last Promotion Date\",).cast(pl.Date)) #üß©\n",
    "            EmpMaster = EmpMaster.select(EmpMaster_schema) #üß©\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(EmpMaster) #üß©\n",
    "            #ImportüìÉ\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, EmpMaster_TABLE_NAME, EmpMaster) #üß©\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            # Uppdate log_entries for successfully importüí°\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            # Uppdate log_entries for fail importüí°\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception\n",
    "        ) as e:\n",
    "            error_count += 1\n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"\n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True)\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: # Old_File with modifieddate unchangedüìÉ\n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "display_summary(\"EmpMaster\", error_count) #üß©\n",
    "process_and_save_log(log_df, log_entries, log_EmpMaster_path) #üß©\n",
    "logger.info(\"===== Processing of the EmpMaster data source is complete =====\") #üß©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd483483-5974-474e-bc42-7cf5f97eb53a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 1Ô∏è‚É£0Ô∏è‚É£[GLB]Terminationüíæ\n",
    "logger.info(\"===== Start Termination Process =====\") #üß©\n",
    "log_df = read_or_create_log(log_Termination_path) #üß©\n",
    "log_entries, error_count = Default_variable()\n",
    "# Loop Folderüí°\n",
    "for filename in Folder_Termination.glob(\"*.xlsx\"): #üß©\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Add ModifiedDateüìÉ\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0)\n",
    "    file_basename = filename.name # Add FileNameüìÉ\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Compare FileName with LogüìÉ\n",
    "    is_new_file = log_entry.is_empty()\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1) \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Compare ModifiedDate with LogüìÉ\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # Old_File with modifieddate changedüìÉ\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # Is_New_fileüìÉ\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try:\n",
    "            if not is_new_file: #Then Delete and ImportüìÉ\n",
    "                delete_data(engine, Termination_TABLE_NAME, file_basename) #üß©\n",
    "            # Process import to Databaseüí°\n",
    "            Termination = (pl.read_excel(filename, infer_schema_length=None, engine=\"calamine\", sheet_name=\"Termination Dump\") #üß©\n",
    "                    .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   )) #üß©\n",
    "            # SCHEMA checküìÉ\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(Termination, Termination_schema, file_basename) #üß©\n",
    "            if has_critical_error:\n",
    "                raise ValueError(critical_schema_error_msg)\n",
    "            # column structureüìÉ\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            Termination = Termination.with_columns(pl.col(\"HIRE_DATE\", \"ORIGINAL_HIRE_DATE\", \"END EMPLOYMENT DATE\", \n",
    "                                                          \"Contract End Date\", \"Termination Date\", \"Termination Date (DD/MM/YY)\", \n",
    "                                                          \"LWD\", \"MOST RECENT TERMINATION - DATE INITIATED\", \"MOST RECENT TERMINATION - DATE COMPLETED\",\n",
    "                                                          \"MOST RECENT TERMINATION - EFFECTIVE DATE\", \"DATE INITIATED\",\n",
    "                                                          \"TERMINATION DATE 2\", \"Continuous Service Date\").cast(pl.Date)) #üß©\n",
    "            Termination = Termination.with_columns(pl.col(\"Action date\", \"COMPELETED DATE AND TIME\").cast(pl.Datetime)) #üß©\n",
    "            Termination = Termination.with_columns(pl.col(\"Resignation Notice served\").cast(pl.Int64)) #üß©\n",
    "            Termination = Termination.select(Termination_schema) #üß©\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(Termination) #üß©\n",
    "            #ImportüìÉ\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, Termination_TABLE_NAME, Termination) #üß©\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            # Uppdate log_entries for successfully importüí°\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            # Uppdate log_entries for fail importüí°\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception\n",
    "        ) as e:\n",
    "            error_count += 1\n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"\n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True)\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: # Old_File with modifieddate unchangedüìÉ\n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "display_summary(\"Termination\", error_count) #üß©\n",
    "process_and_save_log(log_df, log_entries, log_Termination_path) #üß©\n",
    "logger.info(\"===== Processing of the Termination data source is complete =====\") #üß©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cef9f2-5889-464e-9daf-9b93077f1b85",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 1Ô∏è‚É£1Ô∏è‚É£[GLB]Resignationüíæ\n",
    "logger.info(\"===== Start Resignation Process =====\") #üß©\n",
    "log_df = read_or_create_log(log_Resignation_path) #üß©\n",
    "log_entries, error_count = Default_variable()\n",
    "# Loop Folderüí°\n",
    "for filename in Folder_Resignation.glob(\"*.xlsx\"): #üß©\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Add ModifiedDateüìÉ\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0)\n",
    "    file_basename = filename.name # Add FileNameüìÉ\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Compare FileName with LogüìÉ\n",
    "    is_new_file = log_entry.is_empty()\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1) \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Compare ModifiedDate with LogüìÉ\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # Old_File with modifieddate changedüìÉ\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # Is_New_fileüìÉ\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try:\n",
    "            if not is_new_file: #Then Delete and ImportüìÉ\n",
    "                delete_data(engine, Resignation_TABLE_NAME, file_basename) #üß©\n",
    "            # Process import to Databaseüí°\n",
    "            Resignation = (pl.read_excel(filename, infer_schema_length=None, engine=\"calamine\", sheet_name=\"Resignation\") #üß©\n",
    "                   .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   )) #üß©\n",
    "            # SCHEMA checküìÉ\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(Resignation, Resignation_schema, file_basename) #üß©\n",
    "            if has_critical_error:\n",
    "                raise ValueError(critical_schema_error_msg)\n",
    "            # column structureüìÉ\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            Resignation = Resignation.with_columns(pl.col(\"Notification Date\", \"Hire Date\", \"Proposed Termination Date\",).cast(pl.Date)) #üß©\n",
    "            Resignation = Resignation.with_columns(pl.col(\"Action Date\", \"Date and Time Initiated\").cast(pl.Datetime)) #üß©\n",
    "            Resignation = Resignation.with_columns(pl.col(\"Notice Served\").cast(pl.Int64)) #üß©\n",
    "            Resignation = Resignation.select(Resignation_schema) #üß©\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(Resignation) #üß©\n",
    "            #ImportüìÉ\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, Resignation_TABLE_NAME, Resignation) #üß©\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            # Uppdate log_entries for successfully importüí°\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            # Uppdate log_entries for fail importüí°\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception\n",
    "        ) as e:\n",
    "            error_count += 1\n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"\n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True)\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: # Old_File with modifieddate unchangedüìÉ\n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "display_summary(\"Resignation\", error_count) #üß©\n",
    "process_and_save_log(log_df, log_entries, log_Resignation_path) #üß©\n",
    "logger.info(\"===== Processing of the Resignation data source is complete =====\") #üß©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3e7e4c-fd0a-48d5-8ca6-801fdc2a0240",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 1Ô∏è‚É£2Ô∏è‚É£[BKN]CPI_PEGAüíæ\n",
    "logger.info(\"===== Start CPI_PEGA Process =====\") #üß©\n",
    "log_df = read_or_create_log(log_CPI_PEGA_path) #üß©\n",
    "log_entries, error_count = Default_variable()\n",
    "# Loop Folderüí°\n",
    "for filename in Folder_CPI_PEGA.glob(\"*.csv\"): #üß©\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Add ModifiedDateüìÉ\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0)\n",
    "    file_basename = filename.name # Add FileNameüìÉ\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Compare FileName with LogüìÉ\n",
    "    is_new_file = log_entry.is_empty()\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1) \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Compare ModifiedDate with LogüìÉ\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # Old_File with modifieddate changedüìÉ\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # Is_New_fileüìÉ\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try:\n",
    "            if not is_new_file: #Then Delete and ImportüìÉ\n",
    "                delete_data(engine, CPI_PEGA_TABLE_NAME, file_basename) #üß©\n",
    "            # Process import to Databaseüí°\n",
    "            CPI_PEGA = (pl.read_csv(filename, infer_schema_length=None) #üß©\n",
    "                    .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   )) #üß©\n",
    "            # SCHEMA checküìÉ\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(CPI_PEGA, CPI_PEGA_schema, file_basename) #üß©\n",
    "            if has_critical_error:\n",
    "                raise ValueError(critical_schema_error_msg)\n",
    "            # column structureüìÉ\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            CPI_PEGA = CPI_PEGA.with_columns(parse_date(pl.col(\"Day of Date\")).alias(\"Day of Date\")) #üß©\n",
    "            CPI_PEGA = CPI_PEGA.with_columns(pl.col(\"# Swivels\", \"Count of ServiceCase or Interaction\").cast(pl.Int64)) #üß©\n",
    "            CPI_PEGA = CPI_PEGA.select(CPI_PEGA_schema) #üß©\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(CPI_PEGA) #üß©   \n",
    "            #ImportüìÉ\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, CPI_PEGA_TABLE_NAME, CPI_PEGA) #üß©\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            # Uppdate log_entries for successfully importüí°\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            # Uppdate log_entries for fail importüí°\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception\n",
    "        ) as e:\n",
    "            error_count += 1\n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"\n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True)\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: # Old_File with modifieddate unchangedüìÉ\n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "display_summary(\"CPI_PEGA\", error_count) #üß©\n",
    "process_and_save_log(log_df, log_entries, log_CPI_PEGA_path) #üß©\n",
    "logger.info(\"===== Processing of the CPI_PEGA data source is complete =====\") #üß©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c00afc0-01a4-4c18-a3d9-8303ba35b4e9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 1Ô∏è‚É£3Ô∏è‚É£[BKN]Staffüíæ\n",
    "logger.info(\"===== Start Staff Process =====\") #üß©\n",
    "log_df = read_or_create_log(log_Staff_path) #üß©\n",
    "log_entries, error_count = Default_variable()\n",
    "# Loop Folderüí°\n",
    "for filename in Folder_Staff.glob(\"*.xlsx\"): #üß©\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Add ModifiedDateüìÉ\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0)\n",
    "    file_basename = filename.name # Add FileNameüìÉ\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Compare FileName with LogüìÉ\n",
    "    is_new_file = log_entry.is_empty()\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1) \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Compare ModifiedDate with LogüìÉ\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # Old_File with modifieddate changedüìÉ\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # Is_New_fileüìÉ\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try:\n",
    "            if not is_new_file: #Then Delete and ImportüìÉ\n",
    "                delete_data(engine, Staff_TABLE_NAME, file_basename) #üß©\n",
    "            # Process import to Databaseüí°\n",
    "            Staff = (pl.read_excel(filename, infer_schema_length=None, engine=\"calamine\", sheet_name=\"Sheet1\") #üß©\n",
    "                   .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   )) #üß©\n",
    "            # SCHEMA checküìÉ\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(Staff, Staff_schema, file_basename) #üß©\n",
    "            if has_critical_error:\n",
    "                raise ValueError(critical_schema_error_msg)\n",
    "            # column structureüìÉ\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            Staff = Staff.with_columns(pl.col(\"Language Start Date\", \"Hire_Date\", \"PST_Start_Date\", \"Production_Start_Date\", \n",
    "                                              \"LWD\", \"Termination_Date\").cast(pl.Date))\n",
    "            Staff = Staff.select(Staff_schema)\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(Staff) #üß©\n",
    "            #ImportüìÉ\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, Staff_TABLE_NAME, Staff) #üß©\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            # Uppdate log_entries for successfully importüí°\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            # Uppdate log_entries for fail importüí°\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception\n",
    "        ) as e:\n",
    "            error_count += 1\n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"\n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True)\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: # Old_File with modifieddate unchangedüìÉ\n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "display_summary(\"Staff\", error_count) #üß©\n",
    "process_and_save_log(log_df, log_entries, log_Staff_path) #üß©\n",
    "logger.info(\"===== Processing of the Staff data source is complete =====\") #üß©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbc1428-f85c-434d-b7ab-578e9939c33f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 1Ô∏è‚É£4Ô∏è‚É£[BKN]ConTracküíæ\n",
    "logger.info(\"===== Start ConTrack Process =====\") #üß©\n",
    "log_df = read_or_create_log(log_ConTrack_path) #üß©\n",
    "log_entries, error_count = Default_variable()\n",
    "# Loop Folderüí°\n",
    "for filename in Folder_ConTrack.glob(\"*.xlsx\"): #üß©\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Add ModifiedDateüìÉ\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0)\n",
    "    file_basename = filename.name # Add FileNameüìÉ\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Compare FileName with LogüìÉ\n",
    "    is_new_file = log_entry.is_empty()\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1) \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Compare ModifiedDate with LogüìÉ\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # Old_File with modifieddate changedüìÉ\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # Is_New_fileüìÉ\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try:\n",
    "            if not is_new_file: #Then Delete and ImportüìÉ\n",
    "                delete_data(engine, ConTrack_TABLE_NAME, file_basename) #üß©\n",
    "            # Process import to Databaseüí°\n",
    "            ConTrack = (pl.read_excel(filename, infer_schema_length=1000, sheet_name=\"Sheet1\",\n",
    "                         schema_overrides={\"Reservation Number\": pl.String, \"Note\": pl.String, \n",
    "                                           \"Reason If Skipped\": pl.String}) #üß©\n",
    "                   .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   )) #üß©\n",
    "            # SCHEMA checküìÉ\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(ConTrack, ConTrack_schema, file_basename) #üß©\n",
    "            if has_critical_error:\n",
    "                raise ValueError(critical_schema_error_msg)\n",
    "            # column structureüìÉ\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            ConTrack = ConTrack.with_columns(pl.col(\"Start time\", \"Completion time\").cast(pl.Datetime)) #üß©\n",
    "            ConTrack = ConTrack.select(ConTrack_schema) #üß©\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(ConTrack) #üß©\n",
    "            #ImportüìÉ\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, ConTrack_TABLE_NAME, ConTrack) #üß©\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            # Uppdate log_entries for successfully importüí°\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            # Uppdate log_entries for fail importüí°\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception\n",
    "        ) as e:\n",
    "            error_count += 1\n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"\n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True)\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: # Old_File with modifieddate unchangedüìÉ\n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "display_summary(\"ConTrack\", error_count) #üß©\n",
    "process_and_save_log(log_df, log_entries, log_ConTrack_path) #üß©\n",
    "logger.info(\"===== Processing of the ConTrack data source is complete =====\") #üß©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c86925-ee19-44ee-9810-1971b64f54ae",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 1Ô∏è‚É£5Ô∏è‚É£[BKN]Qualityüíæ\n",
    "logger.info(\"===== Start Quality Process =====\") #üß©\n",
    "log_df = read_or_create_log(log_Quality_path) #üß©\n",
    "log_entries, error_count = Default_variable()\n",
    "# Loop Folderüí°\n",
    "for filename in Folder_Quality.glob(\"*.xlsx\"): #üß©\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Add ModifiedDateüìÉ\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0)\n",
    "    file_basename = filename.name # Add FileNameüìÉ\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Compare FileName with LogüìÉ\n",
    "    is_new_file = log_entry.is_empty()\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1) \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Compare ModifiedDate with LogüìÉ\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # Old_File with modifieddate changedüìÉ\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # Is_New_fileüìÉ\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try:\n",
    "            if not is_new_file: #Then Delete and ImportüìÉ\n",
    "                delete_data(engine, Quality_TABLE_NAME, file_basename) #üß©\n",
    "            # Process import to Databaseüí°\n",
    "            Quality = (pl.read_excel(filename, infer_schema_length=None, engine=\"calamine\", sheet_name=\"Sheet1\") #üß©\n",
    "                   .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   ).rename({' score_question_weight': 'score_question_weight'})) #üß©\n",
    "            # SCHEMA checküìÉ\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(Quality, Quality_schema, file_basename) #üß©\n",
    "            if has_critical_error:\n",
    "                raise ValueError(critical_schema_error_msg)\n",
    "            # column structureüìÉ\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            Quality = Quality.with_columns(pl.col(\"eval_date\").cast(pl.Date)) #üß©\n",
    "            Quality = Quality.with_columns(pl.col(\"score_n\",\"score_question_weight\").cast(pl.Int64)) #üß©\n",
    "            Quality = Quality.select(Quality_schema) #üß©\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(Quality) #üß©  \n",
    "            #ImportüìÉ\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, Quality_TABLE_NAME, Quality) #üß©\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            # Uppdate log_entries for successfully importüí°\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            # Uppdate log_entries for fail importüí°\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception\n",
    "        ) as e:\n",
    "            error_count += 1\n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"\n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True)\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: # Old_File with modifieddate unchangedüìÉ\n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "display_summary(\"Quality\", error_count) #üß©\n",
    "process_and_save_log(log_df, log_entries, log_Quality_path) #üß©\n",
    "logger.info(\"===== Processing of the Quality data source is complete =====\") #üß©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3f262a-62bf-4d19-9642-91c9049c0f3c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 1Ô∏è‚É£6Ô∏è‚É£[BKN]RONAüíæ\n",
    "logger.info(\"===== Start RONA Process =====\") #üß©\n",
    "log_df = read_or_create_log(log_RONA_path) #üß©\n",
    "log_entries, error_count = Default_variable()\n",
    "# Loop Folderüí°\n",
    "for filename in Folder_RONA.glob(\"*.xlsx\"): #üß©\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Add ModifiedDateüìÉ\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0)\n",
    "    file_basename = filename.name # Add FileNameüìÉ\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Compare FileName with LogüìÉ\n",
    "    is_new_file = log_entry.is_empty()\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1) \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Compare ModifiedDate with LogüìÉ\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # Old_File with modifieddate changedüìÉ\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # Is_New_fileüìÉ\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try:\n",
    "            if not is_new_file: #Then Delete and ImportüìÉ\n",
    "                delete_data(engine, RONA_TABLE_NAME, file_basename) #üß©\n",
    "            # Process import to Databaseüí°\n",
    "            RONA = (pl.read_excel(filename, infer_schema_length=None, engine=\"calamine\", sheet_name=\"HCM_RONA-Agent Team Historical \") #üß©\n",
    "                   .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   )) #üß©\n",
    "            # SCHEMA checküìÉ\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(RONA, RONA_schema, file_basename) #üß©\n",
    "            if has_critical_error:\n",
    "                raise ValueError(critical_schema_error_msg)\n",
    "            # column structureüìÉ\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            RONA = RONA.select(RONA_schema) #üß©\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(RONA) #üß©\n",
    "            #ImportüìÉ\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, RONA_TABLE_NAME, RONA) #üß©\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            # Uppdate log_entries for successfully importüí°\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            # Uppdate log_entries for fail importüí°\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception\n",
    "        ) as e:\n",
    "            error_count += 1\n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"\n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True)\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: # Old_File with modifieddate unchangedüìÉ\n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "display_summary(\"RONA\", error_count) #üß©\n",
    "process_and_save_log(log_df, log_entries, log_RONA_path) #üß©\n",
    "logger.info(\"===== Processing of the RONA data source is complete =====\") #üß©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f031ed1f-f439-4022-b27c-02b5bd9031f8",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1Ô∏è‚É£7Ô∏è‚É£[BKN]CUICüíæ\n",
    "logger.info(\"===== Start CUIC Process =====\") #üß©\n",
    "log_df = read_or_create_log(log_CUIC_path) #üß©\n",
    "log_entries, error_count = Default_variable()\n",
    "# Loop Folderüí°\n",
    "for filename in Folder_CUIC.glob(\"*.xlsx\"): #üß©\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Add ModifiedDateüìÉ\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0)\n",
    "    file_basename = filename.name # Add FileNameüìÉ\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Compare FileName with LogüìÉ\n",
    "    is_new_file = log_entry.is_empty()\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1) \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Compare ModifiedDate with LogüìÉ\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # Old_File with modifieddate changedüìÉ\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # Is_New_fileüìÉ\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try:\n",
    "            if not is_new_file: #Then Delete and ImportüìÉ\n",
    "                delete_data(engine, CUIC_TABLE_NAME, file_basename) #üß©\n",
    "            # Process import to Databaseüí°\n",
    "            CUIC = (pl.read_excel(filename, infer_schema_length=None, engine=\"calamine\", sheet_name=\"Attendance Check-Sample_Attenda\") #üß©\n",
    "                    .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   )) #üß©\n",
    "            # SCHEMA checküìÉ\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(CUIC, CUIC_schema, file_basename) #üß©\n",
    "            if has_critical_error:\n",
    "                raise ValueError(critical_schema_error_msg)\n",
    "            # column structureüìÉ\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            CUIC = CUIC.with_columns(pl.col(\"Interval\").str.strptime(pl.Datetime, \"%m/%d/%y %I:%M:%S %p\")) #üß©\n",
    "            CUIC = CUIC.with_columns(pl.col(\"AgentAvailTime\", \"AgentLoggedOnTime\").cast(pl.Float64)) #üß©\n",
    "            CUIC = CUIC.select(CUIC_schema) #üß©\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(CUIC) #üß© \n",
    "            #ImportüìÉ\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, CUIC_TABLE_NAME, CUIC) #üß©\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            # Uppdate log_entries for successfully importüí°\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            # Uppdate log_entries for fail importüí°\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception\n",
    "        ) as e:\n",
    "            error_count += 1\n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"\n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True)\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: # Old_File with modifieddate unchangedüìÉ\n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "display_summary(\"CUIC\", error_count) #üß©\n",
    "process_and_save_log(log_df, log_entries, log_CUIC_path) #üß©\n",
    "logger.info(\"===== Processing of the CUIC data source is complete =====\") #üß©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1009fa11-fceb-4a37-9be5-8452bcec3e46",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 1Ô∏è‚É£8Ô∏è‚É£[BKN]KPI_Targetüíæ\n",
    "logger.info(\"===== Start KPI_Target Process =====\") #üß©\n",
    "log_df = read_or_create_log(log_KPI_Target_path) #üß©\n",
    "log_entries, error_count = Default_variable()\n",
    "# Loop Folderüí°\n",
    "for filename in Folder_KPI_Target.glob(\"*.xlsx\"): #üß©\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Add ModifiedDateüìÉ\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0)\n",
    "    file_basename = filename.name # Add FileNameüìÉ\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Compare FileName with LogüìÉ\n",
    "    is_new_file = log_entry.is_empty()\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1) \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Compare ModifiedDate with LogüìÉ\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # Old_File with modifieddate changedüìÉ\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # Is_New_fileüìÉ\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try:\n",
    "            if not is_new_file: #Then Delete and ImportüìÉ\n",
    "                delete_data(engine, KPI_Target_TABLE_NAME, file_basename) #üß©\n",
    "            # Process import to Databaseüí°\n",
    "            KPI_Target = (pl.read_excel(filename, infer_schema_length=None, engine=\"calamine\", sheet_name=\"Sheet1\") #üß©\n",
    "                   .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   )) #üß©\n",
    "            # SCHEMA checküìÉ\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(KPI_Target, KPI_Target_schema, file_basename) #üß©\n",
    "            if has_critical_error:\n",
    "                raise ValueError(critical_schema_error_msg)\n",
    "            # column structureüìÉ\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            KPI_Target = KPI_Target.with_columns(pl.col(\"Week\").cast(pl.Int64)) #üß©\n",
    "            KPI_Target = KPI_Target.with_columns(pl.col('Overall CPH tar', 'Phone CPH tar', 'Non Phone CPH tar', 'Quality - Customer Impact tar', \n",
    "                                                        'Quality - Business Impact tar', 'Quality - Compliance Impact tar', 'Quality - Overall tar', \n",
    "                                                        'AHT Phone tar', 'AHT Non-phone tar', 'AHT Overall tar', 'Hold (phone) tar', 'AACW (phone) tar', \n",
    "                                                        'Avg Talk Time tar', 'Phone CSAT tar', 'Non phone CSAT tar', 'Overall CSAT tar', 'PSAT tar', \n",
    "                                                        'PSAT Vietnamese tar', 'PSAT English (American) tar', 'PSAT English (Great Britain) tar', 'CSAT Reso tar').cast(pl.Float64)) # Import Schemaüß©\n",
    "            KPI_Target = KPI_Target.select(KPI_Target_schema) #üß©\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(KPI_Target) #üß©     \n",
    "            #ImportüìÉ\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, KPI_Target_TABLE_NAME, KPI_Target) #üß©\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            # Uppdate log_entries for successfully importüí°\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            # Uppdate log_entries for fail importüí°\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception\n",
    "        ) as e:\n",
    "            error_count += 1\n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"\n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True)\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: # Old_File with modifieddate unchangedüìÉ\n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "display_summary(\"KPI_Target\", error_count) #üß©\n",
    "process_and_save_log(log_df, log_entries, log_KPI_Target_path) #üß©\n",
    "logger.info(\"===== Processing of the KPI_Target data source is complete =====\") #üß©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2db9eca-d565-4d03-8e46-757b9a88e681",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 1Ô∏è‚É£9Ô∏è‚É£[BKN]LogoutCountüíæ\n",
    "logger.info(\"===== Start LogoutCount Process =====\") #üß©\n",
    "log_df = read_or_create_log(log_LogoutCount_path) #üß©\n",
    "log_entries, error_count = Default_variable()\n",
    "# Loop Folderüí°\n",
    "for filename in Folder_LogoutCount.glob(\"*.csv\"): #üß©\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Add ModifiedDateüìÉ\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0)\n",
    "    file_basename = filename.name # Add FileNameüìÉ\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Compare FileName with LogüìÉ\n",
    "    is_new_file = log_entry.is_empty()\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1) \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Compare ModifiedDate with LogüìÉ\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # Old_File with modifieddate changedüìÉ\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # Is_New_fileüìÉ\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try:\n",
    "            if not is_new_file: #Then Delete and ImportüìÉ\n",
    "                delete_data(engine, LogoutCount_TABLE_NAME, file_basename) #üß©\n",
    "            # Process import to Databaseüí°\n",
    "            LogoutCount = (pl.read_csv(filename, infer_schema_length=None) #üß©\n",
    "                   .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   )) #üß©\n",
    "            # SCHEMA checküìÉ\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(LogoutCount, LogoutCount_schema, file_basename) #üß©\n",
    "            if has_critical_error:\n",
    "                raise ValueError(critical_schema_error_msg)\n",
    "            # column structureüìÉ\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            LogoutCount = LogoutCount.with_columns(pl.col('TimeDimension').str.strptime(pl.Date, format='%m/%d/%Y'),\n",
    "                                                   pl.col('KPI Value Formatted').cast(pl.Int64)) #üß©\n",
    "            LogoutCount = LogoutCount.select(LogoutCount_schema) #üß©\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(LogoutCount) #üß©\n",
    "            #ImportüìÉ\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, LogoutCount_TABLE_NAME, LogoutCount) #üß©\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            # Uppdate log_entries for successfully importüí°\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            # Uppdate log_entries for fail importüí°\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception\n",
    "        ) as e:\n",
    "            error_count += 1\n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"\n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True)\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: # Old_File with modifieddate unchangedüìÉ\n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "display_summary(\"LogoutCount\", error_count) #üß©\n",
    "process_and_save_log(log_df, log_entries, log_LogoutCount_path) #üß©\n",
    "logger.info(\"===== Processing of the LogoutCount data source is complete =====\") #üß©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a62bcc-5a33-4d20-8daa-deb4009b42a1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 2Ô∏è‚É£0Ô∏è‚É£[BKN]WpDetailüíæ\n",
    "logger.info(\"===== Start WpDetail Process =====\") #üß©\n",
    "log_df = read_or_create_log(log_WpDetail_path) #üß©\n",
    "log_entries, error_count = Default_variable()\n",
    "# Loop Folderüí°\n",
    "for filename in Folder_WpDetail.glob(\"*.xlsx\"): #üß©\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Add ModifiedDateüìÉ\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0)\n",
    "    file_basename = filename.name # Add FileNameüìÉ\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Compare FileName with LogüìÉ\n",
    "    is_new_file = log_entry.is_empty()\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1) \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Compare ModifiedDate with LogüìÉ\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # Old_File with modifieddate changedüìÉ\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # Is_New_fileüìÉ\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try:\n",
    "            if not is_new_file: #Then Delete and ImportüìÉ\n",
    "                delete_data(engine, WpDetail_TABLE_NAME, file_basename) #üß©\n",
    "            # Process import to Databaseüí°\n",
    "            WpDetail = (pl.read_excel(filename, infer_schema_length=None, engine=\"calamine\", sheet_name=\"Sheet1\") #üß©\n",
    "                   .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),\n",
    "                           pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   ))\n",
    "            WpDetail = WpDetail.with_columns(pl.col('Date').cast(pl.Date),\n",
    "                                             pl.col('Start1', 'End1', 'Start2', 'End2').cast(pl.Time))\n",
    "            threshold = t(15, 0, 0) # Time threshold definition (15:00:00)\n",
    "            WpDetail = WpDetail.with_columns( # create Date_end\n",
    "                pl.when(pl.col(\"Start1\").cast(pl.Time) >= threshold)\n",
    "                    .then(pl.col(\"Date\").cast(pl.Date) + timedelta(days=1)) \n",
    "                    .otherwise(pl.col(\"Date\").cast(pl.Date)).alias(\"Date_end\"))\n",
    "            WpDetail = WpDetail.with_columns( # create DateTime_Start & DateTime_End\n",
    "                (pl.col(\"Date\").dt.strftime(\"%Y-%m-%d\") + \" \" + pl.col(\"Start1\").cast(str)).str.strptime(pl.Datetime, \"%Y-%m-%d %H:%M:%S\").alias(\"DateTime_Start\"),\n",
    "                (pl.col(\"Date_end\").dt.strftime(\"%Y-%m-%d\") + \" \" + pl.col(\"End1\").cast(str)).str.strptime(pl.Datetime, \"%Y-%m-%d %H:%M:%S\").alias(\"DateTime_End\"))\n",
    "            WpDetail = WpDetail.filter(pl.col(\"Start2\").is_not_null() & pl.col(\"End2\").is_not_null()) # Filter for non-null values\n",
    "            WpDetail = WpDetail.with_columns( # Create 'Date_Act_Start'\n",
    "                pl.when((pl.col(\"Start1\") >= t(15, 0, 0)) & (pl.col(\"Start2\") <= t(15, 0, 0)))\n",
    "                .then(pl.col(\"Date\") + pl.duration(days=1))  # Directly add a duration to the Date column\n",
    "                .otherwise(pl.col(\"Date\")).alias(\"Date_Act_Start\"))\n",
    "            WpDetail = WpDetail.with_columns( # Create 'DateTime_Act_Start' column\n",
    "                (pl.col(\"Date_Act_Start\").dt.strftime(\"%Y-%m-%d\") + \" \" + pl.col(\"Start2\").cast(str))\n",
    "                .str.strptime(pl.Datetime, \"%Y-%m-%d %H:%M:%S\").alias(\"DateTime_Act_Start\"))\n",
    "            WpDetail = WpDetail.with_columns( # Create 'Date_Act_End' column\n",
    "                pl.when((pl.col(\"Start1\") >= t(15, 0, 0)) & (pl.col(\"End2\") <= t(15, 0, 0)))\n",
    "                .then(pl.col(\"Date\") + pl.duration(days=1))  # Add duration directly to Date column\n",
    "                .otherwise(pl.col(\"Date\")).alias(\"Date_Act_End\"))\n",
    "            WpDetail = WpDetail.with_columns( # Create 'DateTime_Act_End' column\n",
    "                (pl.col(\"Date_Act_End\").dt.strftime(\"%Y-%m-%d\") + \" \" + pl.col(\"End2\").cast(str))\n",
    "                .str.strptime(pl.Datetime, \"%Y-%m-%d %H:%M:%S\").alias(\"DateTime_Act_End\"))\n",
    "            WpDetail = WpDetail.with_columns(( # Convert data and calculate Dur (Hrs)\n",
    "                (pl.col(\"DateTime_End\") - pl.col(\"DateTime_Start\")).dt.total_seconds() / 3600).alias(\"Dur\"))\n",
    "            WpDetail = WpDetail.with_columns(( # Convert data and calculate Act_Dur (Hrs)\n",
    "                (pl.col(\"DateTime_Act_End\") - pl.col(\"DateTime_Act_Start\")).dt.total_seconds() / 3600).alias(\"Act_Dur\"))\n",
    "            WpDetail = WpDetail.rename({'Date':'Date_Start','Start1':'Time_Start','End1':'Time_End','Schedule Act':'Action',\n",
    "                                        'Start2':'Time_Act_Start','End2':'Time_Act_End'}) #üß©\n",
    "            # SCHEMA checküìÉ\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(WpDetail, WpDetail_schema, file_basename) #üß©\n",
    "            if has_critical_error:\n",
    "                raise ValueError(critical_schema_error_msg)\n",
    "            # column structureüìÉ\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            WpDetail = WpDetail.select(WpDetail_schema) #üß©\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(WpDetail) #üß©\n",
    "            #ImportüìÉ\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, WpDetail_TABLE_NAME, WpDetail) #üß©\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            # Uppdate log_entries for successfully importüí°\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            # Uppdate log_entries for fail importüí°\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception\n",
    "        ) as e:\n",
    "            error_count += 1\n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"\n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True)\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: # Old_File with modifieddate unchangedüìÉ\n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "display_summary(\"WpDetail\", error_count) #üß©\n",
    "process_and_save_log(log_df, log_entries, log_WpDetail_path) #üß©\n",
    "logger.info(\"===== Processing of the WpDetail data source is complete =====\") #üß©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7225ec7-4c62-4958-8cc5-2450e4cd3a3d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 2Ô∏è‚É£1Ô∏è‚É£[BKN]WpSummaryüíæ\n",
    "logger.info(\"===== Start WpSummary Process =====\") #üß©\n",
    "log_df = read_or_create_log(log_WpSummary_path) #üß©\n",
    "log_entries, error_count = Default_variable()\n",
    "# Loop Folderüí°\n",
    "for filename in Folder_WpSummary.glob(\"*.xlsx\"): #üß©\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Add ModifiedDateüìÉ\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0)\n",
    "    file_basename = filename.name # Add FileNameüìÉ\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Compare FileName with LogüìÉ\n",
    "    is_new_file = log_entry.is_empty()\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1) \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Compare ModifiedDate with LogüìÉ\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # Old_File with modifieddate changedüìÉ\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # Is_New_fileüìÉ\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try:\n",
    "            if not is_new_file: #Then Delete and ImportüìÉ\n",
    "                delete_data(engine, WpSummary_TABLE_NAME, file_basename) #üß©\n",
    "            # Process import to Databaseüí°\n",
    "            WpSummary = (pl.read_excel(filename, infer_schema_length=None, engine=\"calamine\", sheet_name=\"Sheet1\") #üß©\n",
    "                    .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   )) #üß©\n",
    "            # SCHEMA checküìÉ\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(WpSummary, WpSummary_schema, file_basename) #üß©\n",
    "            if has_critical_error:\n",
    "                raise ValueError(critical_schema_error_msg)\n",
    "            # column structureüìÉ\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            WpSummary = WpSummary.with_columns(pl.col(\"Date\").cast(pl.Date)) #üß©\n",
    "            WpSummary = WpSummary.with_columns(pl.col('Length', 'Percent').cast(pl.Float64)) #üß©\n",
    "            WpSummary = WpSummary.select(WpSummary_schema) #üß©\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(WpSummary) #üß©  \n",
    "            #ImportüìÉ\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, WpSummary_TABLE_NAME, WpSummary) #üß©\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            # Uppdate log_entries for successfully importüí°\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            # Uppdate log_entries for fail importüí°\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception\n",
    "        ) as e:\n",
    "            error_count += 1\n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"\n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True)\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: # Old_File with modifieddate unchangedüìÉ\n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "display_summary(\"WpSummary\", error_count) #üß©\n",
    "process_and_save_log(log_df, log_entries, log_WpSummary_path) #üß©\n",
    "logger.info(\"===== Processing of the WpSummary data source is complete =====\") #üß©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93abed96-af00-4a3c-a5da-264702c31887",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 2Ô∏è‚É£2Ô∏è‚É£[BKN]RegisteredOTüíæ\n",
    "logger.info(\"===== Start RegisteredOT Process =====\") #üß©\n",
    "log_df = read_or_create_log(log_RegisteredOT_path) #üß©\n",
    "log_entries, error_count = Default_variable()\n",
    "# Loop Folderüí°\n",
    "for filename in Folder_RegisteredOT.glob(\"*.xlsx\"): #üß©\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Add ModifiedDateüìÉ\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0)\n",
    "    file_basename = filename.name # Add FileNameüìÉ\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Compare FileName with LogüìÉ\n",
    "    is_new_file = log_entry.is_empty()\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1) \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Compare ModifiedDate with LogüìÉ\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # Old_File with modifieddate changedüìÉ\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # Is_New_fileüìÉ\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try:\n",
    "            if not is_new_file: #Then Delete and ImportüìÉ\n",
    "                delete_data(engine, RegisteredOT_TABLE_NAME, file_basename) # Import Table Nameüß©\n",
    "            # Process import to Databaseüí°\n",
    "            RegisteredOT = (pl.read_excel(filename, infer_schema_length=None, engine=\"calamine\", sheet_name=\"Sheet1\") #üß©\n",
    "                    .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   )) #üß©\n",
    "            # SCHEMA checküìÉ\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(RegisteredOT, RegisteredOT_schema, file_basename) #üß©\n",
    "            if has_critical_error:\n",
    "                raise ValueError(critical_schema_error_msg)\n",
    "            # column structureüìÉ\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            RegisteredOT = RegisteredOT.with_columns(pl.col(\"Date\").cast(pl.Date)) #üß©\n",
    "            RegisteredOT = RegisteredOT.with_columns(pl.col(\"OT\").cast(pl.Float64)) #üß©\n",
    "            RegisteredOT = RegisteredOT.select(RegisteredOT_schema) #üß©\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(RegisteredOT) #üß©    \n",
    "            #ImportüìÉ\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, RegisteredOT_TABLE_NAME, RegisteredOT) #üß©\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            # Uppdate log_entries for successfully importüí°\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            # Uppdate log_entries for fail importüí°\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception\n",
    "        ) as e:\n",
    "            error_count += 1\n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"\n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True)\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: # Old_File with modifieddate unchangedüìÉ\n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "display_summary(\"RegisteredOT\", error_count) #üß©\n",
    "process_and_save_log(log_df, log_entries, log_RegisteredOT_path) #üß©\n",
    "logger.info(\"===== Processing of the RegisteredOT data source is complete =====\") #üß©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad897981-8686-4291-87b4-2785918d70be",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 2Ô∏è‚É£3Ô∏è‚É£[BKN]CSAT_TPüíæ\n",
    "logger.info(\"===== Start CSAT_TP Process =====\") #üß©\n",
    "log_df = read_or_create_log(log_CSAT_TP_path) #üß©\n",
    "log_entries, error_count = Default_variable()\n",
    "# Loop Folderüí°\n",
    "for filename in Folder_CSAT_TP.glob(\"*.csv\"): #üß©\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Add ModifiedDateüìÉ\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0)\n",
    "    file_basename = filename.name # Add FileNameüìÉ\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Compare FileName with LogüìÉ\n",
    "    is_new_file = log_entry.is_empty()\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1) \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Compare ModifiedDate with LogüìÉ\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # Old_File with modifieddate changedüìÉ\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # Is_New_fileüìÉ\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try:\n",
    "            if not is_new_file: #Then Delete and ImportüìÉ\n",
    "                delete_data(engine, CSAT_TP_TABLE_NAME, file_basename) #üß©\n",
    "            # Process import to Databaseüí°\n",
    "            CSAT_TP = (pl.read_csv(filename, infer_schema_length=None) #üß©\n",
    "                   .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   ))       \n",
    "            CSAT_TP = (CSAT_TP.rename({'Date ': 'Date', '\"\"Comment\"\"': '\"Comment\"'})) #üß©\n",
    "            # SCHEMA checküìÉ\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(CSAT_TP, CSAT_TP_schema, file_basename) #üß©\n",
    "            if has_critical_error:\n",
    "                raise ValueError(critical_schema_error_msg)\n",
    "            # column structureüìÉ\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            CSAT_TP = CSAT_TP.with_columns(parse_date(pl.col('Sort by Dimension', 'Date', 'Max. Sort by Dimension')),\n",
    "                                           pl.col('Sort by Dimension (copy)').cast(pl.Float64)) #üß©\n",
    "            CSAT_TP = CSAT_TP.select(CSAT_TP_schema) #üß©\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(CSAT_TP) #üß©\n",
    "            #ImportüìÉ\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, CSAT_TP_TABLE_NAME, CSAT_TP) #üß©\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            # Uppdate log_entries for successfully importüí°\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            # Uppdate log_entries for fail importüí°\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception\n",
    "        ) as e:\n",
    "            error_count += 1\n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"\n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True)\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: # Old_File with modifieddate unchangedüìÉ\n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "display_summary(\"CSAT_TP\", error_count) #üß©\n",
    "process_and_save_log(log_df, log_entries, log_CSAT_TP_path) #üß©\n",
    "logger.info(\"===== Processing of the CSAT_TP data source is complete =====\") #üß©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a879047-417d-4d7a-b7f2-752a3982717d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 2Ô∏è‚É£4Ô∏è‚É£[BKN]CSAT_RSüíæ\n",
    "logger.info(\"===== Start CSAT_RS Process =====\") #üß©\n",
    "log_df = read_or_create_log(log_CSAT_RS_path) #üß©\n",
    "log_entries, error_count = Default_variable()\n",
    "# Loop Folderüí°\n",
    "for filename in Folder_CSAT_RS.glob(\"*.csv\"): #üß©\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Add ModifiedDateüìÉ\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0)\n",
    "    file_basename = filename.name # Add FileNameüìÉ\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Compare FileName with LogüìÉ\n",
    "    is_new_file = log_entry.is_empty()\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1) \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Compare ModifiedDate with LogüìÉ\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # Old_File with modifieddate changedüìÉ\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # Is_New_fileüìÉ\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try:\n",
    "            if not is_new_file: #Then Delete and ImportüìÉ\n",
    "                delete_data(engine, CSAT_RS_TABLE_NAME, file_basename) #üß©\n",
    "            # Process import to Databaseüí°\n",
    "            CSAT_RS = (pl.read_csv(filename, infer_schema_length=None) #üß©\n",
    "                   .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   ))\n",
    "            CSAT_RS = (CSAT_RS.rename({'Date ': 'Date', '\"\"Comment\"\"': '\"Comment\"'})) #üß©\n",
    "            # SCHEMA checküìÉ\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(CSAT_RS, CSAT_RS_schema, file_basename) #üß©\n",
    "            if has_critical_error:\n",
    "                raise ValueError(critical_schema_error_msg)\n",
    "            # column structureüìÉ\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            CSAT_RS = CSAT_RS.with_columns(\n",
    "                parse_date(pl.col(\"Sort by Dimension\")).alias(\"Sort by Dimension\"),\n",
    "                parse_date(pl.col(\"Max. Sort by Dimension\")).alias(\"Max. Sort by Dimension\"),\n",
    "                pl.col(\"Sort by Dimension (copy)\").cast(pl.Float64),).select(CSAT_RS_schema) #üß©\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(CSAT_RS) #üß©\n",
    "            #ImportüìÉ\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, CSAT_RS_TABLE_NAME, CSAT_RS) #üß©\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            # Uppdate log_entries for successfully importüí°\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            # Uppdate log_entries for fail importüí°\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception\n",
    "        ) as e:\n",
    "            error_count += 1\n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"\n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True)\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: # Old_File with modifieddate unchangedüìÉ\n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "display_summary(\"CSAT_RS\", error_count) #üß©\n",
    "process_and_save_log(log_df, log_entries, log_CSAT_RS_path) #üß©\n",
    "logger.info(\"===== Processing of the CSAT_RS data source is complete =====\") #üß©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024eb106-5577-4a29-af1f-bfaec9f90b7c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 2Ô∏è‚É£5Ô∏è‚É£[BKN]PSATüíæ\n",
    "logger.info(\"===== Start PSAT Process =====\") #üß©\n",
    "log_df = read_or_create_log(log_PSAT_path) #üß©\n",
    "log_entries, error_count = Default_variable()\n",
    "# Loop Folderüí°\n",
    "for filename in Folder_PSAT.glob(\"*.csv\"): #üß©\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Add ModifiedDateüìÉ\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0)\n",
    "    file_basename = filename.name # Add FileNameüìÉ\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Compare FileName with LogüìÉ\n",
    "    is_new_file = log_entry.is_empty()\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1) \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Compare ModifiedDate with LogüìÉ\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # Old_File with modifieddate changedüìÉ\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # Is_New_fileüìÉ\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try:\n",
    "            if not is_new_file: #Then Delete and ImportüìÉ\n",
    "                delete_data(engine, PSAT_TABLE_NAME, file_basename) #üß©\n",
    "            # Process import to Databaseüí°\n",
    "            PSAT = (pl.read_csv(filename, infer_schema_length=None) #üß©\n",
    "                   .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   )) \n",
    "            PSAT = (PSAT.rename({'\"\"Comment\"\"': '\"Comment\"'})) #üß©\n",
    "            # SCHEMA checküìÉ\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(PSAT, PSAT_schema, file_basename) #üß©\n",
    "            if has_critical_error:\n",
    "                raise ValueError(critical_schema_error_msg)\n",
    "            # column structureüìÉ\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            PSAT = PSAT.with_columns(\n",
    "                parse_date(pl.col(\"Sorted By Dimension\")).alias(\"Sorted By Dimension\"),\n",
    "                parse_date(pl.col(\"Date\")).alias(\"Date\"),\n",
    "                pl.col(\"Sorted BY Dimension (copy)\").cast(pl.Float64),).select(PSAT_schema) #üß©\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(PSAT) #üß©            \n",
    "            #ImportüìÉ\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, PSAT_TABLE_NAME, PSAT) #üß©\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            # Uppdate log_entries for successfully importüí°\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            # Uppdate log_entries for fail importüí°\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception\n",
    "        ) as e:\n",
    "            error_count += 1\n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"\n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True)\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: # Old_File with modifieddate unchangedüìÉ\n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "display_summary(\"PSAT\", error_count) #üß©\n",
    "process_and_save_log(log_df, log_entries, log_PSAT_path) #üß©\n",
    "logger.info(\"===== Processing of the PSAT data source is complete =====\") #üß©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ba7d3e-51a2-492c-a1f1-65ebc3a6ea6c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 2Ô∏è‚É£6Ô∏è‚É£[BKN]IEX_Hrsüíæ\n",
    "logger.info(\"===== Start IEX_Hrs Process =====\") #üß©\n",
    "log_df = read_or_create_log(log_IEX_Hrs_path) #üß©\n",
    "log_entries, error_count = Default_variable()\n",
    "# Loop Folderüí°\n",
    "for filename in Folder_IEX_Hrs.glob(\"*.xlsx\"): #üß©\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Add ModifiedDateüìÉ\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0)\n",
    "    file_basename = filename.name # Add FileNameüìÉ\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Compare FileName with LogüìÉ\n",
    "    is_new_file = log_entry.is_empty()\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1) \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Compare ModifiedDate with LogüìÉ\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # Old_File with modifieddate changedüìÉ\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # Is_New_fileüìÉ\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try:\n",
    "            if not is_new_file: #Then Delete and ImportüìÉ\n",
    "                delete_data(engine, IEX_Hrs_TABLE_NAME, file_basename) #üß©\n",
    "            # Process import to Databaseüí°\n",
    "            IEX_Hrs = (pl.read_excel(filename, infer_schema_length=None, engine=\"calamine\", sheet_name=\"Sheet1\") #üß©\n",
    "                    .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   )) #üß©\n",
    "            # SCHEMA checküìÉ\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(IEX_Hrs, IEX_Hrs_schema, file_basename) #üß©\n",
    "            if has_critical_error:\n",
    "                raise ValueError(critical_schema_error_msg)\n",
    "            # column structureüìÉ\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            IEX_Hrs = IEX_Hrs.with_columns(pl.col(\"VNT\", \"CET\").cast(pl.Date)) #üß©\n",
    "            IEX_Hrs = IEX_Hrs.with_columns(pl.col(\"HC\", \"Hour\").cast(pl.Float64)) #üß©\n",
    "            IEX_Hrs = IEX_Hrs.select(IEX_Hrs_schema) #üß©\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(IEX_Hrs) #üß©       \n",
    "            #ImportüìÉ\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, IEX_Hrs_TABLE_NAME, IEX_Hrs) #üß©\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            # Uppdate log_entries for successfully importüí°\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            # Uppdate log_entries for fail importüí°\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception\n",
    "        ) as e:\n",
    "            error_count += 1\n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"\n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True)\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: # Old_File with modifieddate unchangedüìÉ\n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "display_summary(\"IEX_Hrs\", error_count) #üß©\n",
    "process_and_save_log(log_df, log_entries, log_IEX_Hrs_path) #üß©\n",
    "logger.info(\"===== Processing of the IEX_Hrs data source is complete =====\") #üß©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf813105-f648-43e6-9cc2-b480d42d6cca",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 2Ô∏è‚É£7Ô∏è‚É£[BKN]IntervalReqüíæ\n",
    "logger.info(\"===== Start IntervalReq Process =====\") #üß©\n",
    "log_df = read_or_create_log(log_IntervalReq_path) #üß©\n",
    "log_entries, error_count = Default_variable()\n",
    "# Loop Folderüí°\n",
    "for filename in Folder_IntervalReq.glob(\"*.xlsx\"): #üß©\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Add ModifiedDateüìÉ\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0)\n",
    "    file_basename = filename.name # Add FileNameüìÉ\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Compare FileName with LogüìÉ\n",
    "    is_new_file = log_entry.is_empty()\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1) \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Compare ModifiedDate with LogüìÉ\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # Old_File with modifieddate changedüìÉ\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # Is_New_fileüìÉ\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try:\n",
    "            if not is_new_file: #Then Delete and ImportüìÉ\n",
    "                delete_data(engine, IntervalReq_TABLE_NAME, file_basename) #üß©\n",
    "            # Process import to Databaseüí°\n",
    "            IntervalReq = (pl.read_excel(filename, infer_schema_length=None, engine=\"calamine\", sheet_name=\"Sheet1\") #üß©\n",
    "                    .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   )) \n",
    "            IntervalReq = IntervalReq.with_columns(pl.col('Attribute').dt.strftime('%Y-%m-%d'))\n",
    "            IntervalReq = IntervalReq.with_columns(pl.col('Forecast').dt.strftime('%H:%M:%S'))\n",
    "            IntervalReq = (\n",
    "                IntervalReq.with_columns(\n",
    "                    (pl.col(\"Value\") / 95 * 100).alias(\"delivery_requirement\"))\n",
    "                .with_columns(\n",
    "                    pl.col(\"delivery_requirement\").round(2).alias(\"Delivery_Req\")))\n",
    "            IntervalReq = IntervalReq.with_columns(\n",
    "                pl.format(\"{} {}\", pl.col(\"Attribute\"), pl.col(\"Forecast\")).alias(\"Datetime_CET\"))\n",
    "            IntervalReq = IntervalReq.with_columns(pl.col('Datetime_CET'\n",
    "                                ).str.strptime(pl.Datetime, format='%Y-%m-%d %H:%M:%S'\n",
    "                                ).dt.replace_time_zone(\"Europe/Berlin\", ambiguous=\"earliest\"\n",
    "                                ).dt.convert_time_zone(\"Asia/Bangkok\").alias(\"Datetime_VN\").dt.strftime('%Y-%m-%d %H:%M:%S')) #üß©\n",
    "            IntervalReq = IntervalReq.with_columns(pl.col('Datetime_CET'\n",
    "                                ).str.strptime(pl.Datetime, format='%Y-%m-%d %H:%M:%S').dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "                                ).with_columns(pl.col(\"Datetime_CET\").str.to_datetime(\"%Y-%m-%d %H:%M:%S\").cast(pl.Datetime)\n",
    "                                ).with_columns(pl.col(\"Datetime_VN\").str.to_datetime(\"%Y-%m-%d %H:%M:%S\").cast(pl.Datetime)) #üß©\n",
    "            # SCHEMA checküìÉ\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(IntervalReq, IntervalReq_schema, file_basename) #üß©\n",
    "            if has_critical_error:\n",
    "                raise ValueError(critical_schema_error_msg)\n",
    "            # column structureüìÉ\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            IntervalReq = IntervalReq.select(IntervalReq_schema) #üß©\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(IntervalReq) #üß©       \n",
    "            #ImportüìÉ\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, IntervalReq_TABLE_NAME, IntervalReq) #üß©\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            # Uppdate log_entries for successfully importüí°\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            # Uppdate log_entries for fail importüí°\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception\n",
    "        ) as e:\n",
    "            error_count += 1\n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"\n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True)\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: # Old_File with modifieddate unchangedüìÉ\n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "display_summary(\"IntervalReq\", error_count) #üß©\n",
    "process_and_save_log(log_df, log_entries, log_IntervalReq_path) #üß©\n",
    "logger.info(\"===== Processing of the IntervalReq data source is complete =====\") #üß©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f61cda-d080-487b-8c46-b5157e7eb784",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 2Ô∏è‚É£8Ô∏è‚É£[BKN]ExceptionReqüíæ\n",
    "logger.info(\"===== Start ExceptionReq Process =====\") #üß©\n",
    "log_df = read_or_create_log(log_ExceptionReq_path) #üß©\n",
    "log_entries, error_count = Default_variable()\n",
    "# Loop Folderüí°\n",
    "for filename in Folder_ExceptionReq.glob(\"*.xlsx\"): #üß©\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Add ModifiedDateüìÉ\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0)\n",
    "    file_basename = filename.name # Add FileNameüìÉ\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Compare FileName with LogüìÉ\n",
    "    is_new_file = log_entry.is_empty()\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1) \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Compare ModifiedDate with LogüìÉ\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # Old_File with modifieddate changedüìÉ\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # Is_New_fileüìÉ\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try:\n",
    "            if not is_new_file: #Then Delete and ImportüìÉ\n",
    "                delete_data(engine, ExceptionReq_TABLE_NAME, file_basename) #üß©\n",
    "            # Process import to Databaseüí°\n",
    "            ExceptionReq = (pl.read_excel(filename, infer_schema_length=None, engine=\"calamine\", sheet_name=\"Sheet1\") #üß©\n",
    "                    .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   )) #üß©\n",
    "            # SCHEMA checküìÉ\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(ExceptionReq, ExceptionReq_schema, file_basename) #üß©\n",
    "            if has_critical_error:\n",
    "                raise ValueError(critical_schema_error_msg)\n",
    "            # column structureüìÉ\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            ExceptionReq = ExceptionReq.with_columns(pl.col(\"Date (MM/DD/YYYY)\").cast(pl.Date)) #üß©\n",
    "            ExceptionReq = ExceptionReq.with_columns(pl.col(\"Exception request (Minute)\").cast(pl.Float64)) #üß©\n",
    "            ExceptionReq = ExceptionReq.select(ExceptionReq_schema) #üß©\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(ExceptionReq) #üß©        \n",
    "            #ImportüìÉ\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, ExceptionReq_TABLE_NAME, ExceptionReq) #üß©\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            # Uppdate log_entries for successfully importüí°\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            # Uppdate log_entries for fail importüí°\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception\n",
    "        ) as e:\n",
    "            error_count += 1\n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"\n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True)\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: # Old_File with modifieddate unchangedüìÉ\n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "display_summary(\"ExceptionReq\", error_count) #üß©\n",
    "process_and_save_log(log_df, log_entries, log_ExceptionReq_path) #üß©\n",
    "logger.info(\"===== Processing of the ExceptionReq data source is complete =====\") #üß©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45693229-6f62-4fcf-84f0-c10426e63947",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 2Ô∏è‚É£9Ô∏è‚É£[BKN]LTTransfersüíæ\n",
    "logger.info(\"===== Start LTTransfers Process =====\") #üß©\n",
    "log_df = read_or_create_log(log_LTTransfers_path) #üß©\n",
    "log_entries, error_count = Default_variable()\n",
    "# Loop Folderüí°\n",
    "for filename in Folder_LTTransfers.glob(\"*.xlsx\"): #üß©\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Add ModifiedDateüìÉ\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0)\n",
    "    file_basename = filename.name # Add FileNameüìÉ\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Compare FileName with LogüìÉ\n",
    "    is_new_file = log_entry.is_empty()\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1) \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Compare ModifiedDate with LogüìÉ\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # Old_File with modifieddate changedüìÉ\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # Is_New_fileüìÉ\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try:\n",
    "            if not is_new_file: #Then Delete and ImportüìÉ\n",
    "                delete_data(engine, LTTransfers_TABLE_NAME, file_basename) #üß©\n",
    "            # Process import to Databaseüí°\n",
    "            LTTransfers = (pl.read_excel(filename, infer_schema_length=None, engine=\"calamine\", sheet_name=\"Sheet1\") #üß©\n",
    "                    .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   )) #üß©\n",
    "            # SCHEMA checküìÉ\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(LTTransfers, LTTransfers_schema, file_basename) #üß©\n",
    "            if has_critical_error:\n",
    "                raise ValueError(critical_schema_error_msg)\n",
    "            # column structureüìÉ\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            LTTransfers = LTTransfers.with_columns(pl.col(\"LWD\").cast(pl.Date)) #üß©\n",
    "            LTTransfers = LTTransfers.select(LTTransfers_schema) #üß©\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(LTTransfers) #üß©\n",
    "            #ImportüìÉ\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, LTTransfers_TABLE_NAME, LTTransfers) #üß©\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            # Uppdate log_entries for successfully importüí°\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            # Uppdate log_entries for fail importüí°\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception\n",
    "        ) as e:\n",
    "            error_count += 1\n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"\n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True)\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: # Old_File with modifieddate unchangedüìÉ\n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "display_summary(\"LTTransfers\", error_count) #üß©\n",
    "process_and_save_log(log_df, log_entries, log_LTTransfers_path) #üß©\n",
    "logger.info(\"===== Processing of the LTTransfers data source is complete =====\") #üß©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5f7614-c566-4ed9-ab46-024d2b61023e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 3Ô∏è‚É£0Ô∏è‚É£[BKN]DailyReqüíæ\n",
    "logger.info(\"===== Start DailyReq Process =====\") #üß©\n",
    "log_df = read_or_create_log(log_DailyReq_path) #üß©\n",
    "log_entries, error_count = Default_variable()\n",
    "# Loop Folderüí°\n",
    "for filename in Folder_DailyReq.glob(\"*.xlsx\"): #üß©\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Add ModifiedDateüìÉ\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0)\n",
    "    file_basename = filename.name # Add FileNameüìÉ\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Compare FileName with LogüìÉ\n",
    "    is_new_file = log_entry.is_empty()\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1) \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Compare ModifiedDate with LogüìÉ\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # Old_File with modifieddate changedüìÉ\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # Is_New_fileüìÉ\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try:\n",
    "            if not is_new_file: #Then Delete and ImportüìÉ\n",
    "                delete_data(engine, DailyReq_TABLE_NAME, file_basename) #üß©\n",
    "            # Process import to Databaseüí°\n",
    "            DailyReq = (pl.read_excel(filename, infer_schema_length=None, engine=\"calamine\", sheet_name=\"Sheet1\") #üß©\n",
    "                    .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   )) #üß©\n",
    "            # SCHEMA checküìÉ\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(DailyReq, DailyReq_schema, file_basename) #üß©\n",
    "            if has_critical_error:\n",
    "                raise ValueError(critical_schema_error_msg)\n",
    "            # column structureüìÉ\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            DailyReq = DailyReq.with_columns(pl.col(\"Date\").cast(pl.Date)) #üß©\n",
    "            DailyReq = DailyReq.with_columns(pl.col(\"Daily Requirement\", \"Prod Requirement\").cast(pl.Float64)) #üß©\n",
    "            DailyReq = DailyReq.select(DailyReq_schema) #üß©\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(DailyReq) #üß©  \n",
    "            #ImportüìÉ\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, DailyReq_TABLE_NAME, DailyReq) #üß©\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            # Uppdate log_entries for successfully importüí°\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            # Uppdate log_entries for fail importüí°\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception\n",
    "        ) as e:\n",
    "            error_count += 1\n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"\n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True)\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: # Old_File with modifieddate unchangedüìÉ\n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "display_summary(\"DailyReq\", error_count) #üß©\n",
    "process_and_save_log(log_df, log_entries, log_DailyReq_path) #üß©\n",
    "logger.info(\"===== Processing of the DailyReq data source is complete =====\") #üß©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1604a80-20fb-48ec-8b4f-b0896f67fabb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 3Ô∏è‚É£1Ô∏è‚É£[BKN]ProjectedShrinküíæ\n",
    "logger.info(\"===== Start ProjectedShrink Process =====\") #üß©\n",
    "log_df = read_or_create_log(log_ProjectedShrink_path) #üß©\n",
    "log_entries, error_count = Default_variable()\n",
    "# Loop Folderüí°\n",
    "for filename in Folder_ProjectedShrink.glob(\"*.xlsx\"): #üß©\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Add ModifiedDateüìÉ\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0)\n",
    "    file_basename = filename.name # Add FileNameüìÉ\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Compare FileName with LogüìÉ\n",
    "    is_new_file = log_entry.is_empty()\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1) \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Compare ModifiedDate with LogüìÉ\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # Old_File with modifieddate changedüìÉ\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # Is_New_fileüìÉ\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try:\n",
    "            if not is_new_file: #Then Delete and ImportüìÉ\n",
    "                delete_data(engine, ProjectedShrink_TABLE_NAME, file_basename) #üß©\n",
    "            # Process import to Databaseüí°\n",
    "            ProjectedShrink = (pl.read_excel(filename, infer_schema_length=None, engine=\"calamine\", sheet_name=\"Sheet1\") #üß©\n",
    "                    .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   )) #üß©\n",
    "            # SCHEMA checküìÉ\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(ProjectedShrink, ProjectedShrink_schema, file_basename) #üß©\n",
    "            if has_critical_error:\n",
    "                raise ValueError(critical_schema_error_msg)\n",
    "            # column structureüìÉ\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            ProjectedShrink = ProjectedShrink.with_columns(pl.col(\"Week\").cast(pl.Int64)) #üß©\n",
    "            ProjectedShrink = ProjectedShrink.with_columns(pl.col(\"Ratio\").cast(pl.Float64)) #üß©\n",
    "            ProjectedShrink = ProjectedShrink.select(ProjectedShrink_schema) #üß©\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(ProjectedShrink) #üß©\n",
    "            #ImportüìÉ\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, ProjectedShrink_TABLE_NAME, ProjectedShrink) #üß©\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            # Uppdate log_entries for successfully importüí°\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            # Uppdate log_entries for fail importüí°\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception\n",
    "        ) as e:\n",
    "            error_count += 1\n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"\n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True)\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: # Old_File with modifieddate unchangedüìÉ\n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "display_summary(\"ProjectedShrink\", error_count) #üß©\n",
    "process_and_save_log(log_df, log_entries, log_ProjectedShrink_path) #üß©\n",
    "logger.info(\"===== Processing of the ProjectedShrink data source is complete =====\") #üß©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f608a80-bc03-4fbb-a3b5-9e32153153ec",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 3Ô∏è‚É£2Ô∏è‚É£[BKN]OTReqüíæ\n",
    "logger.info(\"===== Start OTReq Process =====\") #üß©\n",
    "log_df = read_or_create_log(log_OTReq_path) #üß©\n",
    "log_entries, error_count = Default_variable()\n",
    "# Loop Folderüí°\n",
    "for filename in Folder_OTReq.glob(\"*.xlsx\"): #üß©\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Add ModifiedDateüìÉ\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0)\n",
    "    file_basename = filename.name # Add FileNameüìÉ\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Compare FileName with LogüìÉ\n",
    "    is_new_file = log_entry.is_empty()\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1) \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Compare ModifiedDate with LogüìÉ\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # Old_File with modifieddate changedüìÉ\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # Is_New_fileüìÉ\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try:\n",
    "            if not is_new_file: #Then Delete and ImportüìÉ\n",
    "                delete_data(engine, OTReq_TABLE_NAME, file_basename) #üß©\n",
    "            # Process import to Databaseüí°\n",
    "            OTReq = (pl.read_excel(filename, infer_schema_length=None, engine=\"calamine\", sheet_name=\"Sheet1\") #üß©\n",
    "                    .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   )) # Import Schemaüß©\n",
    "            # SCHEMA checküìÉ\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(OTReq, OTReq_schema, file_basename) #üß©\n",
    "            if has_critical_error:\n",
    "                raise ValueError(critical_schema_error_msg)\n",
    "            # column structureüìÉ\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            OTReq = OTReq.with_columns(pl.col(\"Date\").cast(pl.Date)) #üß©\n",
    "            OTReq = OTReq.with_columns(pl.col(\"OT Hour\").cast(pl.Float64)) #üß©\n",
    "            OTReq = OTReq.select(OTReq_schema) #üß©\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(OTReq) #üß©\n",
    "            #ImportüìÉ\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, OTReq_TABLE_NAME, OTReq) #üß©\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            # Uppdate log_entries for successfully importüí°\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            # Uppdate log_entries for fail importüí°\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception\n",
    "        ) as e:\n",
    "            error_count += 1\n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"\n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True)\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: # Old_File with modifieddate unchangedüìÉ\n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "display_summary(\"OTReq\", error_count) #üß©\n",
    "process_and_save_log(log_df, log_entries, log_OTReq_path) #üß©\n",
    "logger.info(\"===== Processing of the OTReq data source is complete =====\") #üß©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c711a47-cecf-449d-b40c-8df925114031",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 3Ô∏è‚É£3Ô∏è‚É£[BKN]CapHCüíæ\n",
    "logger.info(\"===== Start CapHC Process =====\") #üß©\n",
    "log_df = read_or_create_log(log_CapHC_path) #üß©\n",
    "log_entries, error_count = Default_variable()\n",
    "# Loop Folderüí°\n",
    "for filename in Folder_CapHC.glob(\"*.xlsx\"): #üß©\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Add ModifiedDateüìÉ\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0)\n",
    "    file_basename = filename.name # Add FileNameüìÉ\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Compare FileName with LogüìÉ\n",
    "    is_new_file = log_entry.is_empty()\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1) \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Compare ModifiedDate with LogüìÉ\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # Old_File with modifieddate changedüìÉ\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # Is_New_fileüìÉ\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try:\n",
    "            if not is_new_file: #Then Delete and ImportüìÉ\n",
    "                delete_data(engine, CapHC_TABLE_NAME, file_basename) #üß©\n",
    "            # Process import to Databaseüí°\n",
    "            CapHC = (pl.read_excel(filename, infer_schema_length=None, engine=\"calamine\", sheet_name=\"Sheet1\") #üß©\n",
    "                    .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   )) #üß©\n",
    "            # SCHEMA checküìÉ\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(CapHC, CapHC_schema, file_basename) #üß©\n",
    "            if has_critical_error:\n",
    "                raise ValueError(critical_schema_error_msg)\n",
    "            # column structureüìÉ\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            CapHC = CapHC.with_columns(pl.col(\"Date\").cast(pl.Date)) #üß©\n",
    "            CapHC = CapHC.with_columns(pl.col(\"Client Requirement (Hours)\").cast(pl.Float64)) #üß©\n",
    "            CapHC = CapHC.select(CapHC_schema) #üß©\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(CapHC) #üß©\n",
    "            #ImportüìÉ\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, CapHC_TABLE_NAME, CapHC) #üß©\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            # Uppdate log_entries for successfully importüí°\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            # Uppdate log_entries for fail importüí°\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception\n",
    "        ) as e:\n",
    "            error_count += 1\n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"\n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True)\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: # Old_File with modifieddate unchangedüìÉ\n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "display_summary(\"CapHC\", error_count) #üß©\n",
    "process_and_save_log(log_df, log_entries, log_CapHC_path) #üß©\n",
    "logger.info(\"===== Processing of the CapHC data source is complete =====\") #üß©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ae8a55-befb-476b-b843-647f3192e582",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 3Ô∏è‚É£4Ô∏è‚É£[BKN]ProjectedHCüíæ\n",
    "logger.info(\"===== Start ProjectedHC Process =====\") #üß©\n",
    "log_df = read_or_create_log(log_ProjectedHC_path) #üß©\n",
    "log_entries, error_count = Default_variable()\n",
    "# Loop Folderüí°\n",
    "for filename in Folder_ProjectedHC.glob(\"*.xlsm\"): #üß©\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Add ModifiedDateüìÉ\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0)\n",
    "    file_basename = filename.name # Add FileNameüìÉ\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Compare FileName with LogüìÉ\n",
    "    is_new_file = log_entry.is_empty()\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1) \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Compare ModifiedDate with LogüìÉ\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # Old_File with modifieddate changedüìÉ\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # Is_New_fileüìÉ\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try:\n",
    "            if not is_new_file: #Then Delete and ImportüìÉ\n",
    "                delete_data(engine, ProjectedHC_TABLE_NAME, file_basename) #üß©\n",
    "            # Process import to Databaseüí°\n",
    "            ProjectedHC = (pl.read_excel(filename, infer_schema_length=None, engine=\"calamine\", sheet_name=\"rawDup\") #üß©\n",
    "                    .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   )) #üß©\n",
    "            # SCHEMA checküìÉ\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(ProjectedHC, ProjectedHC_schema, file_basename) #üß©\n",
    "            if has_critical_error:\n",
    "                raise ValueError(critical_schema_error_msg)\n",
    "            # column structureüìÉ\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            ProjectedHC = ProjectedHC.with_columns(pl.col(\"Date\").cast(pl.Date)) #üß©\n",
    "            ProjectedHC = ProjectedHC.with_columns(pl.col('FTE Required', 'Projected HC', 'Plan Leave', 'Actual Projected HC', '%OO', '%IO', \n",
    "                                                          'Projected HC with Shrink', 'OT', 'Leave allow for Shrink', '% Deli').cast(pl.Float64)) #üß©\n",
    "            ProjectedHC = ProjectedHC.select(ProjectedHC_schema) #üß©\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(ProjectedHC) #üß©\n",
    "            #ImportüìÉ\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, ProjectedHC_TABLE_NAME, ProjectedHC) #üß©\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            # Uppdate log_entries for successfully importüí°\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            # Uppdate log_entries for fail importüí°\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception\n",
    "        ) as e:\n",
    "            error_count += 1\n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"\n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True)\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: # Old_File with modifieddate unchangedüìÉ\n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "display_summary(\"ProjectedHC\", error_count) #üß©\n",
    "process_and_save_log(log_df, log_entries, log_ProjectedHC_path) #üß©\n",
    "logger.info(\"===== Processing of the ProjectedHC data source is complete =====\") #üß©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8eae43-5bab-4311-84af-4a9d96d0bbc9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 3Ô∏è‚É£5Ô∏è‚É£[BKN]RampHCüíæ\n",
    "logger.info(\"===== Start RampHC Process =====\") #üß©\n",
    "log_df = read_or_create_log(log_RampHC_path) #üß©\n",
    "log_entries, error_count = Default_variable()\n",
    "# Loop Folderüí°\n",
    "for filename in Folder_RampHC.glob(\"*.xlsx\"): #üß©\n",
    "    current_modified_date_microseconds = datetime.datetime.fromtimestamp(filename.stat().st_mtime) # Add ModifiedDateüìÉ\n",
    "    current_modified_date = current_modified_date_microseconds.replace(microsecond=0)\n",
    "    file_basename = filename.name # Add FileNameüìÉ\n",
    "    print_colored(f\"Processing fileüîÑÔ∏è: {file_basename}\", \"NavajoWhite\")\n",
    "    logger.info(f\"Processing fileüîÑÔ∏è: {file_basename}\")\n",
    "    log_entry = log_df.filter(pl.col(\"FileName\") == file_basename) # Compare FileName with LogüìÉ\n",
    "    is_new_file = log_entry.is_empty()\n",
    "    previous_modified_date = datetime.datetime(1900, 1, 1, 0, 0, 1) \n",
    "    if not is_new_file:\n",
    "        previous_modified_date = log_entry.select(\"ModifiedDate\").item() # Compare ModifiedDate with LogüìÉ\n",
    "    if is_new_file or current_modified_date > previous_modified_date:\n",
    "        if not is_new_file: # Old_File with modifieddate changedüìÉ\n",
    "            print_colored(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\", \"Coral\")\n",
    "            logger.warning(f\"üß¨ModifiedDate changed for {file_basename}. Deleting old data and re-importing.\")\n",
    "        else: # Is_New_fileüìÉ\n",
    "            print_colored(f\"New file detectedüö¶: {file_basename}. Importing.\", \"Coral\")\n",
    "            logger.info(f\"New file detectedüö¶: {file_basename}. Importing.\")\n",
    "        try:\n",
    "            if not is_new_file: #Then Delete and ImportüìÉ\n",
    "                delete_data(engine, RampHC_TABLE_NAME, file_basename) #üß©\n",
    "            # Process import to Databaseüí°\n",
    "            RampHC = (pl.read_excel(filename, infer_schema_length=None, engine=\"calamine\", sheet_name=\"Sheet1\") #üß©\n",
    "                    .select(pl.all(),pl.lit(file_basename).alias(\"FileName\"),pl.lit(current_modified_date).dt.cast_time_unit(\"ms\").alias(\"ModifiedDate\")\n",
    "                   )) #üß©\n",
    "            # SCHEMA checküìÉ\n",
    "            has_critical_error, critical_schema_error_msg = validate_schema(RampHC, RampHC_schema, file_basename) #üß©\n",
    "            if has_critical_error:\n",
    "                raise ValueError(critical_schema_error_msg)\n",
    "            # column structureüìÉ\n",
    "            print_colored(f\"‚öôÔ∏èAdjust column structure\", \"Olive\")\n",
    "            logger.warning(f\"‚öôÔ∏èAdjust column structure\")\n",
    "            RampHC = RampHC.with_columns(pl.col(\"Date\").cast(pl.Date)) #üß©\n",
    "            RampHC = RampHC.select(RampHC_schema) #üß©\n",
    "            print_colored(f\"‚öôÔ∏èColumn structure adjustment completed\", \"Olive\")\n",
    "            logger.info(f\"‚öôÔ∏èColumn structure adjustment completed\")\n",
    "            info_polars(RampHC) #üß© \n",
    "            #ImportüìÉ\n",
    "            print_colored(f\"üíæStart import file: {file_basename}\", \"Coral\")\n",
    "            logger.info(f\"üíæStart import file: {file_basename}\")\n",
    "            write_data(engine, RampHC_TABLE_NAME, RampHC) #üß©\n",
    "            print_colored(f\"Successfully imported‚ú®: {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Successfully imported‚ú®: {file_basename}\")\n",
    "            # Uppdate log_entries for successfully importüí°\n",
    "            log_entries.append({\"FileName\": file_basename, \"ModifiedDate\": current_modified_date, \"Error\": None})\n",
    "            print_colored(f\"Updated 'Modified' time for {file_basename}\", \"LimeGreen\")\n",
    "            logger.info(f\"Updated 'Modified' time for {file_basename}\")\n",
    "            # Uppdate log_entries for fail importüí°\n",
    "        except (\n",
    "            FileNotFoundError, PermissionError, UnicodeDecodeError, IOError,                \n",
    "            pl.exceptions.NoDataError, pl.exceptions.ComputeError, pl.exceptions.SchemaError, sa.exc.SQLAlchemyError,\n",
    "            Exception\n",
    "        ) as e:\n",
    "            error_count += 1\n",
    "            error_msg_short = f\"{type(e).__name__}: {str(e).splitlines()[0]}\"\n",
    "            logger.error(f\"Error importing {file_basename}: {error_msg_short}\", exc_info=True)\n",
    "            print_colored(f\"‚ùóÔ∏èError importing {file_basename}: {error_msg_short}\", \"OrangeRed\")\n",
    "            log_entries.append({\"FileName\": file_basename,\"ModifiedDate\": previous_modified_date,\"Error\": error_msg_short})\n",
    "    else: # Old_File with modifieddate unchangedüìÉ\n",
    "        print_colored(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\", \"Aquamarine\")\n",
    "        logger.info(f\"ModifiedDate unchanged for {file_basename}. üöÄSkipping import.\")\n",
    "display_summary(\"RampHC\", error_count) #üß©\n",
    "process_and_save_log(log_df, log_entries, log_RampHC_path) #üß©\n",
    "logger.info(\"===== Processing of the RampHC data source is complete =====\") #üß©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb453290-27af-4555-8b15-f96aa9752628",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Close DBüìÉ\n",
    "engine.dispose()\n",
    "print(\"Database connection closed.\")\n",
    "logger.info(\"Database connection closed.\")\n",
    "# Close logging\n",
    "logger.info(\"Close logging system...\")\n",
    "logging.shutdown()\n",
    "print(\"logging shutdown.\")\n",
    "%reset -f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
