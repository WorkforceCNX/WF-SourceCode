{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02cce12-2992-4604-acad-738d7a04d89d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Library listü§ñ\n",
    "import glob, logging, warnings, polars as pl, datetime, os, zipfile, xml.dom.minidom\n",
    "from datetime import datetime as dt, time as t, timedelta\n",
    "import pandas as pd, numpy as np, sqlalchemy as sa, xlsxwriter\n",
    "from sqlalchemy import create_engine, text\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "from polars.exceptions import ColumnNotFoundError, PanicException\n",
    "from pathlib import Path\n",
    "from IPython.display import HTML\n",
    "from tabulate import tabulate\n",
    "# -----------------------------------------------------------------------------------------------#\n",
    "# --- Logging configurationüìú ---\n",
    "log_directory = Path(os.environ['USERPROFILE']) / r'Concentrix Corporation//CNXVN - WFM Team - Documents//DataBase//DataFrame//BKN//ScriptLogs//'\n",
    "log_directory.mkdir(parents=True, exist_ok=True) \n",
    "log_filename = log_directory / f\"import_log_{dt.now():%Y%m%d_%H%M%S}.log\"\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # (DEBUG, INFO, WARNING, ERROR, CRITICAL)\n",
    "    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_filename, encoding='utf-8'), \n",
    "    ],force=True)\n",
    "# Create logger object\n",
    "logger = logging.getLogger('ServerImportScript')\n",
    "# -----------------------------------------------------------------------------------------------#\n",
    "# Source collectionüì•\n",
    "user_credential = Path(os.environ['USERPROFILE']) / r'Concentrix Corporation//CNXVN - WFM Team - Documents//'\n",
    "# [BKN]DATA_TRACKER üìë\n",
    "data_tracker=os.path.join(user_credential, \n",
    "                            r'DataBase//DataFrame//BKN//DATA_TRACKER')\n",
    "atd=os.path.join(user_credential, \n",
    "                            r'DataBase//DataFrame//BKN//ATD_DF')\n",
    "# -----------------------------------------------------------------------------------------------#\n",
    "# Database_Connecterüß¨\n",
    "\n",
    "server_name = \"PHMANVMDEV01V\"\n",
    "server_ip = \"10.5.11.60\"\n",
    "database = \"wfm_vn_dev\"\n",
    "user = \"usr_wfmvn_dev\"\n",
    "password = \"12guWU2OdEj5kEspl9Rlfoglf\"\n",
    "# SQL Server Authentication üîó\n",
    "connection_string = f\"mssql+pyodbc://{user}:{password}@{server_ip}/{database}?driver=ODBC+Driver+17+for+SQL+Server\"\n",
    "# Windows Authentication üîó\n",
    "# connection_string = f\"mssql+pyodbc://{server_name}/{database}?driver=ODBC+Driver+17+for+SQL+Server&Trusted_Connection=yes\"\n",
    "try:\n",
    "    engine = create_engine(connection_string, fast_executemany=True)\n",
    "    logger.info(f\"‚úÖ Successfully connected to DB: {database} server: {server_ip}\")\n",
    "except Exception as e:\n",
    "    logger.exception(\"‚ùå DB Connection error\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4f65a4-12c4-4f22-8c07-ff4d18313904",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function Definitionüõ†Ô∏è\n",
    "\n",
    "# Log Color viewüí°\n",
    "def print_colored(text, color):\n",
    "    display(HTML(f'<span style=\"color: {color};\">{text}</span>'))\n",
    "\n",
    "# Check existing log fileüí°\n",
    "def read_or_create_log(log_path):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore') # Ignor WarningüìÉ\n",
    "        try:\n",
    "            logger.debug(f\"Reading log file: {log_path}\")\n",
    "            log_df = pl.read_excel(log_path)\n",
    "            log_df = log_df.with_columns([pl.col(\"ModifiedDate\").dt.cast_time_unit(\"ms\")], strict=False)\n",
    "            logger.info(f\"Success read log file: {log_path}\")\n",
    "        except FileNotFoundError: # Create new log if can't find logüìÉ\n",
    "            logger.warning(f\"Log file not found: {log_path}. Create new log.\")\n",
    "            log_df = pl.DataFrame(\n",
    "                {\n",
    "                    \"FileName\": pl.Series([], dtype=pl.Utf8),\n",
    "                    \"ModifiedDate\": pl.Series([], dtype=pl.Datetime),\n",
    "                    \"Error\": pl.Series([], dtype=pl.Utf8),})\n",
    "        except Exception as e: # Create new log if can't open logüìÉ\n",
    "            logger.exception(f\"Error reading log file: {log_path}\")\n",
    "            print(f\"Error reading log file: {e}\")\n",
    "            log_df = pl.DataFrame(\n",
    "                {\n",
    "                    \"FileName\": pl.Series([], dtype=pl.Utf8),\n",
    "                    \"ModifiedDate\": pl.Series([], dtype=pl.Datetime),\n",
    "                    \"Error\": pl.Series([], dtype=pl.Utf8),})\n",
    "        return log_df\n",
    "        \n",
    "# Update log_dfüí°\n",
    "def process_and_save_log(log_df, log_entries, log_path):\n",
    "    if log_entries:\n",
    "        new_log_df = pl.DataFrame(log_entries)\n",
    "        log_df = log_df.with_columns(pl.col('ModifiedDate').dt.cast_time_unit(\"ms\"))\n",
    "        log_df = (pl.concat([log_df, new_log_df], how=\"diagonal_relaxed\") # Combine and remove duplicate New_Log and Old_LogüìÉ\n",
    "                  .sort(\"ModifiedDate\", descending=[False])\n",
    "                  .unique(subset=[\"FileName\"], keep=\"last\")\n",
    "                  .sort(\"FileName\", descending=[False])\n",
    "                  .select([\"FileName\", \"ModifiedDate\", \"Error\"]))\n",
    "        try:\n",
    "            log_df.write_excel(log_path, worksheet=\"ImportLog\", autofit=True)\n",
    "            print(f\"Import log saved to: {log_path}\")\n",
    "            logger.info(f\"Import log saved to: {log_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error writing log file: {e}\")\n",
    "            logger.error(f\"Error writing log file: {log_path} - {e}\")\n",
    "\n",
    "# write_dataüí°\n",
    "def write_data(engine, table_name, df): # write to databaseüìÉ\n",
    "     df.write_database(table_name=table_name, connection=engine, if_table_exists=\"append\")\n",
    "    \n",
    "# delete_dataüí°\n",
    "def delete_data(engine, table_name, filename):\n",
    "    try:\n",
    "        with engine.connect() as connection:\n",
    "            print_colored(f\"Prepare to delete old data for '{filename}' in '{table_name}'\", \"DarkTurquoise\")\n",
    "            logger.warning(f\"Prepare to delete old data for '{filename}' in '{table_name}'\")\n",
    "            delete_query = text(f\"DELETE FROM {table_name} WHERE [FileName] = :filename\")\n",
    "            connection.execute(delete_query, {\"filename\": filename})\n",
    "            connection.commit()\n",
    "            print_colored(f\"Old data deleted successfullyüßπ\", \"DarkTurquoise\")\n",
    "            logger.info(f\"'{filename}' data deleted successfully in '{table_name}' üßπ.\")\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Error while delete data for '{filename}' in '{table_name}'\")\n",
    "        print_colored(f\"Error while delete data for '{filename}' in '{table_name}'\", \"DarkTurquoise\")\n",
    "        raise \n",
    "        \n",
    "# Check Timeüí°\n",
    "def is_time_between(begin_time, end_time, check_time=None):\n",
    "    check_time = check_time or datetime.utcnow().time() # If check time is not given, default to current UTC timeüìÉ\n",
    "    if begin_time < end_time:\n",
    "        return check_time >= begin_time and check_time <= end_time\n",
    "    else: # crosses midnightüìÉ\n",
    "        return check_time >= begin_time or check_time <= end_time\n",
    "def time_difference(time1, time2):\n",
    "    seconds1 = time1.hour * 3600 + time1.minute * 60 + time1.second # Convert times to secondsüìÉ\n",
    "    seconds2 = time2.hour * 3600 + time2.minute * 60 + time2.second\n",
    "    diff_seconds = seconds1 - seconds2\n",
    "    return diff_seconds\n",
    "\n",
    "# Final Summaryüí°\n",
    "def display_summary(source_name: str, error_count: int) -> None:\n",
    "    \"\"\"Final Notice.\"\"\"\n",
    "    if error_count > 0:\n",
    "        print_colored(f\"Finished processing all files ({error_count} have errorsüõ†Ô∏è).\", \"OrangeRed\")\n",
    "        logger.warning(f\"Finished processing all files ({error_count} have errorsüõ†Ô∏è).\")\n",
    "    else:\n",
    "        print_colored(f\"Finished processing all files (no errorsüéâ).\", \"PaleVioletRed\")\n",
    "        logger.info(f\"Finished processing [{source_name}] (no errorsüéâ).\")\n",
    "\n",
    "# Default_variableüí°\n",
    "def Default_variable():\n",
    "    log_entries = []\n",
    "    error_count = 0\n",
    "    return log_entries, error_count\n",
    "\n",
    "# parse_dateüí°\n",
    "def parse_date(col: pl.Expr) -> pl.Expr:\n",
    "    return pl.coalesce(\n",
    "        col.str.strptime(pl.Date, format=\"%m/%d/%Y\", strict=False),\n",
    "        col.str.strptime(pl.Date, format=\"%Y-%m-%d\", strict=False),\n",
    "        col.str.strptime(pl.Date, format=\"%d %B %Y\", strict=False),\n",
    "        col.str.strptime(pl.Date, format=\"%B %d, %Y\", strict=False),\n",
    "        col.str.strptime(pl.Date, format=\"%d-%b-%y\", strict=False),\n",
    "        col.str.strptime(pl.Date, format=\"%Y%m%d\", strict=False),\n",
    "        col.str.strptime(pl.Date, format=\"%d/%m/%y\", strict=False),\n",
    "        col.str.strptime(pl.Date, format=\"%d-%m-%Y\", strict=False),\n",
    "    )\n",
    "\n",
    "# validate_schemaüí°\n",
    "def validate_schema(df: pl.DataFrame, expected_schema: list[str], filename: str) -> tuple[bool, str | None]:\n",
    "    # Start validation\n",
    "    start_msg = f\"üîç Starting schema validation for file: {filename}\"\n",
    "    logger.info(start_msg)\n",
    "    print_colored(start_msg, \"DodgerBlue\")\n",
    "    actual_columns = df.columns\n",
    "    expected_set = set(expected_schema)\n",
    "    actual_set = set(actual_columns)\n",
    "    missing_columns = expected_set - actual_set\n",
    "    extra_columns = actual_set - expected_set\n",
    "    has_critical_error = False\n",
    "    critical_error_message = None\n",
    "    has_warnings = False\n",
    "    # 1. Schema error (Missing columns)\n",
    "    if missing_columns:\n",
    "        has_critical_error = True\n",
    "        critical_error_message = f\"Schema error in the file: '{filename}'. Missing columns: {sorted(list(missing_columns))}\"\n",
    "        logger.error(critical_error_message)\n",
    "        print_colored(f\"‚ùóÔ∏è {critical_error_message}\", \"OrangeRed\")\n",
    "    # 2. warning extra columns\n",
    "    if extra_columns:\n",
    "        has_warnings = True\n",
    "        warning_message = f\"warning schema for file '{filename}'. Extra columns: {sorted(list(extra_columns))}. These columns will be excluded from the import process.\"\n",
    "        logger.warning(warning_message)\n",
    "        print_colored(f\"‚ö†Ô∏è {warning_message}\", \"Gold\")\n",
    "    # 3. Final results announcement\n",
    "    if not has_critical_error and not has_warnings:\n",
    "        final_msg = f\"‚úÖ Completely valid schema for the file: {filename}.\"\n",
    "        logger.info(final_msg)\n",
    "        print_colored(final_msg, \"MediumSeaGreen\")\n",
    "    elif not has_critical_error and has_warnings:\n",
    "        final_msg = f\"‚ö†Ô∏è File schema check: {filename} Passed (No missing columns, extra columns warned)\"\n",
    "        logger.info(final_msg)\n",
    "        print_colored(final_msg, \"MediumSeaGreen\") # V·∫´n d√πng m√†u xanh l√°\n",
    "    elif has_critical_error:\n",
    "        final_msg = f\"‚ùå Schema validation failed due to missing column(s) for file: {filename}.\"\n",
    "        logger.warning(final_msg) # Log ·ªü m·ª©c warning ho·∫∑c error t√πy √Ω\n",
    "        print_colored(final_msg, \"OrangeRed\")\n",
    "    return has_critical_error, critical_error_message\n",
    "    \n",
    "# DF Infoüí°\n",
    "def info_polars(df: pl.DataFrame):\n",
    "    print_colored(f\"‚öôÔ∏èFinal structure\", \"Olive\")\n",
    "    logger.info(f\"‚öôÔ∏èFinal structure\")\n",
    "    shape = df.shape\n",
    "    print(f\"Shape: {shape}\")\n",
    "    print(\"Data columns:\")  \n",
    "    table_data = []\n",
    "    for i, name in enumerate(df.columns):\n",
    "        dtype = df.dtypes[i]\n",
    "        non_null_count = df.select(pl.col(name).is_not_null().sum()).item()\n",
    "        table_data.append([i, name, non_null_count, dtype])  \n",
    "    headers = [\"#\", \"Column\", \"Non-Null Count\", \"Dtype\"]\n",
    "    print(tabulate(table_data, headers=headers, tablefmt=\"grid\"))\n",
    "    logger.info(tabulate(table_data, headers=headers, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d50864-3a80-4ea8-b858-1dfb2761f544",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# MaintainDatabaseüß∞\n",
    "\n",
    "print_colored(\"===== Starting Index Rebuild Process =====\", \"DodgerBlue\")\n",
    "\n",
    "MaintainDatabase_sql = \"\"\"\n",
    "\n",
    "EXEC BCOM.usp_MaintainDatabase\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    with engine.connect() as connection:\n",
    "        print_colored(\"‚öôÔ∏èExecuting Index Rebuild script (this may take a long time)...\", \"DarkOrange\")\n",
    "        connection.execute(text(MaintainDatabase_sql))\n",
    "        connection.commit() \n",
    "        print_colored(\"‚úîÔ∏èIndex Rebuild script execution command sent and committed. Check SQL Server logs/output for details.\", \"MediumSeaGreen\")\n",
    "except sa.exc.SQLAlchemyError as e_db:\n",
    "    print_colored(f\"‚ùå Database error during Index Rebuild Process: {e_db}\", \"OrangeRed\")\n",
    "except Exception as e_general:\n",
    "    print_colored(f\"‚ùå An unexpected error occurred during Index Rebuild Process: {e_general}\", \"OrangeRed\")\n",
    "\n",
    "print_colored(\"===== Index Rebuild Process attempt is complete (Python perspective) =====\", \"DodgerBlue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d169c9-b768-4c75-aab7-78c43b36508f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ‚öôÔ∏èCreate_EEAAO\n",
    "logger.info(\"===== Starting EEAAO Process =====\")\n",
    "# EXEC EEAAO Procedure\n",
    "Exec_EEAAO = \"\"\"\n",
    "EXEC BCOM.Refresh_EEAAO_Data;\n",
    "\"\"\"\n",
    "select_query = \"\"\"\n",
    "SELECT TOP 5 * FROM BCOM.EEAAO;\n",
    "\"\"\"\n",
    "try:\n",
    "    with engine.connect() as connection:\n",
    "        logger.info(\"‚öôÔ∏èExecuting procedure EEAAO ...\")\n",
    "        print(\"‚öôÔ∏èExecuting procedure EEAAO ...\")\n",
    "        connection.execute(text(Exec_EEAAO))\n",
    "        connection.commit()\n",
    "        logger.info(\"‚úîÔ∏èSuccessfully executed and committed Procedure EEAAO.\")\n",
    "        print(\"‚úîÔ∏èSuccessfully executed and committed Procedure EEAAO.\")\n",
    "        logger.info(f\"Reading data from BCOM.EEAAO with query: {select_query.strip()}\")\n",
    "        print(f\"Reading data from BCOM.EEAAO with query: {select_query.strip()}\")\n",
    "        df_eeao_result = pl.read_database(query=select_query, connection=connection)\n",
    "        if df_eeao_result is not None and not df_eeao_result.is_empty():\n",
    "            print_colored(\"Sample data from BCOM.EEAAO after refresh:\", \"MediumSeaGreen\")\n",
    "            display(df_eeao_result)\n",
    "            logger.info(f\"Successfully read {df_eeao_result.shape[0]} rows from BCOM.EEAAO.\")\n",
    "        else:\n",
    "            logger.warning(\"No data returned from BCOM.EEAAO after refresh or procedure did not complete in time.\")\n",
    "            print_colored(\"No data returned from BCOM.EEAAO after refresh.\", \"OrangeRed\")\n",
    "except sa.exc.SQLAlchemyError as e:\n",
    "    logger.error(f\"Database error during EEAAO Process: {e}\", exc_info=True)\n",
    "    print(f\"Database error: {e}\") \n",
    "except Exception as e:\n",
    "    logger.error(f\"An unexpected error occurred during EEAAO Process: {e}\", exc_info=True)\n",
    "    print(f\"An unexpected error: {e}\")\n",
    "logger.info(\"===== Processing of EEAAO is complete =====\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebb3d68-c268-4c84-a36f-2d59fe42cf9a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# PowerBI Noticeüîä\n",
    "\n",
    "sql_query = \"\"\"\n",
    "/* DATA TRACKER  */\n",
    " \n",
    "--EPS\n",
    "select 'Working Hours' as [DISPLAY_NAME],cast(max([Session Login]) as date) as [LastestData] from BCOM.EPS\n",
    "union all\n",
    "--CPI\n",
    "select 'CPI' as [DISPLAY_NAME],cast(max([Date]) as date) as [LastestData] from BCOM.CPI\n",
    "union all\n",
    "--CPI_PEGA\n",
    "select 'PEGA Swivel' as [DISPLAY_NAME],cast(max([Day of Date]) as date) as [LastestData] from BCOM.CPI_PEGA\n",
    "union all\n",
    "--CSAT_RS\n",
    "select 'CSAT' as [DISPLAY_NAME],cast(max([Sort by Dimension]) as date) as [LastestData] from BCOM.CSAT_RS\n",
    "union all\n",
    "--PSAT\n",
    "select 'PSAT' as [DISPLAY_NAME],cast(max([Sorted by Dimension]) as date) as [LastestData] from BCOM.PSAT\n",
    "union all\n",
    "--Quality\n",
    "select 'QUALITY' as [DISPLAY_NAME],cast(max([eval_date]) as date) as [LastestData] from BCOM.Quality\n",
    "union all\n",
    "--AHT2\n",
    "select 'AHT' as [DISPLAY_NAME],cast(max([Date]) as date) as [LastestData] from BCOM.AHT2\n",
    "union all\n",
    "--ROSTER\n",
    "select 'ROSTER' as [DISPLAY_NAME],cast(max([Attribute]) as date) as [LastestData] from BCOM.ROSTER\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Read Query\n",
    "datatracker_df = pl.read_database(query=sql_query, connection=engine)\n",
    "engine.dispose()\n",
    "datatracker_df = datatracker_df.with_columns(pl.col(\"LastestData\").dt.strftime(\"%Y-%m-%d\"))\n",
    "# Export to CSV\n",
    "os.chdir(data_tracker)\n",
    "datatracker_CSV = datatracker_df.write_excel(workbook=\"BKN_DATA_TRACKER.xlsx\",worksheet=\"Sheet1\")  \n",
    "datatracker_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b57d26f-11ce-4205-af47-9701dd8fff92",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ATD to TEAMSüîó\n",
    "\n",
    "sql_query_link = \"\"\"\n",
    "SELECT [Date],[LOB],[TL_Name],[Emp ID],[Emp_Name] AS [Name],[Shift],\n",
    "CASE WHEN [ScheduleHours(H)]>4 and [CUICLoggedTime(s)]>0 then 1 \n",
    "when [ScheduleHours(H)]>0 and [CUICLoggedTime(s)]>0 then 0.5 else 0 end as [Present],\n",
    "CASE WHEN [SchedLeave(H)]-[SchedUPL(H)]>4 THEN 1 WHEN [SchedLeave(H)]-[SchedUPL(H)]>0 THEN 0.5 ELSE 0 END AS [PlanLeave],\n",
    "CASE WHEN [SchedUPL(H)]>4 THEN 1 WHEN [SchedUPL(H)]>0 THEN 0.5 ELSE 0 END AS [UnplanLeave],\n",
    "CASE WHEN [ScheduleHours(H)]>0 and [CUICLoggedTime(s)]<=0 then \n",
    "(CASE WHEN [ScheduleHours(H)]>4 THEN 1 WHEN [ScheduleHours(H)]>0 THEN 0.5 ELSE 0 END)\n",
    "else 0 end as [UPL-Not present at desk]\n",
    "FROM BCOM.EEAAO\n",
    "where [Date]=DATEADD(DAY, -1,CAST(GETDATE() As Date)) and [Shift]not in ('OFF','Training','New Hire Training')\n",
    "Order by [TL_Name], [Shift] DESC\n",
    "\"\"\"\n",
    "sql_query_overall = \"\"\"\n",
    "with atd as (\n",
    "SELECT [Date],[LOB],[TL_Name],[Emp ID],[Emp_Name] AS [Name],[week_shift],[Shift],\n",
    "CASE WHEN [ScheduleHours(H)]>4 and [CUICLoggedTime(s)]>0 then 1 \n",
    "when [ScheduleHours(H)]>0 and [CUICLoggedTime(s)]>0 then 0.5 else 0 end as [ATD],\n",
    "CASE WHEN [ScheduleHours(H)]>4 THEN 1 WHEN [ScheduleHours(H)]>0 THEN 0.5 ELSE 0 END AS [NormalShift],\n",
    "CASE WHEN [SchedLeave(H)]-[SchedUPL(H)]>4 THEN 1 WHEN [SchedLeave(H)]-[SchedUPL(H)]>0 THEN 0.5 ELSE 0 END AS [PlanLeave],\n",
    "CASE WHEN [SchedUPL(H)]>4 THEN 1 WHEN [SchedUPL(H)]>0 THEN 0.5 ELSE 0 END AS [UnplanLeave],\n",
    "CASE WHEN [ScheduleHours(H)]>0 and [CUICLoggedTime(s)]<=0 then \n",
    "(CASE WHEN [ScheduleHours(H)]>4 THEN 1 WHEN [ScheduleHours(H)]>0 THEN 0.5 ELSE 0 END)\n",
    "else 0 end as [UPL-Not present at desk],\n",
    "CASE WHEN [Shift]='OFF' then 0 else 1 end as [Headcount]\n",
    "FROM BCOM.EEAAO\n",
    "where [Date]=DATEADD(DAY, -1,CAST(GETDATE() As Date)) and [Shift] not in ('OFF','Training','New Hire Training'))\n",
    "select [Date],\n",
    "sum([Headcount]) as [Headcount],sum([ATD]) as [Present],sum([UPL-Not present at desk]) as [UPL-Not present at desk],\n",
    "sum([UnplanLeave]) as [UPL-Request within 7 days],sum([PlanLeave]) as [PlanLeave],\n",
    "case when sum([Headcount])=0 then 0 else sum([ATD])/sum([Headcount]) end as [Present%]\n",
    "from atd\n",
    "group by [Date]\n",
    "\"\"\"\n",
    "sql_query_detail = \"\"\"\n",
    "with atd as (\n",
    "SELECT [Date],[LOB],[TL_Name],[Emp ID],[Emp_Name] AS [Name],[week_shift],[Shift],\n",
    "CASE WHEN [ScheduleHours(H)]>4 and [CUICLoggedTime(s)]>0 then 1 \n",
    "when [ScheduleHours(H)]>0 and [CUICLoggedTime(s)]>0 then 0.5 else 0 end as [ATD],\n",
    "CASE WHEN [ScheduleHours(H)]>4 THEN 1 WHEN [ScheduleHours(H)]>0 THEN 0.5 ELSE 0 END AS [NormalShift],\n",
    "CASE WHEN [SchedLeave(H)]-[SchedUPL(H)]>4 THEN 1 WHEN [SchedLeave(H)]-[SchedUPL(H)]>0 THEN 0.5 ELSE 0 END AS [PlanLeave],\n",
    "CASE WHEN [SchedUPL(H)]>4 THEN 1 WHEN [SchedUPL(H)]>0 THEN 0.5 ELSE 0 END AS [UnplanLeave],\n",
    "CASE WHEN [ScheduleHours(H)]>0 and [CUICLoggedTime(s)]<=0 then \n",
    "(CASE WHEN [ScheduleHours(H)]>4 THEN 1 WHEN [ScheduleHours(H)]>0 THEN 0.5 ELSE 0 END)\n",
    "else 0 end as [UPL-Not present at desk],\n",
    "CASE WHEN [Shift]='OFF' then 0 else 1 end as [Headcount]\n",
    "FROM BCOM.EEAAO\n",
    "where [Date]=DATEADD(DAY, -1,CAST(GETDATE() As Date)) and [Shift] not in ('OFF','Training','New Hire Training'))\n",
    "select [Date],[TL_Name],\n",
    "sum([Headcount]) as [Headcount],sum([ATD]) as [Present],sum([UPL-Not present at desk]) as [UPL-Not present at desk],\n",
    "sum([UnplanLeave]) as [UPL-Request within 7 days],sum([PlanLeave]) as [PlanLeave],\n",
    "case when sum([Headcount])=0 then 0 else sum([ATD])/sum([Headcount]) end as [Present%]\n",
    "from atd\n",
    "group by [Date],[TL_Name]\n",
    "order by case when sum([Headcount])=0 then 0 else sum([ATD])/sum([Headcount]) end asc\n",
    "\"\"\"\n",
    "\n",
    "# Read Query\n",
    "atd_link = pl.read_database(query=sql_query_link, connection=engine)\n",
    "atd_overall = pl.read_database(query=sql_query_overall, connection=engine)\n",
    "atd_detail = pl.read_database(query=sql_query_detail, connection=engine)\n",
    "atd_overall = atd_overall.with_columns(pl.col(\"Date\").dt.to_string(\"%Y-%m-%d\"))\n",
    "atd_detail = atd_detail.with_columns(pl.col(\"Date\").dt.to_string(\"%Y-%m-%d\"))\n",
    "atd_overall = atd_overall.to_pandas()\n",
    "atd_detail = atd_detail.to_pandas()\n",
    "atd_overall['Present%'] = atd_overall['Present%'].map('{:.2%}'.format)\n",
    "atd_detail['Present%'] = atd_detail['Present%'].map('{:.2%}'.format)\n",
    "atd_overall = pl.from_pandas(atd_overall)\n",
    "atd_detail = pl.from_pandas(atd_detail)\n",
    "# Export to CSV\n",
    "os.chdir(atd)\n",
    "atd_link_CSV = atd_link.write_csv(\"BKN_ATD_DF.csv\")  \n",
    "atd_overall_CSV = atd_overall.write_excel(workbook=\"BKN_pivot_ATD_DF_total.xlsx\",worksheet=\"Sheet1\",\n",
    "                                           table_name='Frame0', table_style='Table Style Medium 2',autofit=True) \n",
    "atd_detail_CSV = atd_detail.write_excel(workbook=\"BKN_pivot_ATD_DF.xlsx\",worksheet=\"Sheet1\",\n",
    "                                           table_name='Frame0', table_style='Table Style Medium 2',autofit=True)  \n",
    "# Display\n",
    "display(atd_link)\n",
    "display(atd_overall)\n",
    "display(atd_detail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229fc7cb-c8af-4e7f-980d-f0c272d71043",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Close DBüìÉ\n",
    "engine.dispose()\n",
    "print(\"Database connection closed.\")\n",
    "%reset -f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
