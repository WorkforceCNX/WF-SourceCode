{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02cce12-2992-4604-acad-738d7a04d89d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Library listü§ñ\n",
    "import glob, logging, warnings, polars as pl, datetime, os, zipfile, xml.dom.minidom\n",
    "from datetime import datetime as dt, time as t, timedelta\n",
    "import pandas as pd, numpy as np, sqlalchemy as sa, xlsxwriter\n",
    "from sqlalchemy import create_engine, text\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "from polars.exceptions import ColumnNotFoundError, PanicException\n",
    "from pathlib import Path\n",
    "from IPython.display import HTML\n",
    "from tabulate import tabulate\n",
    "# -----------------------------------------------------------------------------------------------#\n",
    "# --- Logging configurationüìú ---\n",
    "log_directory = Path(os.environ['USERPROFILE']) / r'Concentrix Corporation//CNXVN - WFM Team - Documents//DataBase//DataFrame//BKN//ScriptLogs//'\n",
    "log_directory.mkdir(parents=True, exist_ok=True) \n",
    "log_filename = log_directory / f\"import_log_{dt.now():%Y%m%d_%H%M%S}.log\"\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # (DEBUG, INFO, WARNING, ERROR, CRITICAL)\n",
    "    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_filename, encoding='utf-8'), \n",
    "    ],force=True)\n",
    "# Create logger object\n",
    "logger = logging.getLogger('ServerImportScript')\n",
    "# -----------------------------------------------------------------------------------------------#\n",
    "# Source collectionüì•\n",
    "user_credential = Path(os.environ['USERPROFILE']) / r'Concentrix Corporation//CNXVN - WFM Team - Documents//'\n",
    "\n",
    "# 0Ô∏è‚É£1Ô∏è‚É£[BKN]AHT2üóÉÔ∏è\n",
    "AHT2_TABLE_NAME = \"BCOM.AHT2\"\n",
    "Folder_AHT2 = user_credential / r'DataBase//DataRaw//BKN//AHT2//'\n",
    "log_AHT2_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//AHT2_log.xlsx'\n",
    "AHT2_schema = ['FileName', 'ModifiedDate', 'Date', 'Agent Name Display', 'Answered Language Name', 'Measure Names', 'Measure Values']\n",
    "# 0Ô∏è‚É£2Ô∏è‚É£[BKN]ROSTERüóÉÔ∏è\n",
    "ROSTER_TABLE_NAME = \"BCOM.ROSTER\"\n",
    "Folder_ROSTER = user_credential / r'DataBase//DataRaw//BKN//ROSTER//'\n",
    "log_ROSTER_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//ROSTER_log.xlsx'\n",
    "ROSTER_schema = ['FileName', 'ModifiedDate', 'Emp ID', 'Name', 'Attribute', 'Value', 'LOB', \n",
    "                 'team_leader', 'week_shift', 'week_off', 'OM', 'DPE', 'Work Type']\n",
    "# 0Ô∏è‚É£3Ô∏è‚É£[BKN]EPSüóÉÔ∏è\n",
    "EPS_TABLE_NAME = \"BCOM.EPS\"\n",
    "Folder_EPS = user_credential / r'DataBase//DataRaw//BKN//EPS//'\n",
    "log_EPS_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//EPS_log.xlsx'\n",
    "EPS_schema = ['FileName', 'ModifiedDate', 'sitecode', 'manager_username', 'Username', 'Date', 'Session Login', \n",
    "              'Session Logout', 'Session Time', 'BPE Code', 'Total Time', 'SessionLogin_VN', 'SessionLogout_VN',\n",
    "              'NightTime', 'DayTime', 'Night_BPE', 'Day_BPE']\n",
    "# 0Ô∏è‚É£4Ô∏è‚É£[BKN]CPIüóÉÔ∏è\n",
    "CPI_TABLE_NAME = \"BCOM.CPI\"\n",
    "Folder_CPI = user_credential / r'DataBase//DataRaw//BKN//CPI//'\n",
    "log_CPI_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//CPI_log.xlsx'\n",
    "CPI_schema = ['FileName', 'ModifiedDate', 'Date', 'Staff Name', 'Hour Interval Selected', 'Channel', \n",
    "              'Item Label', 'Item ID', \"'Item ID'\", 'Time Alert', 'Nr. Contacts', 'Item Link', 'Time']\n",
    "# 0Ô∏è‚É£5Ô∏è‚É£[GLB]RAMCOüóÉÔ∏è\n",
    "RAMCO_TABLE_NAME = \"GLB.RAMCO\"\n",
    "Folder_RAMCO = user_credential / r'DataBase//DataRaw//GLOBAL//RAMCO//'\n",
    "log_RAMCO_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//RAMCO_log.xlsx'\n",
    "RAMCO_schema = ['FileName', 'ModifiedDate', 'EID', 'Employee_Name', 'Employee_type', 'Date', 'Code']\n",
    "# 0Ô∏è‚É£6Ô∏è‚É£[GLB]OT_RAMCOüóÉÔ∏è\n",
    "OT_RAMCO_TABLE_NAME = \"GLB.OT_RAMCO\"\n",
    "Folder_OT_RAMCO = user_credential / r'DataBase//DataRaw//GLOBAL//OT_RAMCO//'\n",
    "log_OT_RAMCO_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//OT_RAMCO_log.xlsx'\n",
    "OT_RAMCO_schema = ['FileName', 'ModifiedDate', 'employee_code', 'employee_name', 'Employee Type', 'OT Type', 'Date', 'Status', 'Hours']\n",
    "# 0Ô∏è‚É£7Ô∏è‚É£[GLB]PremHdaysüóÉÔ∏è\n",
    "PremHdays_TABLE_NAME = \"GLB.PremHdays\"\n",
    "Folder_PremHdays = user_credential / r'DataBase//DataRaw//GLOBAL//HOLIDAY_MAPPING//'\n",
    "log_PremHdays_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//PremHdays_log.xlsx'\n",
    "PremHdays_schema = ['FileName', 'ModifiedDate', 'Date', 'Holiday']\n",
    "# 0Ô∏è‚É£8Ô∏è‚É£[GLB]NormHdaysüóÉÔ∏è\n",
    "NormHdays_TABLE_NAME = \"GLB.NormHdays\"\n",
    "Folder_NormHdays = user_credential / r'DataBase//DataRaw//GLOBAL//HOLIDAY_MAPPING_NONBILLABLE//'\n",
    "log_NormHdays_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//NormHdays_log.xlsx'\n",
    "NormHdays_schema = ['FileName', 'ModifiedDate', 'Solar Day', 'Lunar Day', 'Holiday']\n",
    "# 0Ô∏è‚É£9Ô∏è‚É£[GLB]EmpMasterüóÉÔ∏è\n",
    "EmpMaster_TABLE_NAME = \"GLB.EmpMaster\"\n",
    "Folder_EmpMaster = user_credential / r'DataBase//DataRaw//GLOBAL//WDD//'\n",
    "log_EmpMaster_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//EmpMaster_log.xlsx'\n",
    "EmpMaster_schema = ['FileName', 'ModifiedDate', 'EMPLOYEE_NUMBER', 'PREVIOUS_PAYROLL_ID', 'FIRST_NAME', 'MIDDLE_NAME', 'LAST_NAME', \n",
    "                    'FULL_NAME', 'Work Related Status', 'Work Related (Extended Status)', 'Service Type', 'WAH & Hybrid Platform', \t\n",
    "                    'ORIGINAL_DATE_OF_HIRE', 'LEGAL_EMPLOYER_HIRE_DATE', 'Continuous Service Date', 'Fixed Term Hire End Date', \n",
    "                    'Contract End Date', 'PERSON_TYPE', 'WORKER_CATEGORY', 'Time Type', 'Employee Type', 'Last Promotion Date', \n",
    "                    'Assignment Category', 'Email - Work', 'BUSINESS_UNIT', 'Job Code', 'Job Title', 'Business Title', 'Cost Center - ID', \n",
    "                    'Cost Center - Name', 'LOCATION_CODE', 'LOCATION_NAME', 'CNX BU', 'Concentrix LOB', 'Process', 'COMPANY', \n",
    "                    'MANAGEMENT_LEVEL', 'Job Level', 'Compensation Grade', 'JOB_FUNCTION_DESCRIPTION', 'JOB_FAMILY', 'MSA', 'MSA Client', \n",
    "                    'MSA Program', 'ACTIVITY ID', 'SUPERVISOR_ID', 'SUPERVISOR_FULL_NAME', 'SUPERVISOR_EMAIL_ID', 'MANAGER_02_ID', \n",
    "                    'MANAGER_02_FULL_NAME', 'MANAGER_02_EMAIL_ID', 'COMP_CODE', 'CITY', 'Location', 'Country', 'Employee Status', 'Work Shift']\n",
    "# 1Ô∏è‚É£0Ô∏è‚É£[GLB]TerminationüóÉÔ∏è\n",
    "Termination_TABLE_NAME = \"GLB.Termination\"\n",
    "Folder_Termination = user_credential / r'DataBase//DataRaw//GLOBAL//WDD//'\n",
    "log_Termination_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//Termination_log.xlsx'\n",
    "Termination_schema = ['FileName', 'ModifiedDate', 'EMPLOYEE_ID', 'PREVIOUS_PAYROLL_ID', 'FIRST_NAME', 'MIDDLE_NAME', 'LAST_NAME', 'FULL_NAME', \n",
    "                      'EMAIL_ADDRESS', 'HIRE_DATE', 'ORIGINAL_HIRE_DATE', 'END EMPLOYMENT DATE', 'Contract End Date', 'Termination Date', \n",
    "                      'Termination Date (DD/MM/YY)', 'Eligible for Rehire', 'LWD', 'MOST RECENT TERMINATION - DATE INITIATED', \n",
    "                      'MOST RECENT TERMINATION - DATE COMPLETED', 'MOST RECENT TERMINATION - EFFECTIVE DATE', 'MOST RECENT TERMINATION - REASON', \n",
    "                      'Action date', 'DATE INITIATED', 'COMPELETED DATE AND TIME', 'TERMINATION DATE 2', 'Is Initiated through Resignation', \n",
    "                      'Termination Reason', 'Resignation Reason', 'Secondary Termination Reasons', 'Resignation Notice served', 'PERSON_TYPE', \n",
    "                      'Time Type', 'Employee Type', 'Worker Type', 'Assignment Category', 'WORKER_CATEGORY', 'BUSINESS_UNIT', 'Cost Center', \n",
    "                      'Cost Center - ID', 'JOB_CODE', 'JOB_TITLE', 'BUSINESS_TITLE', 'LOCATION_NAME', 'LOCATION_CODE', 'COUNTRY', 'COMPANY', \n",
    "                      'MANAGEMENT LEVEL', 'JOB LEVEL', 'JOB_FAMILY', 'JOB_FUNCTION', 'JOB_ROLE', 'MSA', 'CNX BU', 'Concentrix LOB', 'Process', \n",
    "                      'Client Name ( Process )', 'Compensation Grade', 'SUPERVISOR_ID', 'SUPERVISOR_FULL_NAME', 'SUPERVISOR_EMAIL_ID', 'COMP_CODE', \n",
    "                      'CITY', 'LOCATION_DESCRIPTION', 'EMPLOYEE STATUS', 'Continuous Service Date', 'Work Related Status', \n",
    "                      'Work Related (Extended Status)', 'Activity', 'MSA Legacy Project ID']\n",
    "# 1Ô∏è‚É£1Ô∏è‚É£[GLB]ResignationüóÉÔ∏è\n",
    "Resignation_TABLE_NAME = \"GLB.Resignation\"\n",
    "Folder_Resignation = user_credential / r'DataBase//DataRaw//GLOBAL//WDD//'\n",
    "log_Resignation_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//Resignation_log.xlsx'\n",
    "Resignation_schema = ['FileName', 'ModifiedDate', 'Employee ID', 'Full Name', 'Job Family', 'MSA Client', 'Country', 'Location', 'Action', \n",
    "                      'Action Date', 'Date and Time Initiated', 'Status', 'Primary Reason', 'Secondary Reasons', 'Notification Date', 'Awaiting Persons', \n",
    "                      'Resignation Primary Reason', 'Hire Date', 'Proposed Termination Date', 'Notice Served', 'Sup ID', 'Supervisor Name', \n",
    "                      'Employee Status', 'Activity', 'MSA Legacy Project ID', 'Initiated By']\n",
    "# 1Ô∏è‚É£2Ô∏è‚É£[BKN]CPI_PEGAüóÉÔ∏è\n",
    "CPI_PEGA_TABLE_NAME = \"BCOM.CPI_PEGA\"\n",
    "Folder_CPI_PEGA = user_credential / r'DataBase//DataRaw//BKN//CPI_PEGA//'\n",
    "log_CPI_PEGA_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//CPI_PEGA_log.xlsx'\n",
    "CPI_PEGA_schema = ['FileName', 'ModifiedDate', 'Staff Name', 'Operator Def', 'Service Case Type New', 'Channel Def',\t\n",
    "                   'Lang Def', 'Reason For No Service Case', 'Topic Def New', 'Subtopics', 'Case Id', 'Reservation Id Def',\n",
    "                   'Day of Date', 'Blank', '# Swivels', 'Count of ServiceCase or Interaction']\n",
    "# 1Ô∏è‚É£3Ô∏è‚É£[BKN]StaffüóÉÔ∏è\n",
    "Staff_TABLE_NAME = \"BCOM.Staff\"\n",
    "Folder_Staff = user_credential / r'DataBase//DataRaw//BKN//AGENTS//'\n",
    "log_Staff_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//Staff_log.xlsx'\n",
    "Staff_schema = ['FileName', 'ModifiedDate', 'Employee_ID', 'GEO', 'Site_ID', 'Employee_Last_Name', 'Employee_First_Name', 'Status', 'Wave #', \n",
    "                'Role', 'Booking Login ID', 'Language Start Date', 'TED Name', 'CUIC Name', 'EnterpriseName', 'Hire_Date', 'PST_Start_Date',\n",
    "                'Production_Start_Date', 'LWD', 'Termination_Date', 'Designation', 'cnx_email', 'Booking Email', 'WAH Category', 'Full name',\n",
    "                'IEX', 'serial_number', 'BKN_ID', 'Extension Number']\n",
    "# 1Ô∏è‚É£4Ô∏è‚É£[BKN]ConTracküóÉÔ∏è\n",
    "ConTrack_TABLE_NAME = \"BCOM.ConTrack\"\n",
    "Folder_ConTrack = Path(os.environ['USERPROFILE']) / r'OneDrive - Concentrix Corporation//DataBase//DataRaw//BKN//ContactTracker//'\n",
    "log_ConTrack_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//ConTrack_log.xlsx'\n",
    "ConTrack_schema = ['FileName', 'ModifiedDate', 'Id', 'Start time', 'Completion time', 'Email', 'Name', 'Reservation Number', 'Contact Types',\n",
    "                   'Contact Parties', 'Unbabel Tool Used?', 'Backlog Case', 'How many days since guest contacted? (ex: 30)', 'Topics',\n",
    "                   'Resolutions', 'Reason If Skipped', 'CRM used', 'Outbound to Senior', 'Outbound Status','Reason (Name - Site of Senior)',\n",
    "                   'Note', 'Reason for cannot make OB call to Guest', 'Is it possible to make Outbound call to Guest?¬†', 'Language']\n",
    "# 1Ô∏è‚É£5Ô∏è‚É£[BKN]QualityüóÉÔ∏è\n",
    "Quality_TABLE_NAME = \"BCOM.Quality\"\n",
    "Folder_Quality = user_credential / r'DataBase//DataRaw//BKN//QUALITY//'\n",
    "log_Quality_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//Quality_log.xlsx'\n",
    "Quality_schema = ['FileName', 'ModifiedDate', 'eps_name', 'eval_id', 'eval_date', 'agent_username', 'evaluator_username', 'result',\t\n",
    "                  'final_question_grouping', 'template_group', 'eval_template_name', 'sections', 'sitecode', 'score_n', 'score_question_weight',\n",
    "                  'eval_language', 'eval_reference', 'tix_final_topic', 'tix_final_subtopic', 'csat_language_code', 'csat_satisfied']\n",
    "# 1Ô∏è‚É£6Ô∏è‚É£[BKN]RONAüóÉÔ∏è\n",
    "RONA_TABLE_NAME = \"BCOM.RONA\"\n",
    "Folder_RONA = user_credential / r'DataBase//DataRaw//BKN//RONA//'\n",
    "log_RONA_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//RONA_log.xlsx'\n",
    "RONA_schema = ['FileName', 'ModifiedDate', 'Agent', 'DateTime', 'RONA']\n",
    "# 1Ô∏è‚É£7Ô∏è‚É£[BKN]CUICüóÉÔ∏è\n",
    "CUIC_TABLE_NAME = \"BCOM.CUIC\"\n",
    "Folder_CUIC = user_credential / r'DataBase//DataRaw//BKN//CUIC//'\n",
    "log_CUIC_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//CUIC_log.xlsx'\n",
    "CUIC_schema = ['FileName', 'ModifiedDate', 'FullName', 'LoginName', 'Interval', 'AgentAvailTime', 'AgentLoggedOnTime']\n",
    "# 1Ô∏è‚É£8Ô∏è‚É£[BKN]KPI_TargetüóÉÔ∏è\n",
    "KPI_Target_TABLE_NAME = \"BCOM.KPI_Target\"\n",
    "Folder_KPI_Target = user_credential / r'DataBase//DataRaw//BKN//KPI_TARGET//'\n",
    "log_KPI_Target_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//KPI_Target_log.xlsx'\n",
    "KPI_Target_schema = ['FileName', 'ModifiedDate', 'LOB', 'LOB Group', 'Week', 'Tenure days', 'Overall CPH tar', 'Phone CPH tar', 'Non Phone CPH tar',\t\n",
    "                     'Quality - Customer Impact tar', 'Quality - Business Impact tar', 'Quality - Compliance Impact tar', 'Quality - Overall tar', 'AHT Phone tar',\t\n",
    "                     'AHT Non-phone tar', 'AHT Overall tar', 'Hold (phone) tar', 'AACW (phone) tar', 'Avg Talk Time tar', 'Phone CSAT tar', 'Non phone CSAT tar',\t\n",
    "                     'Overall CSAT tar', 'PSAT tar', 'PSAT Vietnamese tar', 'PSAT English (American) tar', 'PSAT English (Great Britain) tar', 'CSAT Reso tar',\n",
    "                     'Quality - personalization tar', 'Quality - proactivity tar', 'Quality - resolution tar']\n",
    "# 1Ô∏è‚É£9Ô∏è‚É£[BKN]LogoutCountüóÉÔ∏è\n",
    "LogoutCount_TABLE_NAME = \"BCOM.LogoutCount\"\n",
    "Folder_LogoutCount = user_credential / r'DataBase//DataRaw//BKN//LOGOUT_COUNT//'\n",
    "log_LogoutCount_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//LogoutCount_log.xlsx'\n",
    "LogoutCount_schema = ['FileName', 'ModifiedDate', 'Aggregation', 'TimeDimension', 'KPI Value Formatted']\n",
    "# 2Ô∏è‚É£0Ô∏è‚É£[BKN]WpDetailüóÉÔ∏è\n",
    "WpDetail_TABLE_NAME = \"BCOM.WpDetail\"\n",
    "Folder_WpDetail = user_credential / r'DataBase//DataRaw//BKN//WP_DETAIL//'\n",
    "log_WpDetail_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//WpDetail_log.xlsx'\n",
    "WpDetail_schema = ['FileName', 'ModifiedDate', 'LOB', 'ID', 'DateTime_Start', 'DateTime_End', 'Date_Start', 'Date_end', 'Time_Start', 'Time_End', \n",
    "                   'Dur', 'Action', 'DateTime_Act_Start', 'DateTime_Act_End', 'Date_Act_Start', 'Date_Act_End', 'Time_Act_Start', 'Time_Act_End', 'Act_Dur']\n",
    "# 2Ô∏è‚É£1Ô∏è‚É£[BKN]WpSummaryüóÉÔ∏è\n",
    "WpSummary_TABLE_NAME = \"BCOM.WpSummary\"\n",
    "Folder_WpSummary = user_credential / r'DataBase//DataRaw//BKN//WP_SUMMARY//'\n",
    "log_WpSummary_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//WpSummary_log.xlsx'\n",
    "WpSummary_schema = ['FileName', 'ModifiedDate', 'LOB', 'Date', 'Agent ID', 'Agent Name', 'Scheduled Activity', 'Length', 'Percent']\n",
    "# 2Ô∏è‚É£2Ô∏è‚É£[BKN]RegisteredOTüóÉÔ∏è\n",
    "RegisteredOT_TABLE_NAME = \"BCOM.RegisteredOT\"\n",
    "Folder_RegisteredOT = user_credential / r'DataBase//DataRaw//BKN//OVERTIME//'\n",
    "log_RegisteredOT_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//RegisteredOT_log.xlsx'\n",
    "RegisteredOT_schema = ['FileName', 'ModifiedDate', 'Emp ID', 'Name', 'Date', 'Value', 'OT', 'LOB','Type']\n",
    "# 2Ô∏è‚É£3Ô∏è‚É£[BKN]CSAT_TPüóÉÔ∏è\n",
    "CSAT_TP_TABLE_NAME = \"BCOM.CSAT_TP\"\n",
    "Folder_CSAT_TP = user_credential / r'DataBase//DataRaw//BKN//CSAT//'\n",
    "log_CSAT_TP_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//CSAT_TP_log.xlsx'\n",
    "CSAT_TP_schema = ['FileName', 'ModifiedDate', 'Sort by Dimension', 'Survey Id', 'Reservation', 'Team', 'Channel', 'Staff', 'Type', 'Date',\n",
    "                  'Topic of the first Ticket', 'Language', 'Csat 2.0 Score', 'Has Comment', '\"Comment\"', 'Reservation Link', 'View comment',\n",
    "                  'Sort by Dimension (copy)', 'Max. Sort by Dimension']\n",
    "# 2Ô∏è‚É£4Ô∏è‚É£[BKN]CSAT_RSüóÉÔ∏è\n",
    "CSAT_RS_TABLE_NAME = \"BCOM.CSAT_RS\"\n",
    "Folder_CSAT_RS = user_credential / r'DataBase//DataRaw//BKN//CSAT_RESO//'\n",
    "log_CSAT_RS_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//CSAT_RS_log.xlsx'\n",
    "CSAT_RS_schema = ['FileName', 'ModifiedDate', 'Sort by Dimension', 'Survey Id', 'Reservation', 'Team', 'Channel', 'Staff', 'Type', 'Date',\n",
    "                  'Topic of the first Ticket', 'Language', 'Csat 2.0 Score', 'Has Comment', '\"Comment\"', 'Reservation Link', 'View comment',\n",
    "                  'Sort by Dimension (copy)', 'Max. Sort by Dimension']\n",
    "# 2Ô∏è‚É£5Ô∏è‚É£[BKN]PSATüóÉÔ∏è\n",
    "PSAT_TABLE_NAME = \"BCOM.PSAT\"\n",
    "Folder_PSAT = user_credential / r'DataBase//DataRaw//BKN//PSAT//'\n",
    "log_PSAT_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//PSAT_log.xlsx'\n",
    "PSAT_schema = ['FileName', 'ModifiedDate', 'Sorted By Dimension', 'Survey Id', 'Date', 'Staff Name', 'Language', 'Final Topics',\n",
    "               'How satisfied were you with our service?', 'How difficult did we make it or you to solve your issue?', 'Agent understood my question',\n",
    "               'Agent did everything possible to help me', 'Did we fully resolve your issue?', 'Channel', 'Hotel Id', '\"Comment\"',\n",
    "               'Has Comment', 'Sorted BY Dimension (copy)']\n",
    "# 2Ô∏è‚É£6Ô∏è‚É£[BKN]IEX_HrsüóÉÔ∏è\n",
    "IEX_Hrs_TABLE_NAME = \"BCOM.IEX_Hrs\"\n",
    "Folder_IEX_Hrs = user_credential / r'DataBase//DataRaw//BKN//WP_INTERVAL//'\n",
    "log_IEX_Hrs_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//IEX_Hrs_log.xlsx'\n",
    "IEX_Hrs_schema = ['FileName', 'ModifiedDate', 'LOB', 'VNT', 'CET', 'HC', 'Hour']\n",
    "# 2Ô∏è‚É£7Ô∏è‚É£[BKN]IntervalReqüóÉÔ∏è\n",
    "IntervalReq_TABLE_NAME = \"BCOM.IntervalReq\"\n",
    "Folder_IntervalReq = user_credential / r'DataBase//DataRaw//BKN//INTERVAL_REQUIREMENT//'\n",
    "log_IntervalReq_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//IntervalReq_log.xlsx'\n",
    "IntervalReq_schema = ['FileName', 'ModifiedDate', 'LOB', 'Datetime_CET', 'Datetime_VN', 'Value', 'Delivery_Req']\n",
    "# 2Ô∏è‚É£8Ô∏è‚É£[BKN]ExceptionReqüóÉÔ∏è\n",
    "ExceptionReq_TABLE_NAME = \"BCOM.ExceptionReq\"\n",
    "Folder_ExceptionReq = user_credential / r'DataBase//DataRaw//BKN//EXCEPTION_REQ//'\n",
    "log_ExceptionReq_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//ExceptionReq_log.xlsx'\n",
    "ExceptionReq_schema = ['FileName', 'ModifiedDate', 'Emp ID', 'Date (MM/DD/YYYY)', 'Exception request (Minute)', 'Reason', 'TL', 'OM']\n",
    "# 2Ô∏è‚É£9Ô∏è‚É£[BKN]LTTransfersüóÉÔ∏è\n",
    "LTTransfers_TABLE_NAME = \"BCOM.LTTransfers\"\n",
    "Folder_LTTransfers = user_credential / r'DataBase//DataRaw//BKN//HC_TRANSFER//'\n",
    "log_LTTransfers_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//LTTransfers_log.xlsx'\n",
    "LTTransfers_schema = ['FileName', 'ModifiedDate', 'EID', 'Full Name', 'Employee Status', 'LWD', 'Remarks']\n",
    "# 3Ô∏è‚É£0Ô∏è‚É£[BKN]DailyReqüóÉÔ∏è\n",
    "DailyReq_TABLE_NAME = \"BCOM.DailyReq\"\n",
    "Folder_DailyReq = user_credential / r'DataBase//DataRaw//BKN//REQUIREMENT_HOURS//'\n",
    "log_DailyReq_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//DailyReq_log.xlsx'\n",
    "DailyReq_schema = ['FileName', 'ModifiedDate', 'LOB', 'Date', 'Daily Requirement', 'Prod Requirement']\n",
    "# 3Ô∏è‚É£1Ô∏è‚É£[BKN]ProjectedShrinküóÉÔ∏è\n",
    "ProjectedShrink_TABLE_NAME = \"BCOM.ProjectedShrink\"\n",
    "Folder_ProjectedShrink = user_credential / r'DataBase//DataRaw//BKN//SHRINKAGE_TARGET//'\n",
    "log_ProjectedShrink_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//ProjectedShrink_log.xlsx'\n",
    "ProjectedShrink_schema = ['FileName', 'ModifiedDate', 'LOB', 'Week', 'Ratio']\n",
    "# 3Ô∏è‚É£2Ô∏è‚É£[BKN]OTReqüóÉÔ∏è\n",
    "OTReq_TABLE_NAME = \"BCOM.OTReq\"\n",
    "Folder_OTReq = user_credential / r'DataBase//DataRaw//BKN//OT_REQ//'\n",
    "log_OTReq_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//OTReq_log.xlsx'\n",
    "OTReq_schema = ['FileName', 'ModifiedDate', 'Date', 'LOB', 'OT Hour', 'Type']\n",
    "# 3Ô∏è‚É£3Ô∏è‚É£[BKN]CapHCüóÉÔ∏è\n",
    "CapHC_TABLE_NAME = \"BCOM.CapHC\"\n",
    "Folder_CapHC = user_credential / r'DataBase//DataRaw//BKN//CAPACITY_HC//'\n",
    "log_CapHC_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//CapHC_log.xlsx'\n",
    "CapHC_schema = ['FileName', 'ModifiedDate', 'LOB', 'Date', 'Client Requirement (Hours)']\n",
    "# 3Ô∏è‚É£4Ô∏è‚É£[BKN]ProjectedHCüóÉÔ∏è\n",
    "ProjectedHC_TABLE_NAME = \"BCOM.ProjectedHC\"\n",
    "Folder_ProjectedHC = user_credential / r'DataBase//DataRaw//BKN//PROJECTED_HEADCOUNT//'\n",
    "log_ProjectedHC_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//ProjectedHC_log.xlsx'\n",
    "ProjectedHC_schema = ['FileName', 'ModifiedDate', 'Date', 'LOB', 'FTE Required', 'Projected HC', 'Plan Leave', \n",
    "                      'Actual Projected HC', '%OO', '%IO', 'Projected HC with Shrink', 'OT', 'Leave allow for Shrink', '% Deli']\n",
    "# 3Ô∏è‚É£5Ô∏è‚É£[BKN]RampHCüóÉÔ∏è\n",
    "RampHC_TABLE_NAME = \"BCOM.RampHC\"\n",
    "Folder_RampHC = user_credential / r'DataBase//DataRaw//BKN//RAMPUP_HC//'\n",
    "log_RampHC_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//RampHC_log.xlsx'\n",
    "RampHC_schema = ['FileName', 'ModifiedDate', 'Date', 'LOB', 'Headcount', 'Hours']\n",
    "\n",
    "# 3Ô∏è‚É£6Ô∏è‚É£[BKN]SEATüóÉÔ∏è\n",
    "SEAT_TABLE_NAME = \"BCOM.SEAT\"\n",
    "Folder_SEAT = user_credential / r'DataBase//DataFrame//BKN//SEAT_MAP//History data//'\n",
    "log_SEAT_path = user_credential / r'DataBase//DataRaw//BKN//MODIFIED_LOG//SEAT_log.xlsx'\n",
    "SEAT_schema = ['FileName','ModifiedDate','Date','Emp ID','TED Name','Week_day','Seat No','Floor','Building']\n",
    "# -----------------------------------------------------------------------------------------------#\n",
    "# Database_Connecterüß¨\n",
    "\n",
    "server_name = \"PHMANVMDEV01V\"\n",
    "server_ip = \"10.5.11.60\"\n",
    "database = \"wfm_vn_dev\"\n",
    "user = \"usr_wfmvn_dev\"\n",
    "password = \"12guWU2OdEj5kEspl9Rlfoglf\"\n",
    "# SQL Server Authentication üîó\n",
    "connection_string = f\"mssql+pyodbc://{user}:{password}@{server_ip}/{database}?driver=ODBC+Driver+17+for+SQL+Server\"\n",
    "# Windows Authentication üîó\n",
    "# connection_string = f\"mssql+pyodbc://{server_name}/{database}?driver=ODBC+Driver+17+for+SQL+Server&Trusted_Connection=yes\"\n",
    "try:\n",
    "    engine = create_engine(connection_string, fast_executemany=True)\n",
    "    logger.info(f\"‚úÖ Successfully connected to DB: {database} server: {server_ip}\")\n",
    "except Exception as e:\n",
    "    logger.exception(\"‚ùå DB Connection error\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4f65a4-12c4-4f22-8c07-ff4d18313904",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function Definitionüõ†Ô∏è\n",
    "\n",
    "# Log Color viewüí°\n",
    "def print_colored(text, color):\n",
    "    display(HTML(f'<span style=\"color: {color};\">{text}</span>'))\n",
    "\n",
    "# Check existing log fileüí°\n",
    "def read_or_create_log(log_path):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore') # Ignor WarningüìÉ\n",
    "        try:\n",
    "            logger.debug(f\"Reading log file: {log_path}\")\n",
    "            log_df = pl.read_excel(log_path)\n",
    "            log_df = log_df.with_columns([pl.col(\"ModifiedDate\").dt.cast_time_unit(\"ms\")], strict=False)\n",
    "            logger.info(f\"Success read log file: {log_path}\")\n",
    "        except FileNotFoundError: # Create new log if can't find logüìÉ\n",
    "            logger.warning(f\"Log file not found: {log_path}. Create new log.\")\n",
    "            log_df = pl.DataFrame(\n",
    "                {\n",
    "                    \"FileName\": pl.Series([], dtype=pl.Utf8),\n",
    "                    \"ModifiedDate\": pl.Series([], dtype=pl.Datetime),\n",
    "                    \"Error\": pl.Series([], dtype=pl.Utf8),})\n",
    "        except Exception as e: # Create new log if can't open logüìÉ\n",
    "            logger.exception(f\"Error reading log file: {log_path}\")\n",
    "            print(f\"Error reading log file: {e}\")\n",
    "            log_df = pl.DataFrame(\n",
    "                {\n",
    "                    \"FileName\": pl.Series([], dtype=pl.Utf8),\n",
    "                    \"ModifiedDate\": pl.Series([], dtype=pl.Datetime),\n",
    "                    \"Error\": pl.Series([], dtype=pl.Utf8),})\n",
    "        return log_df\n",
    "        \n",
    "# Update log_dfüí°\n",
    "def process_and_save_log(log_df, log_entries, log_path):\n",
    "    if log_entries:\n",
    "        new_log_df = pl.DataFrame(log_entries)\n",
    "        log_df = log_df.with_columns(pl.col('ModifiedDate').dt.cast_time_unit(\"ms\"))\n",
    "        log_df = (pl.concat([log_df, new_log_df], how=\"diagonal_relaxed\") # Combine and remove duplicate New_Log and Old_LogüìÉ\n",
    "                  .sort(\"ModifiedDate\", descending=[False])\n",
    "                  .unique(subset=[\"FileName\"], keep=\"last\")\n",
    "                  .sort(\"FileName\", descending=[False])\n",
    "                  .select([\"FileName\", \"ModifiedDate\", \"Error\"]))\n",
    "        try:\n",
    "            log_df.write_excel(log_path, worksheet=\"ImportLog\", autofit=True)\n",
    "            print(f\"Import log saved to: {log_path}\")\n",
    "            logger.info(f\"Import log saved to: {log_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error writing log file: {e}\")\n",
    "            logger.error(f\"Error writing log file: {log_path} - {e}\")\n",
    "\n",
    "# write_dataüí°\n",
    "def write_data(engine, table_name, df): # write to databaseüìÉ\n",
    "     df.write_database(table_name=table_name, connection=engine, if_table_exists=\"append\")\n",
    "    \n",
    "# delete_dataüí°\n",
    "def delete_data(engine, table_name, filename):\n",
    "    try:\n",
    "        with engine.connect() as connection:\n",
    "            print_colored(f\"Prepare to delete old data for '{filename}' in '{table_name}'\", \"DarkTurquoise\")\n",
    "            logger.warning(f\"Prepare to delete old data for '{filename}' in '{table_name}'\")\n",
    "            delete_query = text(f\"DELETE FROM {table_name} WHERE [FileName] = :filename\")\n",
    "            connection.execute(delete_query, {\"filename\": filename})\n",
    "            connection.commit()\n",
    "            print_colored(f\"Old data deleted successfullyüßπ\", \"DarkTurquoise\")\n",
    "            logger.info(f\"'{filename}' data deleted successfully in '{table_name}' üßπ.\")\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Error while delete data for '{filename}' in '{table_name}'\")\n",
    "        print_colored(f\"Error while delete data for '{filename}' in '{table_name}'\", \"DarkTurquoise\")\n",
    "        raise \n",
    "        \n",
    "# Check Timeüí°\n",
    "def is_time_between(begin_time, end_time, check_time=None):\n",
    "    check_time = check_time or datetime.utcnow().time() # If check time is not given, default to current UTC timeüìÉ\n",
    "    if begin_time < end_time:\n",
    "        return check_time >= begin_time and check_time <= end_time\n",
    "    else: # crosses midnightüìÉ\n",
    "        return check_time >= begin_time or check_time <= end_time\n",
    "def time_difference(time1, time2):\n",
    "    seconds1 = time1.hour * 3600 + time1.minute * 60 + time1.second # Convert times to secondsüìÉ\n",
    "    seconds2 = time2.hour * 3600 + time2.minute * 60 + time2.second\n",
    "    diff_seconds = seconds1 - seconds2\n",
    "    return diff_seconds\n",
    "\n",
    "# Final Summaryüí°\n",
    "def display_summary(source_name: str, error_count: int) -> None:\n",
    "    \"\"\"Final Notice.\"\"\"\n",
    "    if error_count > 0:\n",
    "        print_colored(f\"Finished processing all files ({error_count} have errorsüõ†Ô∏è).\", \"OrangeRed\")\n",
    "        logger.warning(f\"Finished processing all files ({error_count} have errorsüõ†Ô∏è).\")\n",
    "    else:\n",
    "        print_colored(f\"Finished processing all files (no errorsüéâ).\", \"PaleVioletRed\")\n",
    "        logger.info(f\"Finished processing [{source_name}] (no errorsüéâ).\")\n",
    "\n",
    "# Default_variableüí°\n",
    "def Default_variable():\n",
    "    log_entries = []\n",
    "    error_count = 0\n",
    "    return log_entries, error_count\n",
    "\n",
    "# parse_dateüí°\n",
    "def parse_date(col: pl.Expr) -> pl.Expr:\n",
    "    return pl.coalesce(\n",
    "        col.str.strptime(pl.Date, format=\"%m/%d/%Y\", strict=False),\n",
    "        col.str.strptime(pl.Date, format=\"%Y-%m-%d\", strict=False),\n",
    "        col.str.strptime(pl.Date, format=\"%d %B %Y\", strict=False),\n",
    "        col.str.strptime(pl.Date, format=\"%B %d, %Y\", strict=False),\n",
    "        col.str.strptime(pl.Date, format=\"%d-%b-%y\", strict=False),\n",
    "        col.str.strptime(pl.Date, format=\"%Y%m%d\", strict=False),\n",
    "        col.str.strptime(pl.Date, format=\"%d/%m/%y\", strict=False),\n",
    "        col.str.strptime(pl.Date, format=\"%d-%m-%Y\", strict=False),\n",
    "    )\n",
    "\n",
    "# validate_schemaüí°\n",
    "def validate_schema(df: pl.DataFrame, expected_schema: list[str], filename: str) -> tuple[bool, str | None]:\n",
    "    # Start validation\n",
    "    start_msg = f\"üîç Starting schema validation for file: {filename}\"\n",
    "    logger.info(start_msg)\n",
    "    print_colored(start_msg, \"DodgerBlue\")\n",
    "    actual_columns = df.columns\n",
    "    expected_set = set(expected_schema)\n",
    "    actual_set = set(actual_columns)\n",
    "    missing_columns = expected_set - actual_set\n",
    "    extra_columns = actual_set - expected_set\n",
    "    has_critical_error = False\n",
    "    critical_error_message = None\n",
    "    has_warnings = False\n",
    "    # 1. Schema error (Missing columns)\n",
    "    if missing_columns:\n",
    "        has_critical_error = True\n",
    "        critical_error_message = f\"Schema error in the file: '{filename}'. Missing columns: {sorted(list(missing_columns))}\"\n",
    "        logger.error(critical_error_message)\n",
    "        print_colored(f\"‚ùóÔ∏è {critical_error_message}\", \"OrangeRed\")\n",
    "    # 2. warning extra columns\n",
    "    if extra_columns:\n",
    "        has_warnings = True\n",
    "        warning_message = f\"warning schema for file '{filename}'. Extra columns: {sorted(list(extra_columns))}. These columns will be excluded from the import process.\"\n",
    "        logger.warning(warning_message)\n",
    "        print_colored(f\"‚ö†Ô∏è {warning_message}\", \"Gold\")\n",
    "    # 3. Final results announcement\n",
    "    if not has_critical_error and not has_warnings:\n",
    "        final_msg = f\"‚úÖ Completely valid schema for the file: {filename}.\"\n",
    "        logger.info(final_msg)\n",
    "        print_colored(final_msg, \"MediumSeaGreen\")\n",
    "    elif not has_critical_error and has_warnings:\n",
    "        final_msg = f\"‚ö†Ô∏è File schema check: {filename} Passed (No missing columns, extra columns warned)\"\n",
    "        logger.info(final_msg)\n",
    "        print_colored(final_msg, \"MediumSeaGreen\") # V·∫´n d√πng m√†u xanh l√°\n",
    "    elif has_critical_error:\n",
    "        final_msg = f\"‚ùå Schema validation failed due to missing column(s) for file: {filename}.\"\n",
    "        logger.warning(final_msg) # Log ·ªü m·ª©c warning ho·∫∑c error t√πy √Ω\n",
    "        print_colored(final_msg, \"OrangeRed\")\n",
    "    return has_critical_error, critical_error_message\n",
    "    \n",
    "# DF Infoüí°\n",
    "def info_polars(df: pl.DataFrame):\n",
    "    print_colored(f\"‚öôÔ∏èFinal structure\", \"Olive\")\n",
    "    logger.info(f\"‚öôÔ∏èFinal structure\")\n",
    "    shape = df.shape\n",
    "    print(f\"Shape: {shape}\")\n",
    "    print(\"Data columns:\")  \n",
    "    table_data = []\n",
    "    for i, name in enumerate(df.columns):\n",
    "        dtype = df.dtypes[i]\n",
    "        non_null_count = df.select(pl.col(name).is_not_null().sum()).item()\n",
    "        table_data.append([i, name, non_null_count, dtype])  \n",
    "    headers = [\"#\", \"Column\", \"Non-Null Count\", \"Dtype\"]\n",
    "    print(tabulate(table_data, headers=headers, tablefmt=\"grid\"))\n",
    "    logger.info(tabulate(table_data, headers=headers, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d50864-3a80-4ea8-b858-1dfb2761f544",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# MaintainDatabaseüß∞\n",
    "\n",
    "print_colored(\"===== Starting Index Rebuild Process =====\", \"DodgerBlue\")\n",
    "\n",
    "MaintainDatabase_sql = \"\"\"\n",
    "\n",
    "EXEC BCOM.usp_MaintainDatabase\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    with engine.connect() as connection:\n",
    "        print_colored(\"‚öôÔ∏èExecuting Index Rebuild script (this may take a long time)...\", \"DarkOrange\")\n",
    "        connection.execute(text(MaintainDatabase_sql))\n",
    "        connection.commit() \n",
    "        print_colored(\"‚úîÔ∏èIndex Rebuild script execution command sent and committed. Check SQL Server logs/output for details.\", \"MediumSeaGreen\")\n",
    "except sa.exc.SQLAlchemyError as e_db:\n",
    "    print_colored(f\"‚ùå Database error during Index Rebuild Process: {e_db}\", \"OrangeRed\")\n",
    "except Exception as e_general:\n",
    "    print_colored(f\"‚ùå An unexpected error occurred during Index Rebuild Process: {e_general}\", \"OrangeRed\")\n",
    "\n",
    "print_colored(\"===== Index Rebuild Process attempt is complete (Python perspective) =====\", \"DodgerBlue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d169c9-b768-4c75-aab7-78c43b36508f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ‚öôÔ∏èCreate_EEAAO\n",
    "logger.info(\"===== Starting EEAAO Process =====\")\n",
    "# EXEC EEAAO Procedure\n",
    "Exec_EEAAO = \"\"\"\n",
    "EXEC BCOM.Refresh_EEAAO_Data;\n",
    "\"\"\"\n",
    "select_query = \"\"\"\n",
    "SELECT TOP 5 * FROM BCOM.EEAAO;\n",
    "\"\"\"\n",
    "try:\n",
    "    with engine.connect() as connection:\n",
    "        logger.info(\"‚öôÔ∏èExecuting procedure EEAAO ...\")\n",
    "        print(\"‚öôÔ∏èExecuting procedure EEAAO ...\")\n",
    "        connection.execute(text(Exec_EEAAO))\n",
    "        connection.commit()\n",
    "        logger.info(\"‚úîÔ∏èSuccessfully executed and committed Procedure EEAAO.\")\n",
    "        print(\"‚úîÔ∏èSuccessfully executed and committed Procedure EEAAO.\")\n",
    "        logger.info(f\"Reading data from BCOM.EEAAO with query: {select_query.strip()}\")\n",
    "        print(f\"Reading data from BCOM.EEAAO with query: {select_query.strip()}\")\n",
    "        df_eeao_result = pl.read_database(query=select_query, connection=connection)\n",
    "        if df_eeao_result is not None and not df_eeao_result.is_empty():\n",
    "            print_colored(\"Sample data from BCOM.EEAAO after refresh:\", \"MediumSeaGreen\")\n",
    "            display(df_eeao_result)\n",
    "            logger.info(f\"Successfully read {df_eeao_result.shape[0]} rows from BCOM.EEAAO.\")\n",
    "        else:\n",
    "            logger.warning(\"No data returned from BCOM.EEAAO after refresh or procedure did not complete in time.\")\n",
    "            print_colored(\"No data returned from BCOM.EEAAO after refresh.\", \"OrangeRed\")\n",
    "except sa.exc.SQLAlchemyError as e:\n",
    "    logger.error(f\"Database error during EEAAO Process: {e}\", exc_info=True)\n",
    "    print(f\"Database error: {e}\") \n",
    "except Exception as e:\n",
    "    logger.error(f\"An unexpected error occurred during EEAAO Process: {e}\", exc_info=True)\n",
    "    print(f\"An unexpected error: {e}\")\n",
    "logger.info(\"===== Processing of EEAAO is complete =====\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229fc7cb-c8af-4e7f-980d-f0c272d71043",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Close DBüìÉ\n",
    "engine.dispose()\n",
    "print(\"Database connection closed.\")\n",
    "%reset -f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
