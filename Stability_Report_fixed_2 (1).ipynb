{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#libraries\n",
    "import os, glob, re\n",
    "from datetime import datetime as dt, time as t, date as d\n",
    "import polars as pl\n",
    "from sqlalchemy import create_engine, text\n",
    "import statistics\n",
    "a=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Data code 1\n",
    "# Táº¡o engine káº¿t ná»‘i\n",
    "SERVER_NAME = \"PHMANVMDEV01V\"\n",
    "DATABASE = \"wfm_vn_dev\"\n",
    "connection_string = f\"mssql+pyodbc://{SERVER_NAME}/{DATABASE}?driver=ODBC+Driver+17+for+SQL+Server&Trusted_Connection=yes\"\n",
    "engine = create_engine(connection_string)\n",
    " \n",
    "# CÃ¢u lá»‡nh SQL\n",
    "sql_query = \"\"\"\n",
    " \n",
    " \n",
    "WITH CombinedData AS (\n",
    "-- 01/ BCOM.AHT \n",
    "    SELECT 'BKN' AS [SCHEMA], 'AHT' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM BCOM.AHT\n",
    "    UNION ALL\n",
    "-- 02/ BCOM.CapHC \n",
    "    SELECT 'BKN' AS [SCHEMA], 'CapHC' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM BCOM.CapHC\n",
    "    UNION ALL\n",
    "-- 03/ BCOM.Contrack\n",
    "    SELECT 'BKN' AS [SCHEMA], 'Contrack' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM BCOM.Contrack\n",
    "    UNION ALL\n",
    "-- 04/ BCOM.CPI\n",
    "    SELECT 'BKN' AS [SCHEMA], 'CPI' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM BCOM.CPI\n",
    "    UNION ALL\n",
    "-- 05/ BCOM.CPI_PEGA\n",
    "    SELECT 'BKN' AS [SCHEMA], 'CPI_PEGA' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM BCOM.CPI_PEGA\n",
    "    UNION ALL\n",
    "-- 06/ BCOM.CSAT_RS\n",
    "    SELECT 'BKN' AS [SCHEMA], 'CSAT_RS' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM BCOM.CSAT_RS\n",
    "    UNION ALL\n",
    "-- 07/ BCOM.CSAT_TP\n",
    "    SELECT 'BKN' AS [SCHEMA], 'CSAT_TP' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM BCOM.CSAT_TP\n",
    "    UNION ALL\n",
    "-- 08/ BCOM.CUIC\n",
    "    SELECT 'BKN' AS [SCHEMA], 'CUIC' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM BCOM.CUIC\n",
    "    UNION ALL\n",
    "-- 09/ BCOM.CUIC_RTMonitor\n",
    "    SELECT 'BKN' AS [SCHEMA], 'CUIC_RTMonitor' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM BCOM.CUIC_RTMonitor\n",
    "    UNION ALL\n",
    "-- 10/ BCOM.DailyReq\n",
    "    SELECT 'BKN' AS [SCHEMA], 'DailyReq' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM BCOM.DailyReq\n",
    "    UNION ALL\n",
    "-- 11/ BCOM.EPS\n",
    "    SELECT 'BKN' AS [SCHEMA], 'EPS' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM BCOM.EPS\n",
    "    UNION ALL\n",
    "-- 12/ BCOM.ExceptionReq\n",
    "    SELECT 'BKN' AS [SCHEMA], 'ExceptionReq' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM BCOM.ExceptionReq\n",
    "    UNION ALL\n",
    "-- 13/ BCOM.IEX_Hrs\n",
    "    SELECT 'BKN' AS [SCHEMA], 'IEX_Hrs' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM BCOM.IEX_Hrs\n",
    "    UNION ALL\n",
    "-- 14/ BCOM.IntervalReq\n",
    "    SELECT 'BKN' AS [SCHEMA], 'IntervalReq' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM BCOM.IntervalReq\n",
    "    UNION ALL\n",
    "-- 15/ BCOM.KPI_Target\n",
    "    SELECT 'BKN' AS [SCHEMA], 'KPI_Target' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM BCOM.KPI_Target\n",
    "    UNION ALL\n",
    "-- 16/ BCOM.LogoutCount\n",
    "    SELECT 'BKN' AS [SCHEMA], 'LogoutCount' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM BCOM.LogoutCount\n",
    "    UNION ALL\n",
    "-- 17/ BCOM.LTTransfers\n",
    "    SELECT 'BKN' AS [SCHEMA], 'LTTransfers' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM BCOM.LTTransfers\n",
    "    UNION ALL\n",
    "-- 18/ BCOM.OTReq\n",
    "    SELECT 'BKN' AS [SCHEMA], 'OTReq' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM BCOM.OTReq\n",
    "    UNION ALL\n",
    "-- 19/ BCOM.ProjectedHC\n",
    "    SELECT 'BKN' AS [SCHEMA], 'ProjectedHC' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM BCOM.ProjectedHC\n",
    "    UNION ALL\n",
    "-- 20/ BCOM.ProjectedShrink\n",
    "    SELECT 'BKN' AS [SCHEMA], 'ProjectedShrink' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM BCOM.ProjectedShrink\n",
    "    UNION ALL\n",
    "-- 21/ BCOM.PSAT\n",
    "    SELECT 'BKN' AS [SCHEMA], 'PSAT' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM BCOM.PSAT\n",
    "    UNION ALL\n",
    "-- 22/ BCOM.Quality\n",
    "    SELECT 'BKN' AS [SCHEMA], 'Quality' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM BCOM.Quality\n",
    "    UNION ALL\n",
    "-- 23/ BCOM.RegisteredOT\n",
    "    SELECT 'BKN' AS [SCHEMA], 'RegisteredOT' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM BCOM.RegisteredOT\n",
    "    UNION ALL\n",
    "-- 24/ BCOM.RONA\n",
    "    SELECT 'BKN' AS [SCHEMA], 'RONA' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM BCOM.RONA\n",
    "    UNION ALL\n",
    "-- 25/ BCOM.ROSTER\n",
    "    SELECT 'BKN' AS [SCHEMA], 'ROSTER' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM BCOM.ROSTER\n",
    "    UNION ALL\n",
    "-- 26/ BCOM.Staff\n",
    "    SELECT 'BKN' AS [SCHEMA], 'Staff' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM BCOM.Staff\n",
    "    UNION ALL\n",
    "-- 27/ BCOM.WpDetail\n",
    "    SELECT 'BKN' AS [SCHEMA], 'WpDetail' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM BCOM.WpDetail\n",
    "    UNION ALL\n",
    "-- 28/ BCOM.WpSummary\n",
    "    SELECT 'BKN' AS [SCHEMA], 'WpSummary' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM BCOM.WpSummary\n",
    "    UNION ALL\n",
    "-- 29/ GLB.EmpMaster\n",
    "    SELECT 'GLB' AS [SCHEMA], 'EmpMaster' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM GLB.EmpMaster\n",
    "    UNION ALL\n",
    "-- 30/ GLB.NormHdays\n",
    "    SELECT 'GLB' AS [SCHEMA], 'NormHdays' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM GLB.NormHdays\n",
    "    UNION ALL\n",
    "-- 31/ GLB.OT_RAMCO\n",
    "    SELECT 'GLB' AS [SCHEMA], 'OT_RAMCO' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM GLB.OT_RAMCO\n",
    "    UNION ALL\n",
    "-- 32/ GLB.PremHdays\n",
    "    SELECT 'GLB' AS [SCHEMA], 'PremHdays' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM GLB.PremHdays\n",
    "    UNION ALL\n",
    "-- 33/ GLB.RAMCO\n",
    "    SELECT 'GLB' AS [SCHEMA], 'RAMCO' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM GLB.RAMCO\n",
    "    UNION ALL\n",
    "-- 34/ GLB.Resignation\n",
    "    SELECT 'GLB' AS [SCHEMA], 'Resignation' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM GLB.Resignation\n",
    "    UNION ALL\n",
    "-- 35/ GLB.Termination\n",
    "    SELECT 'GLB' AS [SCHEMA], 'Termination' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM GLB.Termination\n",
    "),\n",
    "FileNameCheck AS (\n",
    "    SELECT [SCHEMA],[FOLDER NAME],[FileName] + ' - ' + CONVERT(VARCHAR, [ModifiedDate], 120) AS [FileName - ModifiedDate],COUNT(*) \n",
    "\tAS [ROW_NUMBER]\n",
    "    FROM CombinedData\n",
    "    GROUP BY [SCHEMA], [FOLDER NAME], [FileName] + ' - ' + CONVERT(VARCHAR, [ModifiedDate], 120)\n",
    "    HAVING COUNT(*) > 1\n",
    ")\n",
    "SELECT * FROM FileNameCheck\n",
    "ORDER BY [SCHEMA] ASC, [FOLDER NAME] DESC, [FileName - ModifiedDate] ASC\n",
    " \n",
    "\"\"\"\n",
    "\n",
    "# Äá»c dá»¯ liá»‡u vÃ o DataFrame\n",
    "Code1_Result = pl.read_database(query=sql_query, connection=engine)\n",
    "engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Data code 2\n",
    "engine = create_engine(connection_string)\n",
    " \n",
    "# CÃ¢u lá»‡nh SQL\n",
    "\n",
    "sql_query2 = \"\"\"\n",
    "with\n",
    "-- 1. Agent Raw\n",
    "Agents_Raw1 as\n",
    "(Select Employee_ID, COUNT(*) as [Count]\n",
    "From BCOM.Staff\n",
    "Group by Employee_ID Having COUNT(*)>1),\n",
    "-- 2. AHT Raw ðŸ“\n",
    "AHT_Raw1 as\n",
    "(Select  [Staff]+cast([Date] AS varchar)+[Language]+[Topic]+[Subtopic]+cast([Handling Time] as varchar)+[Tooltip Phone Time]+[First Reservation Id] as [Concat], COUNT(*) as [Count]\n",
    "From     BCOM.AHT\n",
    "Where    [Staff]+cast([Date] AS varchar)+[Language]+[Topic]+[Subtopic]+cast([Handling Time] as varchar)+[Tooltip Phone Time]+[First Reservation Id] Is not Null\n",
    "Group by [Staff]+cast([Date] AS varchar)+[Language]+[Topic]+[Subtopic]+cast([Handling Time] as varchar)+[Tooltip Phone Time]+[First Reservation Id] Having COUNT(*)>1),\n",
    "-- 3. Capacity HC ðŸ“\n",
    "CapacityHC_Raw1 as\n",
    "(Select  [LOB]+cast([Date] as varchar) as [Concat], COUNT(*) as [Count]\n",
    "From     BCOM.CapHC\n",
    "Where    [LOB]+cast([Date] as varchar) is not Null\n",
    "Group by [LOB]+cast([Date] as varchar) Having COUNT(*)>1),\n",
    "-- 4. CSAT Raw ðŸ“\n",
    "CSAT_Raw1 as\n",
    "(Select  cast([Sort by Dimension] as varchar)+[Staff]+[Type]+[Team]+[Survey Id]+[Reservation]+[Channel]+[Topic of the first Ticket]+[Language]+[Csat 2.0 Score] as [Concat], COUNT(*) as [Count]\n",
    "From     BCOM.CSAT_TP\n",
    "Where    cast([Sort by Dimension] as varchar)+[Staff]+[Type]+[Team]+[Survey Id]+[Reservation]+[Channel]+[Topic of the first Ticket]+[Language]+[Csat 2.0 Score] is not Null\n",
    "Group by cast([Sort by Dimension] as varchar)+[Staff]+[Type]+[Team]+[Survey Id]+[Reservation]+[Channel]+[Topic of the first Ticket]+[Language]+[Csat 2.0 Score] Having COUNT(*)>1),\n",
    "-- 5. CSAT Reso Raw ðŸ“\n",
    "CSAT_Reso_Raw as\n",
    "(Select  cast([Sort by Dimension] as varchar)+[Staff]+[Type]+[Team]+[Survey Id]+[Reservation]+[Channel]+[Topic of the first Ticket]+[Language]+[Csat 2.0 Score] as [Concat], COUNT(*) as [Count]\n",
    "From     BCOM.CSAT_RS\n",
    "Where    cast([Sort by Dimension] as varchar)+[Staff]+[Type]+[Team]+[Survey Id]+[Reservation]+[Channel]+[Topic of the first Ticket]+[Language]+[Csat 2.0 Score] is not Null\n",
    "Group by cast([Sort by Dimension] as varchar)+[Staff]+[Type]+[Team]+[Survey Id]+[Reservation]+[Channel]+[Topic of the first Ticket]+[Language]+[Csat 2.0 Score] Having COUNT(*)>1),\n",
    "-- 6. CUIC Raw ðŸ“\n",
    "CUIC_Raw1 as\n",
    "(Select [FullName]+[LoginName]+cast([Interval] as varchar)+cast([AgentLoggedOnTime] as varchar)+cast([AgentAvailTime] as varchar) as [Concat], COUNT(*) as [Count]\n",
    "From     BCOM.CUIC\n",
    "Where    [FullName]+[LoginName]+cast([Interval] as varchar)+cast([AgentLoggedOnTime] as varchar)+cast([AgentAvailTime] as varchar) is not Null\n",
    "Group by [FullName]+[LoginName]+cast([Interval] as varchar)+cast([AgentLoggedOnTime] as varchar)+cast([AgentAvailTime] as varchar) Having COUNT(*)>1),\n",
    "-- 7. EEAAO ðŸ“\n",
    "\n",
    "-- 8. EPS Raw ðŸ“\n",
    "EPS_Raw1 as\n",
    "(Select  [Username]+cast([Session Login] as varchar)+cast([Session Logout] as varchar)+[BPE Code]+[Session Time]+cast([Total Time] as varchar) as [Concat], COUNT(*) as [Count]\n",
    "From     BCOM.EPS\n",
    "Where    [Username]+cast([Session Login] as varchar)+cast([Session Logout] as varchar)+[BPE Code]+[Session Time]+cast([Total Time] as varchar) is not Null\n",
    "Group by [Username]+cast([Session Login] as varchar)+cast([Session Logout] as varchar)+[BPE Code]+[Session Time]+cast([Total Time] as varchar) Having COUNT(*)>1),\n",
    "-- 9. Exception Req ðŸ“\n",
    "Exception_Req as\n",
    "(Select [Emp ID]+cast([Date (MM/DD/YYYY)] as varchar) as [Concat], COUNT(*) as [Count]\n",
    "From BCOM.ExceptionReq\n",
    "Where [Emp ID]+cast([Date (MM/DD/YYYY)] as varchar) is not null\n",
    "Group by [Emp ID]+cast([Date (MM/DD/YYYY)] as varchar) Having COUNT(*)>1),\n",
    "-- 10. HC Transfer ðŸ“\n",
    "HC_Transfer as\n",
    "(Select  [EID]+cast([LWD] as varchar) as [Concat], COUNT(*) as [Count]\n",
    "From     BCOM.LTTransfers\n",
    "Where    [EID]+cast([LWD] as varchar) is not Null\n",
    "Group by [EID]+cast([LWD] as varchar) Having COUNT(*)>1),\n",
    "-- 11. Holiday Raw ðŸ“\n",
    "Holiday_Raw1 as\n",
    "(Select cast([Date] as varchar) as [Concat], COUNT(*) as [Count]\n",
    "From GLB.PremHdays\n",
    "Group by cast([Date] as varchar) Having COUNT(*)>1),\n",
    "-- 12. IEX Raw ðŸ“\n",
    "\n",
    "-- 13. IntervalReq_Raw1 ðŸ“\n",
    "IntervalReq_Raw1 as\n",
    "(Select  [LOB]+cast([Datetime_VN] as varchar) as [Concat], COUNT(*) as [Count]\n",
    "From     BCOM.IntervalReq\n",
    "Where    [LOB]+cast([Datetime_VN] as varchar) is not Null\n",
    "Group by [LOB]+cast([Datetime_VN] as varchar) Having COUNT(*)>1),\n",
    "-- 14. KPI Targer (LOB) ðŸ“\n",
    "LOB_Tar as\n",
    "(Select  cast([Week] as varchar)+[LOB]+[Tenure days] as [Concat], COUNT(*) as [Count]\n",
    "From     BCOM.KPI_Target\n",
    "Where    cast([Week] as varchar)+[LOB]+[Tenure days] is not Null\n",
    "Group by cast([Week] as varchar)+[LOB]+[Tenure days] Having COUNT(*)>1),\n",
    "-- 15. KPI Targer (LOB Group) ðŸ“\n",
    "LOBGR_Tar as\n",
    "(Select  cast([Week] as varchar)+[LOB Group]+[Tenure days] as [Concat], COUNT(*) as [Count]\n",
    "From     BCOM.KPI_Target\n",
    "where    cast([Week] as varchar)+[LOB Group]+[Tenure days] is not Null And [LOB] is Null\n",
    "Group by cast([Week] as varchar)+[LOB Group]+[Tenure days] Having COUNT(*)>1),\n",
    "-- 16. Logout Count ðŸ“\n",
    "LOGOUT_COUNT as\n",
    "(Select  [Aggregation]+cast([TimeDimension] as varchar) as [Concat], COUNT(*) as [Count]\n",
    "From     BCOM.LogoutCount\n",
    "Where    [Aggregation]+cast([TimeDimension] as varchar) is not Null\n",
    "Group by [Aggregation]+cast([TimeDimension] as varchar) Having COUNT(*)>1),\n",
    "-- 17. OT Ramco ðŸ“\n",
    "OT_Ramco as\n",
    "(Select  cast([Date] as varchar)+[employee_code]+[OT Type] as [Concat], COUNT(*) as [Count]\n",
    "From     GLB.OT_RAMCO\n",
    "Where    cast([Date] as varchar)+[employee_code]+[OT Type] is not Null\n",
    "Group by cast([Date] as varchar)+[employee_code]+[OT Type] Having COUNT(*)>1),\n",
    "-- 18. OverTime Raw ðŸ“\n",
    "OverTime_Raw1 as\n",
    "(Select  cast([Date] as varchar)+[Emp ID] as [Concat], COUNT(*) as [Count]\n",
    "From     BCOM.RegisteredOT\n",
    "WHere    cast([Date] as varchar)+[Emp ID] is not Null\n",
    "Group by cast([Date] as varchar)+[Emp ID] Having COUNT(*)>1),\n",
    "-- 19. PSAT ðŸ“\n",
    "PSAT_Raw1 as\n",
    "(Select  cast([Date] as varchar)+[Survey Id]+[Has Comment]+[Channel]+[Final Topics] as [Concat], COUNT(*) as [Count]\n",
    "From     BCOM.PSAT\n",
    "Where    cast([Date] as varchar)+[Survey Id]+[Has Comment]+[Channel]+[Final Topics] Is not Null\n",
    "Group by cast([Date] as varchar)+[Survey Id]+[Has Comment]+[Channel]+[Final Topics] Having COUNT(*)>1),\n",
    "-- 20. Quality_Raw1 ðŸ“\n",
    "Quality_Raw1 as\n",
    "(Select  cast([eval_date] as varchar)+[eval_id]+[agent_username]+[final_question_grouping]+[sections]+[template_group]+[csat_satisfied]+[tix_final_subtopic]+cast([score_n] as varchar)+cast([score_question_weight] as varchar)+[eval_language]+[eval_reference]+[csat_language_code]+[tix_final_topic] as [Concat], COUNT(*) as [Count]\n",
    "From     BCOM.Quality\n",
    "Where    cast([eval_date] as varchar)+[eval_id]+[agent_username]+[final_question_grouping]+[sections]+[template_group]+[csat_satisfied]+[tix_final_subtopic]+cast([score_n] as varchar)+cast([score_question_weight] as varchar)+[eval_language]+[eval_reference]+[csat_language_code]+[tix_final_topic] Is not Null\n",
    "Group by cast([eval_date] as varchar)+[eval_id]+[agent_username]+[final_question_grouping]+[sections]+[template_group]+[csat_satisfied]+[tix_final_subtopic]+cast([score_n] as varchar)+cast([score_question_weight] as varchar)+[eval_language]+[eval_reference]+[csat_language_code]+[tix_final_topic] Having COUNT(*)>1),\n",
    "-- 21. Ramco Raw ðŸ“\n",
    "Ramco_Raw1 as\n",
    "(Select  cast([Date] as varchar)+[EID] as [Concat], COUNT(*) as [Count]\n",
    "From     GLB.RAMCO\n",
    "Where    cast([Date] as varchar)+[EID] is not Null\n",
    "Group by cast([Date] as varchar)+[EID] Having COUNT(*)>1),\n",
    "-- 22. RamUp HC ðŸ“\n",
    "\n",
    "-- 23. Requirement Hours ðŸ“\n",
    "Requirement_Hours as\n",
    "(Select  [LOB]+cast([Date] as varchar) as [Concat], COUNT(*) as [Count]\n",
    "From     BCOM.DailyReq\n",
    "Where    [LOB]+cast([Date] as varchar) is not Null\n",
    "Group by [LOB]+cast([Date] as varchar) Having COUNT(*)>1),\n",
    "-- 24. Resignation Dump ðŸ“\n",
    "Resignation_Dump as\n",
    "(Select [Employee ID], COUNT(*) as [Count]\n",
    "From GLB.Resignation\n",
    "Group by [Employee ID] Having COUNT(*)>1),\n",
    "-- 25. RONA ðŸ“\n",
    "RONA_Raw1 as\n",
    "(Select  [Agent]+cast([RONA] as varchar)+cast([DateTime] as varchar) as [Concat], COUNT(*) as [Count]\n",
    "From     BCOM.RONA\n",
    "where    [Agent]+cast([RONA] as varchar)+cast([DateTime] as varchar) is not null\n",
    "Group by [Agent]+cast([RONA] as varchar)+cast([DateTime] as varchar) Having COUNT(*)>1),\n",
    "-- 26. Roster Raw ðŸ“\n",
    "Roster_Raw as\n",
    "(Select [Emp ID]+cast([Attribute] as varchar) as [Concat], COUNT(*) as [Count]\n",
    "From BCOM.ROSTER\n",
    "Where [Emp ID]+cast([Attribute] as varchar) is not Null\n",
    "Group by [Emp ID]+cast([Attribute] as varchar) Having COUNT(*)>1),\n",
    "-- 27. SC Labels ðŸ“\n",
    "\n",
    "-- 28. Shrinkage Target ðŸ“\n",
    "Shrinkage_Target as\n",
    "(Select  [LOB]+cast([Week] as varchar) as [Concat], COUNT(*) as [Count]\n",
    "From     BCOM.ProjectedShrink\n",
    "Where    [LOB]+cast([Week] as varchar) is not Null\n",
    "Group by [LOB]+cast([Week] as varchar) Having COUNT(*)>1),\n",
    "-- 29. Termination Dump ðŸ“\n",
    "Termination_Dump as\n",
    "(Select [EMPLOYEE_ID]+cast([Termination Date] as varchar) as [Concat], COUNT(*) as [Count]\n",
    "From GLB.Termination\n",
    "Where [EMPLOYEE_ID]+cast([Termination Date] as varchar) is not Null\n",
    "Group by [EMPLOYEE_ID]+cast([Termination Date] as varchar) Having COUNT(*)>1),\n",
    "-- 30. Ticket Raw ðŸ“\n",
    "Ticket_Raw1 as\n",
    "(Select  cast([Date] as varchar)+[Staff Name]+cast([Hour Interval Selected] as varchar)+[Channel]+[Item Label]+[Item ID]+['Item ID']+[Time Alert]+cast([Nr. Contacts] as varchar)+[Item Link]+[Time] as [Concat], COUNT(*) as [Count]\n",
    "From     BCOM.CPI\n",
    "Where    cast([Date] as varchar)+[Staff Name]+cast([Hour Interval Selected] as varchar)+[Channel]+[Item Label]+[Item ID]+['Item ID']+[Time Alert]+cast([Nr. Contacts] as varchar)+[Item Link]+[Time] is not Null\n",
    "Group by cast([Date] as varchar)+[Staff Name]+cast([Hour Interval Selected] as varchar)+[Channel]+[Item Label]+[Item ID]+['Item ID']+[Time Alert]+cast([Nr. Contacts] as varchar)+[Item Link]+[Time] Having COUNT(*)>1),\n",
    "-- 31. Workplan Raw ðŸ“\n",
    "Workplan_Raw1 as\n",
    "(Select  [LOB]+[ID]+cast([DateTime_Act_Start] as varchar)+cast([DateTime_Act_End] as varchar)+cast([Act_Dur] as varchar)+[Action] as [Concat], COUNT(*) as [Count]\n",
    "From     BCOM.WpDetail\n",
    "Where    [LOB]+[ID]+cast([DateTime_Act_Start] as varchar)+cast([DateTime_Act_End] as varchar)+cast([Act_Dur] as varchar)+[Action] is not Null\n",
    "Group by [LOB]+[ID]+cast([DateTime_Act_Start] as varchar)+cast([DateTime_Act_End] as varchar)+cast([Act_Dur] as varchar)+[Action] Having COUNT(*)>1),\n",
    "-- 32. Workplan Summary Raw ðŸ“\n",
    "Workplan_Summary as\n",
    "(Select  cast([Date] as varchar)+[LOB]+[Agent ID]+[Agent Name]+[Scheduled Activity]+cast([Length] as varchar)+cast([Percent] as varchar) as [Concat], COUNT(*) as [Count]\n",
    "From     BCOM.WpSummary\n",
    "Where    cast([Date] as varchar)+[LOB]+[Agent ID]+[Agent Name]+[Scheduled Activity]+cast([Length] as varchar)+cast([Percent] as varchar) is not Null\n",
    "Group by cast([Date] as varchar)+[LOB]+[Agent ID]+[Agent Name]+[Scheduled Activity]+cast([Length] as varchar)+cast([Percent] as varchar) Having COUNT(*)>1),\n",
    "-- 33. CSAT PEGA ðŸ“\n",
    "\n",
    "-- 34. IPH PEGA ðŸ“\n",
    "IPH_PEGA as\n",
    "(Select  cast([Day of Date] as varchar)+[Staff Name]+[Operator Def]+[Service Case Type New]+[Channel Def]+[Reason For No Service Case]+[Topic Def New]+[Subtopics]+[Case Id]+[Reservation Id Def]+cast([# Swivels] as varchar)+cast([Count of ServiceCase or Interaction] as varchar) as [Concat], COUNT(*) as [Count]\n",
    "From     BCOM.CPI_PEGA\n",
    "Where    cast([Day of Date] as varchar)+[Staff Name]+[Operator Def]+[Service Case Type New]+[Channel Def]+[Reason For No Service Case]+[Topic Def New]+[Subtopics]+[Case Id]+[Reservation Id Def]+cast([# Swivels] as varchar)+cast([Count of ServiceCase or Interaction] as varchar)  is not Null\n",
    "Group by cast([Day of Date] as varchar)+[Staff Name]+[Operator Def]+[Service Case Type New]+[Channel Def]+[Reason For No Service Case]+[Topic Def New]+[Subtopics]+[Case Id]+[Reservation Id Def]+cast([# Swivels] as varchar)+cast([Count of ServiceCase or Interaction] as varchar)  Having COUNT(*)>1),\n",
    "-- 35. Agents Raw(TEDNAME) ðŸ“\n",
    "TEDNAME as\n",
    "(Select [TED Name], COUNT(*) as [Count]\n",
    "From BCOM.Staff\n",
    "Group by [TED Name] Having COUNT(*)>1),\n",
    "-- 36. OTREQ ðŸ“\n",
    "OTREQ as\n",
    "(Select  cast([Date] as varchar)+[LOB]+[Type] as [Concat], COUNT(*) as [Count]\n",
    "From     BCOM.OTReq\n",
    "Where    cast([Date] as varchar)+[LOB]+[Type] is not Null\n",
    "Group by cast([Date] as varchar)+[LOB]+[Type] Having COUNT(*)>1),\n",
    "-- 37. PROHC ðŸ“\n",
    "PROHC as\n",
    "(Select  cast([Date] as varchar)+[LOB] as [Concat], COUNT(*) as [Count]\n",
    "From     BCOM.ProjectedHC\n",
    "Where    cast([Date] as varchar)+[LOB] is not Null\n",
    "Group by cast([Date] as varchar)+[LOB] Having COUNT(*)>1)\n",
    "--\n",
    "--------------------------------------------------------[Nvidia]Prcess---------------------------------------------------------------\n",
    "--\n",
    "------[ðŸ“¥]--(ðŸ‘‰ï¾Ÿãƒ®ï¾Ÿ)ðŸ‘‰                   Agents_Raw1\n",
    "Select    '01' as [No.],    Count(*) as [CheckDup],    'Agents Raw' as [Table]            ,    'IMPORTANT' as [Note]\n",
    "From       Agents_Raw1                    UNION ALL\n",
    "------[ðŸ“¥]--(ðŸ‘‰ï¾Ÿãƒ®ï¾Ÿ)ðŸ‘‰                   AHT_Raw1\n",
    "Select    '02' as [No.],    Count(*) as [CheckDup],    'AHT Raw' as [Table]               ,    '' as [Note]\n",
    "From       AHT_Raw1                       UNION ALL\n",
    "------[ðŸ“¥]--(ðŸ‘‰ï¾Ÿãƒ®ï¾Ÿ)ðŸ‘‰                   CapacityHC_Raw1\n",
    "Select    '03' as [No.],    Count(*) as [CheckDup],    'Capacity HC' as [Table]           ,    '' as [Note]\n",
    "From       CapacityHC_Raw1                UNION ALL\n",
    "------[ðŸ“¥]--(ðŸ‘‰ï¾Ÿãƒ®ï¾Ÿ)ðŸ‘‰                   CSAT_Raw1\n",
    "Select    '04' as [No.],    Count(*) as [CheckDup],    'CSAT Raw' as [Table]              ,    '' as [Note]\n",
    "From       CSAT_Raw1                      UNION ALL\n",
    "------[ðŸ“¥]--(ðŸ‘‰ï¾Ÿãƒ®ï¾Ÿ)ðŸ‘‰                   CSAT_Reso_Raw\n",
    "Select    '05' as [No.],    Count(*) as [CheckDup],    'CSAT Reso Raw' as [Table]         ,    '' as [Note]\n",
    "From       CSAT_Reso_Raw                 UNION ALL\n",
    "------[ðŸ“¥]--(ðŸ‘‰ï¾Ÿãƒ®ï¾Ÿ)ðŸ‘‰                   CUIC_Raw1\n",
    "Select    '06' as [No.],    Count(*) as [CheckDup],    'CUIC Raw' as [Table]              ,    '' as [Note]\n",
    "From       CUIC_Raw1                      UNION ALL\n",
    "------[ðŸ“¥]--(ðŸ‘‰ï¾Ÿãƒ®ï¾Ÿ)ðŸ‘‰                   EPS_Raw1\n",
    "Select    '08' as [No.],    Count(*) as [CheckDup],    'EPS Raw' as [Table]               ,    '' as [Note]\n",
    "From       EPS_Raw1                       UNION ALL\n",
    "------[ðŸ“¥]--(ðŸ‘‰ï¾Ÿãƒ®ï¾Ÿ)ðŸ‘‰                   Exception_Req\n",
    "Select    '09' as [No.],    Count(*) as [CheckDup],    'Exception Req' as [Table]         ,    '' as [Note]\n",
    "From       Exception_Req                 UNION ALL\n",
    "------[ðŸ“¥]--(ðŸ‘‰ï¾Ÿãƒ®ï¾Ÿ)ðŸ‘‰                   HC_Transfer\n",
    "Select    '10' as [No.],    Count(*) as [CheckDup],    'HC Transfer' as [Table]           ,    '' as [Note]\n",
    "From       HC_Transfer                   UNION ALL\n",
    "------[ðŸ“¥]--(ðŸ‘‰ï¾Ÿãƒ®ï¾Ÿ)ðŸ‘‰                   Holiday_Raw1\n",
    "Select    '11' as [No.],    Count(*) as [CheckDup],    'Holiday Raw' as [Table]           ,    '' as [Note]\n",
    "From       Holiday_Raw1                   UNION ALL\n",
    "------[ðŸ“¥]--(ðŸ‘‰ï¾Ÿãƒ®ï¾Ÿ)ðŸ‘‰                   IntervalReq_Raw1\n",
    "Select    '13' as [No.],    Count(*) as [CheckDup],    'IntervalReq' as [Table]       ,    'IMPORTANT' as [Note]\n",
    "From       IntervalReq_Raw1               UNION ALL\n",
    "------[ðŸ“¥]--(ðŸ‘‰ï¾Ÿãƒ®ï¾Ÿ)ðŸ‘‰                   LOB_Tar\n",
    "Select    '14' as [No.],    Count(*) as [CheckDup],    'KPI Targer (LOB)' as [Table]      ,    'IMPORTANT' as [Note]\n",
    "From       LOB_Tar                       UNION ALL\n",
    "------[ðŸ“¥]--(ðŸ‘‰ï¾Ÿãƒ®ï¾Ÿ)ðŸ‘‰                   LOBGR_Tar\n",
    "Select    '15' as [No.],    Count(*) as [CheckDup],    'KPI Targer (LOB Group)' as [Table],    'IMPORTANT' as [Note]\n",
    "From       LOBGR_Tar                     UNION ALL\n",
    "------[ðŸ“¥]--(ðŸ‘‰ï¾Ÿãƒ®ï¾Ÿ)ðŸ‘‰                   LOGOUT_COUNT\n",
    "Select    '16' as [No.],    Count(*) as [CheckDup],    'Logout Count' as [Table]          ,    'IMPORTANT' as [Note]\n",
    "From       LOGOUT_COUNT                  UNION ALL\n",
    "------[ðŸ“¥]--(ðŸ‘‰ï¾Ÿãƒ®ï¾Ÿ)ðŸ‘‰                   OT_Ramco\n",
    "Select    '17' as [No.],    Count(*) as [CheckDup],    'OT Ramco' as [Table]              ,    'IMPORTANT' as [Note]\n",
    "From       OT_Ramco                      UNION ALL\n",
    "------[ðŸ“¥]--(ðŸ‘‰ï¾Ÿãƒ®ï¾Ÿ)ðŸ‘‰                   OverTime_Raw1\n",
    "Select    '18' as [No.],    Count(*) as [CheckDup],    'OverTime Raw' as [Table]          ,    'IMPORTANT' as [Note]\n",
    "From       OverTime_Raw1                  UNION ALL\n",
    "------[ðŸ“¥]--(ðŸ‘‰ï¾Ÿãƒ®ï¾Ÿ)ðŸ‘‰                   PSAT_Raw1\n",
    "Select    '19' as [No.],    Count(*) as [CheckDup],    'PSAT' as [Table]                  ,    '' as [Note]\n",
    "From       PSAT_Raw1                      UNION ALL\n",
    "------[ðŸ“¥]--(ðŸ‘‰ï¾Ÿãƒ®ï¾Ÿ)ðŸ‘‰                   Quality_Raw1\n",
    "Select    '20' as [No.],    Count(*) as [CheckDup],    'Quality_Raw' as [Table]           ,    '' as [Note]\n",
    "From       Quality_Raw1                   UNION ALL\n",
    "------[ðŸ“¥]--(ðŸ‘‰ï¾Ÿãƒ®ï¾Ÿ)ðŸ‘‰                   Ramco_Raw1\n",
    "Select    '21' as [No.],    Count(*) as [CheckDup],    'Ramco Raw' as [Table]             ,    'IMPORTANT' as [Note]\n",
    "From       Ramco_Raw1                     UNION ALL\n",
    "------[ðŸ“¥]--(ðŸ‘‰ï¾Ÿãƒ®ï¾Ÿ)ðŸ‘‰                   Requirement_Hours\n",
    "Select    '23' as [No.],    Count(*) as [CheckDup],    'Daily Requirement' as [Table]     ,    'IMPORTANT' as [Note]\n",
    "From       Requirement_Hours             UNION ALL\n",
    "------[ðŸ“¥]--(ðŸ‘‰ï¾Ÿãƒ®ï¾Ÿ)ðŸ‘‰                   Resignation_Dump\n",
    "Select    '24' as [No.],    Count(*) as [CheckDup],    'Resignation Dump' as [Table]      ,    '' as [Note]\n",
    "From       Resignation_Dump              UNION ALL\n",
    "------[ðŸ“¥]--(ðŸ‘‰ï¾Ÿãƒ®ï¾Ÿ)ðŸ‘‰                   RONA_Raw1\n",
    "Select    '25' as [No.],    Count(*) as [CheckDup],    'RONA' as [Table]                  ,    '' as [Note]\n",
    "From       RONA_Raw1                      UNION ALL\n",
    "------[ðŸ“¥]--(ðŸ‘‰ï¾Ÿãƒ®ï¾Ÿ)ðŸ‘‰                   Roster_Raw\n",
    "Select    '26' as [No.],    Count(*) as [CheckDup],    'Roster Raw' as [Table]            ,    'IMPORTANT' as [Note]\n",
    "From       Roster_Raw                    UNION ALL\n",
    "------[ðŸ“¥]--(ðŸ‘‰ï¾Ÿãƒ®ï¾Ÿ)ðŸ‘‰                   Shrinkage_Target\n",
    "Select    '28' as [No.],    Count(*) as [CheckDup],    'Shrinkage Target' as [Table]      ,    '' as [Note]\n",
    "From       Shrinkage_Target              UNION ALL\n",
    "------[ðŸ“¥]--(ðŸ‘‰ï¾Ÿãƒ®ï¾Ÿ)ðŸ‘‰                   Termination_Dump\n",
    "Select    '29' as [No.],    Count(*) as [CheckDup],    'Termination Dump' as [Table]      ,    '' as [Note]\n",
    "From       Termination_Dump              UNION ALL\n",
    "------[ðŸ“¥]--(ðŸ‘‰ï¾Ÿãƒ®ï¾Ÿ)ðŸ‘‰                   Ticket_Raw1\n",
    "Select    '30' as [No.],    Count(*) as [CheckDup],    'Ticket Raw' as [Table]            ,    '' as [Note]\n",
    "From       Ticket_Raw1                    UNION ALL\n",
    "------[ðŸ“¥]--(ðŸ‘‰ï¾Ÿãƒ®ï¾Ÿ)ðŸ‘‰                   Workplan_Raw1\n",
    "Select    '31' as [No.],    Count(*) as [CheckDup],    'Workplan Raw' as [Table]          ,    '' as [Note]\n",
    "From       Workplan_Raw1                  UNION ALL\n",
    "------[ðŸ“¥]--(ðŸ‘‰ï¾Ÿãƒ®ï¾Ÿ)ðŸ‘‰                   Workplan_Summary\n",
    "Select    '32' as [No.],    Count(*) as [CheckDup],    'Workplan Summary Raw' as [Table]  ,    '' as [Note]\n",
    "From       Workplan_Summary              UNION ALL\n",
    "------[ðŸ“¥]--(ðŸ‘‰ï¾Ÿãƒ®ï¾Ÿ)ðŸ‘‰                   IPH_PEGA\n",
    "Select    '34' as [No.],    Count(*) as [CheckDup],    'IPH_PEGA' as [Table]              ,    '' as [Note]\n",
    "From       IPH_PEGA                     UNION ALL     \n",
    "------[ðŸ“¥]--(ðŸ‘‰ï¾Ÿãƒ®ï¾Ÿ)ðŸ‘‰                   TED Name\n",
    "Select    '35' as [No.],    Count(*) as [CheckDup],    'TEDNAME' as [Table]              ,    'IMPORTANT' as [Note]\n",
    "From       TEDNAME                      UNION ALL\n",
    "------[ðŸ“¥]--(ðŸ‘‰ï¾Ÿãƒ®ï¾Ÿ)ðŸ‘‰                   TED Name\n",
    "Select    '36' as [No.],    Count(*) as [CheckDup],    'OTREQ' as [Table]              ,      'IMPORTANT' as [Note]\n",
    "From       OTREQ                        UNION ALL\n",
    "------[ðŸ“¥]--(ðŸ‘‰ï¾Ÿãƒ®ï¾Ÿ)ðŸ‘‰                   TED Name\n",
    "Select    '37' as [No.],    Count(*) as [CheckDup],    'PROHC' as [Table]              ,      'IMPORTANT' as [Note]\n",
    "From       PROHC\n",
    "--          \n",
    "\"\"\"\n",
    "# Äá»c dá»¯ liá»‡u vÃ o DataFrame\n",
    "Code2_Result = pl.read_database(query=sql_query2, connection=engine)\n",
    "engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source collection\n",
    "user_credential = os.path.join(os.environ['USERPROFILE'],r'Concentrix Corporation//CNXVN - WFM Team - Documents//')\n",
    "\n",
    "# INPUT-----ðŸ’¾-----ðŸ’¾-----ðŸ’¾-----ðŸ’¾-----ðŸ’¾-----ðŸ’¾-----ðŸ’¾-----ðŸ’¾-----ðŸ’¾-----ðŸ’¾\n",
    "# [BKN]Error Log\n",
    "LOG_LINK = os.path.join(user_credential,\n",
    "                                r'DataBase//DataRaw//BKN//MODIFIED_LOG//*.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#DEFINITION\n",
    "#Name Pattern definition\n",
    "def Namepattern(folder_name):\n",
    "    match folder_name:\n",
    "        case \"Staff\":\n",
    "            pattern = r\"CNX Global Master Roster.xlsx\"\n",
    "        case \"ProjectedShrink\":\n",
    "            pattern = r\"IO Shrinkage.xlsx\"\n",
    "        case \"LTTransfers\":\n",
    "            pattern = r\"transfer.xlsx\"\n",
    "        case \"KPI_Target\":\n",
    "            pattern = r\"kpi_target.xlsx\"\n",
    "        case \"EPS\":\n",
    "            pattern = r\"EPS Tableau - (\\d{8})\\.csv\"\n",
    "        case \"DailyReq\":\n",
    "            pattern = r\"(\\d{6})(_(\\d{6}))?\\..{4}\"\n",
    "        case \"IntervalReq\":\n",
    "            pattern = r\"(\\d{6,8})(_(\\d{6,8}))?\\..{4}\"   \n",
    "        case \"Contrack\":\n",
    "            pattern = r\"(\\[WFM\\] Contact Tracker|(\\d{8})(_(\\d{8}))?|W(\\d{2})-(\\d{4}))\\..{4}\"  \n",
    "        case \"CapHC\":\n",
    "            pattern = r\"(\\d{4})(_(\\d{4}))\\..{4}\"  \n",
    "        case \"AHT\":\n",
    "            pattern = r\"(\\d{8})_(\\d{8})_Items_data\\..{4}\"  \n",
    "        case \"Termination\":\n",
    "            pattern = r\"WDD.xlsx\" \n",
    "        case \"Resignation\":\n",
    "            pattern = r\"WDD.xlsx\" \n",
    "        case \"PremHdays\":\n",
    "            pattern = r\"Holiday Mapping.csv\" \n",
    "        case \"NormHdays\":\n",
    "            pattern = r\"Holiday Nonbillable.csv\"\n",
    "        case \"EmpMaster\":\n",
    "            pattern = r\"WDD.xlsx\"\n",
    "        case \"CUIC_RTMonitor\":\n",
    "            pattern = r\"00_RTA_View-Agent Team Real Time.xlsx\"\n",
    "        case \"CUIC\":\n",
    "            pattern = r\"(\\d{4,8})_(\\d{2,8})(_(\\d{2}))?(_\\d)?\\..{4}\"\n",
    "        case _:\n",
    "            pattern = r\"(\\d{8})(_(\\d{8}))?\\..{3,4}\"\n",
    "    return pattern\n",
    "\n",
    "#table1\n",
    "Folder_column_name = 'FOLDER NAME'\n",
    "Name_Coumn_name = 'FileName - ModifiedDate'\n",
    "Row_column_name = 'ROW_NUMBER'\n",
    "#table2\n",
    "important_column= 'Note'\n",
    "Check_dup_column = 'CheckDup'\n",
    "Table_column = 'Table'\n",
    "#log file\n",
    "Error_log_column ='Error'\n",
    "Log_name_column='FileName'\n",
    "\n",
    "#Exception list\n",
    "Expection_list=[\n",
    "    \"EPS Tableau - 20240831\",\n",
    "    \"20231001_20231231\",\n",
    "    \"20220919_20230930\"\n",
    "    ]\n",
    "Exception_pattern = r\"\\..{3,4}\"\n",
    "#Color code\n",
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKCYAN = '\\033[96m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    FAILVIOLET = '\\033[35m'\n",
    "\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Func Compare row for the rest\n",
    "def Row_Compare_Special(Dataframe,Extract_file):\n",
    "    rowdata = Dataframe[Row_column_name].to_list()\n",
    "    # Standard Deviation Math\n",
    "    mean = statistics.mean(rowdata)  \n",
    "    std_dev = statistics.stdev(rowdata) \n",
    "#Tweak the sensitivity of the comparison, lower = more strict\n",
    "    tolerance = 2.5\n",
    "    catch_list=[]\n",
    "    for row in rowdata:\n",
    "         distance = abs(row -mean)\n",
    "         if distance > tolerance * std_dev:\n",
    "              catch_list.append(row)\n",
    "\n",
    "              \n",
    "    if not(catch_list):\n",
    "         return\n",
    "\n",
    "    # Filter the file has been mark out\n",
    "    Sus_file = Dataframe.filter(pl.col(Row_column_name).is_in(catch_list))\n",
    "    for row in Sus_file.iter_rows(named= True):\n",
    "        check =re.split(Exception_pattern,row[Name_Coumn_name])\n",
    "        if check[0] not in Expection_list:\n",
    "            Extract_file.append((row[Name_Coumn_name],row[Folder_column_name],\"Inconsistent row\"))\n",
    "            print(f'{bcolors.WARNING}File {row[Name_Coumn_name]} in {row[Folder_column_name]} has inconsistent number of rows {bcolors.ENDC}')\n",
    "            \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Func Compare Row with pattern\n",
    "def Row_Compare(Dataframe,Extract_file,Folder):\n",
    "    desired_pattern = r\"(\\d{8})_(\\d{8})\"\n",
    "    FilteredDataframe= Dataframe.sort(Name_Coumn_name,descending=False)\n",
    "    rowdivdate =[]\n",
    "    namefilter=[]\n",
    "    row_avg = []\n",
    "    #calculate average number for row in one day we should expected\n",
    "    for row  in FilteredDataframe.iter_rows(named= True):\n",
    "         \n",
    "        match = re.match(desired_pattern,row[Name_Coumn_name])\n",
    "        start_date_string,end_date_start_date_string = match.groups()\n",
    "        try:\n",
    "            start_date = dt.strptime(start_date_string,\"%Y%m%d\").date()\n",
    "            end_date = dt.strptime(end_date_start_date_string,\"%Y%m%d\").date()\n",
    "            #average number of rows should have per day\n",
    "            namefilter.append(row[Name_Coumn_name])\n",
    "            row_avg.append((row[Row_column_name]/((end_date - start_date).days)))\n",
    "        \n",
    "        #Find out if file has the same start date and end date  \n",
    "        except ZeroDivisionError:\n",
    "            print(f\"{bcolors.OKBLUE}File {row[Name_Coumn_name]} in {Folder} has the same start and end date{bcolors.ENDC}\")\n",
    "            Extract_file.append((row[Name_Coumn_name],Folder,\"Same Start and End date\"))\n",
    "            continue\n",
    "\n",
    "    namefilter_series = pl.Series(\"File_name\", namefilter)\n",
    "    row_avg_series = pl.Series(\"Average_Row\", row_avg)\n",
    "    #If none calculated, return\n",
    "    if len(namefilter)<2:\n",
    "        return\n",
    "\n",
    "     #Create Dataframe\n",
    "\n",
    "    dffinal = pl.DataFrame([namefilter_series,row_avg_series])\n",
    "    rowdata = dffinal['Average_Row'].to_list()\n",
    "    # Standard Deviation Math\n",
    "    mean = statistics.mean(rowdata)  \n",
    "    std_dev = statistics.stdev(rowdata) \n",
    "    #Tweak the sensitivity of the comparison, lower = more strict\n",
    "    tolerance = 2.5\n",
    "    catch_list=[]\n",
    "\n",
    "    for row in rowdata:\n",
    "        distance = abs(row -mean)\n",
    "        if distance > tolerance * std_dev:\n",
    "            catch_list.append(row)\n",
    "\n",
    "    \n",
    "    if not(catch_list):\n",
    "        return\n",
    "     \n",
    "     # Filter the file has been mark out\n",
    "    Sus_file = dffinal.filter(pl.col('Average_Row').is_in(catch_list))\n",
    "    for row in Sus_file.iter_rows(named= True):\n",
    "        check =re.split(Exception_pattern,row['File_name'])\n",
    "        if check[0] not in Expection_list:\n",
    "            Extract_file.append((row['File_name'],Folder,\"Inconsistent average row\"))\n",
    "            print(f'{bcolors.FAIL}File {row['File_name']} in {Folder} has inconsistent number of rows {bcolors.ENDC}')\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#Func Duplicate date check\n",
    "def Duplicate_Date_Check(list_name,extract_list,folder):\n",
    "    file_ranges = []\n",
    "    Valid_list = []\n",
    "    desired_pattern = r\"(\\d{8})_(\\d{8})\"\n",
    "    \n",
    "    for file_name in list_name:\n",
    "        match = re.match(desired_pattern,file_name)\n",
    "        if match:\n",
    "                start_date_string,end_date_start_date_string = match.groups()\n",
    "                try:\n",
    "                    start_date = dt.strptime(start_date_string,\"%Y%m%d\").date()\n",
    "                    end_date = dt.strptime(end_date_start_date_string,\"%Y%m%d\").date()\n",
    "                    #Put date into a list to compare\n",
    "                    file_ranges.append((start_date,end_date,file_name))\n",
    "                    # Valid file will be use to further check\n",
    "                    Valid_list.append(file_name)\n",
    "                except ValueError:\n",
    "                    print(f\"{bcolors.OKBLUE} {file_name} has invalid name format in {folder}, cannot compare date for further check{bcolors.ENDC}\")\n",
    "\n",
    "    file_ranges.sort()\n",
    "    \n",
    "    for i in range(len(file_ranges)):\n",
    "        start_i, end_i, file_i = file_ranges[i]\n",
    "        overlap_count =0\n",
    "        for j in range(len(file_ranges)):\n",
    "            if i!= j:\n",
    "                start_j, end_j, file_j = file_ranges[j]\n",
    "                #Check if current file is totally in another file\n",
    "                if start_j <= start_i <= end_j and start_j <= end_i <= end_j:\n",
    "                    extract_list.append((file_i,folder,\"Duplicate file\"))\n",
    "                    print(f'{bcolors.FAIL}Duplicate date file: {file_i}{bcolors.ENDC}')\n",
    "                else:\n",
    "                # Check if currentfile has a part in another file\n",
    "                    if start_i <= end_j and end_i >= start_j:\n",
    "                        overlap_count +=1\n",
    "                if overlap_count >=2:\n",
    "                    extract_list.append((file_i,folder,\"Has overlap days\"))\n",
    "                    print(f'{bcolors.WARNING}File might have issue with overlapping date: {file_i}{bcolors.ENDC}')\n",
    "\n",
    "                    break\n",
    "    \n",
    "    return Valid_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Func Check update \n",
    "\n",
    "def File_need_update_check(list_name,extract_list,Folder):\n",
    "    #pattern to get the modified date\n",
    "    time_pattern = r\"(\\d{4})\\-(\\d{1,2})\\-(\\d{1,2})\"\n",
    "    current_date = dt.now()\n",
    "    #flag to report\n",
    "    Most_recent_CUIC_file = 1\n",
    "    Most_recent_EPS_file = 5\n",
    "    Most_recent_Quality_file = 5\n",
    "    Most_recent_WPsummary_file = 7\n",
    "    Most_recent_IEX_hours_file = 7\n",
    "    Most_recent_WpDetail_file = 7\n",
    "    \n",
    "\n",
    "    for name in list_name:\n",
    "        #Check last time file was modified by seaching the pattern, assume that file is up to date\n",
    "        match = re.search(time_pattern,name)\n",
    "        How_Long_have_not_update=0\n",
    "        if match:\n",
    "            year_string,month_string,day_string = match.groups()\n",
    "            last_edit = dt(int(year_string),int(month_string),int(day_string))\n",
    "            How_Long_have_not_update = (current_date-last_edit).days\n",
    "        \n",
    "        #Threshhold for each type of folder\n",
    "        match Folder:\n",
    "            case 'ExceptionReq':\n",
    "                if  How_Long_have_not_update >1:\n",
    "                    print(f'{bcolors.OKCYAN}File {name} in {Folder} might need to update{bcolors.ENDC}')\n",
    "                    extract_list.append((name,Folder,\"May need update\"))           \n",
    "            #Update CPI\n",
    "            case 'CPI':\n",
    "                How_old_is_this_file =4\n",
    "                # Calculate how old is this file and should it be neccessary to update it\n",
    "                yyyymmdd_pattern = r'(\\d{8})'\n",
    "                matchformat = re.search(yyyymmdd_pattern,name)\n",
    "                if matchformat:\n",
    "                    start_date_string = matchformat.group()\n",
    "                    start_date = dt.strptime(start_date_string,\"%Y%m%d\")\n",
    "                    How_old_is_this_file = (current_date-start_date).days\n",
    "\n",
    "                if  How_old_is_this_file < 3 and How_Long_have_not_update >=1:\n",
    "                        print(f'{bcolors.OKCYAN}File {name} in {Folder} might need to update{bcolors.ENDC}')\n",
    "                        extract_list.append((name,Folder,\"May need update\"))\n",
    "            #Update CUIC\n",
    "            case 'CUIC':\n",
    "                How_old_is_this_file =4\n",
    "                # Calculate how old is this file and should it be neccessary to update it\n",
    "              \n",
    "                yyyy_dd_mm_pattern = r'(\\d{4})_(\\d{2})_(\\d{2})'\n",
    "                matchformat = re.match(yyyy_dd_mm_pattern,name)\n",
    "                if matchformat:\n",
    "                    year_s,mon_s,day_s = matchformat.groups()\n",
    "                    start_2 = dt(int(year_s),int(mon_s),int(day_s))\n",
    "                    How_old_is_this_file = (current_date-start_2).days\n",
    "\n",
    "                    #Flag to check for the newest file\n",
    "                    if How_old_is_this_file <=0:\n",
    "                        Most_recent_CUIC_file = 0\n",
    "\n",
    "                if  How_old_is_this_file < 3 and How_Long_have_not_update >1:\n",
    "                        print(f'{bcolors.OKCYAN}File {name} in {Folder} might need to update{bcolors.ENDC}')\n",
    "                        extract_list.append((name,Folder,\"May need update\"))\n",
    "\n",
    "            #update 1-3 days\n",
    "            case 'CSAT_RS'|'CSAT_TP'|'LogoutCount'|'PSAT'|'RAMCO'|'ROSTER'|'AHT':\n",
    "\n",
    "                # Calculate how old is this file and should it be neccessary to update it\n",
    "                start_end_pattern = r'(\\d{8})_(\\d{8})'\n",
    "                How_old_is_this_file =8\n",
    "                matchformat = re.search(start_end_pattern,name)\n",
    "                \n",
    "                if matchformat:\n",
    "                    start_date_string,end_date_string = matchformat.groups()\n",
    "                    end_date = dt.strptime(end_date_string,\"%Y%m%d\")\n",
    "                    How_old_is_this_file = (current_date-end_date).days\n",
    "\n",
    "                    if  How_Long_have_not_update > 2 and How_old_is_this_file <4:  \n",
    "                        print(f'{bcolors.OKCYAN}File {name} in {Folder} might need to update{bcolors.ENDC}')\n",
    "                        extract_list.append((name,Folder,\"May need update\"))\n",
    "\n",
    "            #update weekly\n",
    "            case 'CPI_PEGA'|'OT_RAMCO'|'OTReq'|'RegisteredOT'|'RONA':\n",
    "                # Calculate how old is this file and should it be neccessary to update it\n",
    "                start_end_pattern = r'(\\d{8})_(\\d{8})'\n",
    "                How_old_is_this_file =8\n",
    "                matchformat = re.search(start_end_pattern,name)\n",
    "                if matchformat:\n",
    "                    start_date_string,end_date_string = matchformat.groups()\n",
    "                    end_date = dt.strptime(end_date_string,\"%Y%m%d\")\n",
    "                    How_old_is_this_file = (current_date-end_date).days\n",
    "                \n",
    "                    if  How_Long_have_not_update > 5 and How_old_is_this_file <7:  \n",
    "                        print(f'{bcolors.OKCYAN}File {name} in {Folder} might need to update{bcolors.ENDC}')\n",
    "                        extract_list.append((name,Folder,\"May need update\"))\n",
    "\n",
    "            #update Contract\n",
    "            case 'Contrack':\n",
    "                # Calculate how old is this file and should it be neccessary to update it\n",
    "                wfm_pattern = r'[WFM] Contact Tracker'\n",
    "                matchformat = re.search(wfm_pattern,name)  \n",
    "                if How_Long_have_not_update> 2 and matchformat:  \n",
    "                    print(f'{bcolors.OKCYAN}File {name} in {Folder} might need to update{bcolors.ENDC}')\n",
    "                    extract_list.append((name,Folder,\"May need update\"))\n",
    "        \n",
    "            #Update IEX_Hrs\n",
    "            case 'IEX_Hrs':\n",
    "                # Calculate how old is this file and should it be neccessary to update it\n",
    "                yyyymmdd_pattern = r'(\\d{8})'\n",
    "                matchformat = re.search(yyyymmdd_pattern,name)                \n",
    "                How_old_is_this_file = 7\n",
    "                if matchformat:\n",
    "                    start_date_string = matchformat.group()\n",
    "                    start_date = dt.strptime(start_date_string,\"%Y%m%d\")\n",
    "                    How_old_is_this_file = (current_date-start_date).days\n",
    "                # Flag the folder if the file is not newest\n",
    "                    if  How_old_is_this_file < Most_recent_IEX_hours_file:  \n",
    "                        Most_recent_IEX_hours_file= How_old_is_this_file\n",
    "\n",
    "            #update monthly\n",
    "            case 'CapHC'|'DailyReq'|'IntervalReq'|'KPI_Target'|'LTTransfers'|'ProjectedHC':\n",
    "                if  How_Long_have_not_update > 30:  \n",
    "                    print(f'{bcolors.OKCYAN}File {name} in {Folder} might need to update{bcolors.ENDC}')\n",
    "                    extract_list.append((name,Folder,\"May need update\"))\n",
    "\n",
    "            #update yearly\n",
    "            case 'PremHdays'|'NormHdays':\n",
    "                if How_Long_have_not_update > 364:\n",
    "                    print(f'{bcolors.OKCYAN}File {name} in {Folder} might need to update{bcolors.ENDC}')\n",
    "                    extract_list.append((name,Folder,\"May need update\"))\n",
    "\n",
    "            #update WDD/ProjectedShrink/Staff\n",
    "            case 'EmpMaster'|'Resignation'|'Termination'|'ProjectedShrink'|'Staff':\n",
    "                if How_Long_have_not_update > 8 and Folder != 'ProjectedShrink' :\n",
    "                    print(f'{bcolors.OKCYAN}File {name} in {Folder} might need to update{bcolors.ENDC}')\n",
    "                    extract_list.append((name,Folder,\"May need update\"))\n",
    "\n",
    "            #Update EPS\n",
    "            case 'EPS':\n",
    "                How_old_is_this_file =5\n",
    "                yyyymmdd_pattern = r'(\\d{8})'\n",
    "                matchformat = re.search(yyyymmdd_pattern,name)\n",
    "                if matchformat:\n",
    "                    start_date_string = matchformat.group()\n",
    "                    start_date = dt.strptime(start_date_string,\"%Y%m%d\")\n",
    "                    How_old_is_this_file = (current_date-start_date).days\n",
    "                # Flag the folder if the file is not newest\n",
    "                    if How_old_is_this_file < Most_recent_EPS_file:\n",
    "                        Most_recent_EPS_file = How_old_is_this_file\n",
    "                \n",
    "                if  How_old_is_this_file < 4 and How_Long_have_not_update >=3:\n",
    "                        print(f'{bcolors.OKCYAN}File {name} in {Folder} might need to update{bcolors.ENDC}')\n",
    "                        extract_list.append((name,Folder,\"May need update\"))\n",
    "\n",
    "            #Update Quality\n",
    "            case 'Quality':\n",
    "                How_old_is_this_file =5\n",
    "                yyyymmdd_pattern = r'(\\d{8})'\n",
    "                matchformat = re.search(yyyymmdd_pattern,name)\n",
    "                if matchformat:\n",
    "                    start_date_string = matchformat.group()\n",
    "                    start_date = dt.strptime(start_date_string,\"%Y%m%d\")\n",
    "                    How_old_is_this_file = (current_date-start_date).days\n",
    "                # Flag the folder if the file is not newest\n",
    "                    if How_old_is_this_file < Most_recent_Quality_file:\n",
    "                        Most_recent_Quality_file = How_old_is_this_file\n",
    "\n",
    "                if  How_old_is_this_file < 4 and How_Long_have_not_update >=3:\n",
    "                        print(f'{bcolors.OKCYAN}File {name} in {Folder} might need to update{bcolors.ENDC}')\n",
    "                        extract_list.append((name,Folder,\"May need update\"))\n",
    "\n",
    "            #Update WPSUMA\n",
    "            case 'WpSummary':\n",
    "                How_old_is_this_file =7\n",
    "                yyyymmdd_pattern = r'(\\d{8})'\n",
    "                matchformat = re.search(yyyymmdd_pattern,name)\n",
    "                if matchformat:\n",
    "                    start_date_string = matchformat.group()\n",
    "                    start_date = dt.strptime(start_date_string,\"%Y%m%d\")\n",
    "                    How_old_is_this_file = (current_date-start_date).days\n",
    "                # Flag the folder if the file is not newest\n",
    "                    if How_old_is_this_file < Most_recent_WPsummary_file:\n",
    "                        Most_recent_WPsummary_file = How_old_is_this_file\n",
    "                if  How_old_is_this_file < 7 and How_Long_have_not_update >=2:\n",
    "                        print(f'{bcolors.OKCYAN}File {name} in {Folder} might need to update{bcolors.ENDC}')\n",
    "                        extract_list.append((name,Folder,\"May need update\"))\n",
    "\n",
    "            #Update WpDetail\n",
    "            case 'WpDetail':\n",
    "                start_end_pattern = r'(\\d{8})_(\\d{8})'\n",
    "                How_old_is_this_file =7\n",
    "                matchformat = re.search(start_end_pattern,name)\n",
    "                if matchformat:\n",
    "                    start_date_string,end_date_string = matchformat.groups()\n",
    "                    end_date = dt.strptime(end_date_string,\"%Y%m%d\")\n",
    "                    How_old_is_this_file = (current_date-end_date).days\n",
    "                else:\n",
    "                    extra_pattern = r'(\\d{8})'\n",
    "                    matchformat = re.search(extra_pattern,name)\n",
    "                    if matchformat:\n",
    "                        start_date_string = matchformat.group()\n",
    "                        end_date = dt.strptime(start_date_string,\"%Y%m%d\")\n",
    "                        How_old_is_this_file = (current_date-end_date).days\n",
    "\n",
    "\n",
    "                # Flag the folder if the file is not newest\n",
    "                if  How_old_is_this_file < Most_recent_WpDetail_file:\n",
    "                    Most_recent_WpDetail_file= How_old_is_this_file\n",
    "  \n",
    "            #folder not found\n",
    "            case _:\n",
    "                print(f'{bcolors.WARNING}{Folder} is not expected for the update check {bcolors.ENDC}')\n",
    "                return\n",
    "    \n",
    "    # Raise notify base on flag \n",
    "    if Most_recent_CUIC_file!=0 and Folder==\"CUIC\":\n",
    "        print(f'{bcolors.OKCYAN}{Folder} need to update the newest data {bcolors.ENDC}')\n",
    "        extract_list.append((\"Missing Today File\",Folder,\"Need update\"))\n",
    "    if Most_recent_EPS_file>=3 and Folder==\"EPS\":\n",
    "        print(f'{bcolors.OKCYAN}{Folder} need to update the newest data {bcolors.ENDC}')\n",
    "        extract_list.append((\"Missing This Week File\",Folder,\"Need update\"))\n",
    "    if Most_recent_Quality_file>=3 and Folder==\"Quality\":\n",
    "        print(f'{bcolors.OKCYAN}{Folder} need to update the newest data {bcolors.ENDC}')\n",
    "        extract_list.append((\"Missing This Week File\",Folder,\"Need update\"))\n",
    "    if Most_recent_WPsummary_file >6 and Folder==\"WpSummary\":\n",
    "        print(f'{bcolors.OKCYAN}{Folder} need to update the newest data {bcolors.ENDC}')\n",
    "        extract_list.append((\"Missing This Week File\",Folder,\"Need update\"))\n",
    "    if Most_recent_IEX_hours_file >6 and Folder==\"IEX_Hrs\":\n",
    "        print(f'{bcolors.OKCYAN}{Folder} need to update the newest data {bcolors.ENDC}')\n",
    "        extract_list.append((\"Missing This Week File\",Folder,\"Need update\"))\n",
    "    if Most_recent_WpDetail_file >6 and Folder==\"WpDetail\":\n",
    "        print(f'{bcolors.OKCYAN}{Folder} need to update the newest data {bcolors.ENDC}')\n",
    "        extract_list.append((\"Missing This Week File\",Folder,\"Need update\"))    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Func CODE 1\n",
    "def CategoryCheck (your_dataframe,file_error):\n",
    "     #Check each folder \n",
    "     for folder_name in your_dataframe[Folder_column_name].unique():\n",
    "               #Current folder checking\n",
    "               current_check = your_dataframe.filter(pl.col(Folder_column_name) == folder_name)\n",
    "               current_extract_grp=[]\n",
    "               #Check naming rule\n",
    "               for file_name in current_check[Name_Coumn_name]:\n",
    "                    match = re.match(Namepattern(folder_name),file_name)\n",
    "                    if match:\n",
    "                          #Extract valid file name to check further\n",
    "                        current_extract_grp.append(f\"{file_name}\")\n",
    "                    else:\n",
    "                        print(f\"{bcolors.FAILVIOLET}Weird Name Pattern at {file_name} in folder {folder_name}{bcolors.ENDC}\")\n",
    "                        current_extract_grp.append(f\"{file_name}\")\n",
    "                        file_error.append((file_name,folder_name,\"Weird Name\"))\n",
    "\n",
    "               #Check update\n",
    "               File_need_update_check(current_extract_grp,file_error,folder_name)\n",
    "               \n",
    "               #Check Duplicate Date, if current folder only have 1 file, skip this\n",
    "               filter_file = []\n",
    "               if current_check.count()[0,1]>1:\n",
    "                    filter_file = Duplicate_Date_Check(current_extract_grp,file_error,folder_name)\n",
    "                    \n",
    "               #Deviation in row check using valid list with format yyyymmdd_yyyymmdd from previous func\n",
    "               if filter_file:\n",
    "                    filtered_DF_standard_pattern = current_check.filter(pl.col(Name_Coumn_name).is_in(filter_file))\n",
    "                    Row_Compare(filtered_DF_standard_pattern,file_error,folder_name)\n",
    "               \n",
    "               #Row check for wild file name\n",
    "               filtered_DF_other_pattern = current_check.filter(pl.col(Name_Coumn_name).is_in(filter_file).not_())\n",
    "               if filtered_DF_other_pattern.count()[0,1]>1 and folder_name != \"CUIC\": \n",
    "                    #Delete the [and folder_name != \"CUIC\"] when feel like it's time (ðŸ‘‰ï¾Ÿãƒ®ï¾Ÿ)ðŸ‘‰Due to CUIC format is under renovation, temporarely remove it from the check so the number of file with new rows pattern will soon be sufficient to not bluffing during this report\n",
    "                    Row_Compare_Special(filtered_DF_other_pattern,file_error)\n",
    "\n",
    "                \n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error check CODE 2 and read LOG\n",
    "def Error_Check(Dataframe,extract_file,LINK):\n",
    "    #Check code 2\n",
    "    Only_Important = Dataframe.filter(pl.col(important_column)==\"IMPORTANT\",pl.col(Check_dup_column)>0)\n",
    "    if Only_Important.count()[0,1]>0:\n",
    "        for row in Only_Important.iter_rows(named=True):\n",
    "                print(f\"{bcolors.FAIL}Folder {row[Table_column]} has {row[Check_dup_column]} duplicate row{\"s\"if row[Check_dup_column]>1 else \" \"}{bcolors.ENDC}\")\n",
    "                extract_file.append((\"\",row[Table_column],F\"{row[Check_dup_column]} row(s) of duplicate data\"))\n",
    "    #Check log\n",
    "    for file in glob.glob(LINK):\n",
    "        #Get file name -> folder name\n",
    "        Current_file = pl.read_excel(file)\n",
    "        filename = os.path.basename(file)\n",
    "        foldername = re.split('_log',filename)\n",
    "        # Looking out for error in error column of log file\n",
    "        try:\n",
    "            if Current_file.filter(pl.col(Error_log_column)!=\"\").count()[0,1]>0:\n",
    "                for row in Current_file.filter(pl.col(Error_log_column)!=\"\").iter_rows(named=True):\n",
    "                    print(f\"{bcolors.FAIL}Folder {row[Log_name_column]} in {foldername[0]} a has fatal error{bcolors.ENDC}\")\n",
    "                    extract_file.append((row[Log_name_column],foldername[0],\"Fatal Error\"))        \n",
    "        except Exception:\n",
    "            continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[96mWpDetail need to update the newest data \u001b[0m\n",
      "\u001b[96mIEX_Hrs need to update the newest data \u001b[0m\n",
      "\u001b[96mFile transfer.xlsx - 2025-02-25 19:59:33 in LTTransfers might need to update\u001b[0m\n",
      "\u001b[93mFile 20250330.csv - 2025-04-03 18:13:34 in CPI has inconsistent number of rows \u001b[0m\n",
      "\u001b[96mFile 202303_202502.xlsx - 2025-02-27 20:34:37 in DailyReq might need to update\u001b[0m\n",
      "\u001b[96mQuality need to update the newest data \u001b[0m\n",
      "\u001b[96mFile EPS Tableau - 20250406.csv - 2025-04-03 23:15:28 in EPS might need to update\u001b[0m\n",
      "\u001b[93mFile [WFM] Contact Tracker.xlsx - 2025-04-07 20:32:15 in Contrack has inconsistent number of rows \u001b[0m\n",
      "\u001b[96mFile 20250101_20251231.xlsx - 2025-03-15 04:47:30 in RONA might need to update\u001b[0m\n",
      "\u001b[93mCUIC_RTMonitor is not expected for the update check \u001b[0m\n",
      "\u001b[96mFile 20240101_20250223.xlsm - 2025-02-24 17:17:24 in ProjectedHC might need to update\u001b[0m\n",
      "\u001b[96mFile 20250224_20250302.xlsm - 2025-02-25 20:16:13 in ProjectedHC might need to update\u001b[0m\n",
      "\u001b[91mFolder 2025-04-07_1.xlsx in CUIC a has fatal error\u001b[0m\n",
      "\u001b[91mFolder 2025-04-07_2.xlsx in CUIC a has fatal error\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#MAIN RUN\n",
    "file_error= []\n",
    "CategoryCheck(Code1_Result,file_error)\n",
    "Error_Check(Code2_Result,file_error,LOG_LINK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Final Report\n",
    "error_report = pl.DataFrame(file_error, schema=[\"File Name\",\"Folder\",\"Reason\"],)\n",
    "Output_path = os.path.join(user_credential,\n",
    "                                r'DataBase//DataRaw//BKN')\n",
    "Output_file = os.path.join(Output_path,\n",
    "                            r'Mod_Log.xlsx')\n",
    "error_report.write_excel(Output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
