{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#libraries\n",
    "import os, glob, re\n",
    "from datetime import datetime as dt, time as t, date as d\n",
    "import polars as pl\n",
    "from sqlalchemy import create_engine, text\n",
    "import statistics\n",
    "a=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Data code 1\n",
    "# T·∫°o engine k·∫øt n·ªëi\n",
    "SERVER_NAME = \"PHMANVMDEV01V\"\n",
    "DATABASE = \"wfm_vn_dev\"\n",
    "connection_string = f\"mssql+pyodbc://{SERVER_NAME}/{DATABASE}?driver=ODBC+Driver+17+for+SQL+Server&Trusted_Connection=yes\"\n",
    "engine = create_engine(connection_string)\n",
    " \n",
    "# C√¢u l·ªánh SQL\n",
    "sql_query = \"\"\"\n",
    " \n",
    " \n",
    "WITH CombinedData AS (\n",
    "-- 01/ BCOM.AHT \n",
    "    SELECT 'BKN' AS [SCHEMA], 'AHT' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM BCOM.AHT\n",
    "    UNION ALL\n",
    "-- 02/ BCOM.CapHC \n",
    "    SELECT 'BKN' AS [SCHEMA], 'CapHC' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM BCOM.CapHC\n",
    "    UNION ALL\n",
    "-- 03/ BCOM.Contrack\n",
    "    SELECT 'BKN' AS [SCHEMA], 'Contrack' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM BCOM.Contrack\n",
    "    UNION ALL\n",
    "-- 04/ BCOM.CPI\n",
    "    SELECT 'BKN' AS [SCHEMA], 'CPI' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM BCOM.CPI\n",
    "    UNION ALL\n",
    "-- 05/ BCOM.CPI_PEGA\n",
    "    SELECT 'BKN' AS [SCHEMA], 'CPI_PEGA' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM BCOM.CPI_PEGA\n",
    "    UNION ALL\n",
    "-- 06/ BCOM.CSAT_RS\n",
    "    SELECT 'BKN' AS [SCHEMA], 'CSAT_RS' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM BCOM.CSAT_RS\n",
    "    UNION ALL\n",
    "-- 07/ BCOM.CSAT_TP\n",
    "    SELECT 'BKN' AS [SCHEMA], 'CSAT_TP' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM BCOM.CSAT_TP\n",
    "    UNION ALL\n",
    "-- 08/ BCOM.CUIC\n",
    "    SELECT 'BKN' AS [SCHEMA], 'CUIC' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM BCOM.CUIC\n",
    "    UNION ALL\n",
    "-- 09/ BCOM.CUIC_RTMonitor\n",
    "    SELECT 'BKN' AS [SCHEMA], 'CUIC_RTMonitor' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM BCOM.CUIC_RTMonitor\n",
    "    UNION ALL\n",
    "-- 10/ BCOM.DailyReq\n",
    "    SELECT 'BKN' AS [SCHEMA], 'DailyReq' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM BCOM.DailyReq\n",
    "    UNION ALL\n",
    "-- 11/ BCOM.EPS\n",
    "    SELECT 'BKN' AS [SCHEMA], 'EPS' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM BCOM.EPS\n",
    "    UNION ALL\n",
    "-- 12/ BCOM.ExceptionReq\n",
    "    SELECT 'BKN' AS [SCHEMA], 'ExceptionReq' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM BCOM.ExceptionReq\n",
    "    UNION ALL\n",
    "-- 13/ BCOM.IEX_Hrs\n",
    "    SELECT 'BKN' AS [SCHEMA], 'IEX_Hrs' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM BCOM.IEX_Hrs\n",
    "    UNION ALL\n",
    "-- 14/ BCOM.IntervalReq\n",
    "    SELECT 'BKN' AS [SCHEMA], 'IntervalReq' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM BCOM.IntervalReq\n",
    "    UNION ALL\n",
    "-- 15/ BCOM.KPI_Target\n",
    "    SELECT 'BKN' AS [SCHEMA], 'KPI_Target' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM BCOM.KPI_Target\n",
    "    UNION ALL\n",
    "-- 16/ BCOM.LogoutCount\n",
    "    SELECT 'BKN' AS [SCHEMA], 'LogoutCount' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM BCOM.LogoutCount\n",
    "    UNION ALL\n",
    "-- 17/ BCOM.LTTransfers\n",
    "    SELECT 'BKN' AS [SCHEMA], 'LTTransfers' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM BCOM.LTTransfers\n",
    "    UNION ALL\n",
    "-- 18/ BCOM.OTReq\n",
    "    SELECT 'BKN' AS [SCHEMA], 'OTReq' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM BCOM.OTReq\n",
    "    UNION ALL\n",
    "-- 19/ BCOM.ProjectedHC\n",
    "    SELECT 'BKN' AS [SCHEMA], 'ProjectedHC' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM BCOM.ProjectedHC\n",
    "    UNION ALL\n",
    "-- 20/ BCOM.ProjectedShrink\n",
    "    SELECT 'BKN' AS [SCHEMA], 'ProjectedShrink' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM BCOM.ProjectedShrink\n",
    "    UNION ALL\n",
    "-- 21/ BCOM.PSAT\n",
    "    SELECT 'BKN' AS [SCHEMA], 'PSAT' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM BCOM.PSAT\n",
    "    UNION ALL\n",
    "-- 22/ BCOM.Quality\n",
    "    SELECT 'BKN' AS [SCHEMA], 'Quality' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM BCOM.Quality\n",
    "    UNION ALL\n",
    "-- 23/ BCOM.RegisteredOT\n",
    "    SELECT 'BKN' AS [SCHEMA], 'RegisteredOT' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM BCOM.RegisteredOT\n",
    "    UNION ALL\n",
    "-- 24/ BCOM.RONA\n",
    "    SELECT 'BKN' AS [SCHEMA], 'RONA' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM BCOM.RONA\n",
    "    UNION ALL\n",
    "-- 25/ BCOM.ROSTER\n",
    "    SELECT 'BKN' AS [SCHEMA], 'ROSTER' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM BCOM.ROSTER\n",
    "    UNION ALL\n",
    "-- 26/ BCOM.Staff\n",
    "    SELECT 'BKN' AS [SCHEMA], 'Staff' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM BCOM.Staff\n",
    "    UNION ALL\n",
    "-- 27/ BCOM.WpDetail\n",
    "    SELECT 'BKN' AS [SCHEMA], 'WpDetail' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM BCOM.WpDetail\n",
    "    UNION ALL\n",
    "-- 28/ BCOM.WpSummary\n",
    "    SELECT 'BKN' AS [SCHEMA], 'WpSummary' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM BCOM.WpSummary\n",
    "    UNION ALL\n",
    "-- 29/ GLB.EmpMaster\n",
    "    SELECT 'GLB' AS [SCHEMA], 'EmpMaster' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM GLB.EmpMaster\n",
    "    UNION ALL\n",
    "-- 30/ GLB.NormHdays\n",
    "    SELECT 'GLB' AS [SCHEMA], 'NormHdays' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM GLB.NormHdays\n",
    "    UNION ALL\n",
    "-- 31/ GLB.OT_RAMCO\n",
    "    SELECT 'GLB' AS [SCHEMA], 'OT_RAMCO' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM GLB.OT_RAMCO\n",
    "    UNION ALL\n",
    "-- 32/ GLB.PremHdays\n",
    "    SELECT 'GLB' AS [SCHEMA], 'PremHdays' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM GLB.PremHdays\n",
    "    UNION ALL\n",
    "-- 33/ GLB.RAMCO\n",
    "    SELECT 'GLB' AS [SCHEMA], 'RAMCO' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM GLB.RAMCO\n",
    "    UNION ALL\n",
    "-- 34/ GLB.Resignation\n",
    "    SELECT 'GLB' AS [SCHEMA], 'Resignation' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM GLB.Resignation\n",
    "    UNION ALL\n",
    "-- 35/ GLB.Termination\n",
    "    SELECT 'GLB' AS [SCHEMA], 'Termination' AS [FOLDER NAME], [FileName], [ModifiedDate] FROM GLB.Termination\n",
    "),\n",
    "FileNameCheck AS (\n",
    "    SELECT [SCHEMA],[FOLDER NAME],[FileName] + ' - ' + CONVERT(VARCHAR, [ModifiedDate], 120) AS [FileName - ModifiedDate],COUNT(*) \n",
    "\tAS [ROW_NUMBER]\n",
    "    FROM CombinedData\n",
    "    GROUP BY [SCHEMA], [FOLDER NAME], [FileName] + ' - ' + CONVERT(VARCHAR, [ModifiedDate], 120)\n",
    "    HAVING COUNT(*) > 1\n",
    ")\n",
    "SELECT * FROM FileNameCheck\n",
    "ORDER BY [SCHEMA] ASC, [FOLDER NAME] DESC, [FileName - ModifiedDate] ASC\n",
    " \n",
    "\"\"\"\n",
    "\n",
    "# ƒê·ªçc d·ªØ li·ªáu v√†o DataFrame\n",
    "Code1_Result = pl.read_database(query=sql_query, connection=engine)\n",
    "engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Data code 2\n",
    "engine = create_engine(connection_string)\n",
    " \n",
    "# C√¢u l·ªánh SQL\n",
    "\n",
    "sql_query2 = \"\"\"\n",
    "with\n",
    "-- 1. Agent Raw\n",
    "Agents_Raw1 as\n",
    "(Select Employee_ID, COUNT(*) as [Count]\n",
    "From BCOM.Staff\n",
    "Group by Employee_ID Having COUNT(*)>1),\n",
    "-- 2. AHT Raw üìù\n",
    "AHT_Raw1 as\n",
    "(Select  [Staff]+cast([Date] AS varchar)+[Language]+[Topic]+[Subtopic]+cast([Handling Time] as varchar)+[Tooltip Phone Time]+[First Reservation Id] as [Concat], COUNT(*) as [Count]\n",
    "From     BCOM.AHT\n",
    "Where    [Staff]+cast([Date] AS varchar)+[Language]+[Topic]+[Subtopic]+cast([Handling Time] as varchar)+[Tooltip Phone Time]+[First Reservation Id] Is not Null\n",
    "Group by [Staff]+cast([Date] AS varchar)+[Language]+[Topic]+[Subtopic]+cast([Handling Time] as varchar)+[Tooltip Phone Time]+[First Reservation Id] Having COUNT(*)>1),\n",
    "-- 3. Capacity HC üìù\n",
    "CapacityHC_Raw1 as\n",
    "(Select  [LOB]+cast([Date] as varchar) as [Concat], COUNT(*) as [Count]\n",
    "From     BCOM.CapHC\n",
    "Where    [LOB]+cast([Date] as varchar) is not Null\n",
    "Group by [LOB]+cast([Date] as varchar) Having COUNT(*)>1),\n",
    "-- 4. CSAT Raw üìù\n",
    "CSAT_Raw1 as\n",
    "(Select  cast([Sort by Dimension] as varchar)+[Staff]+[Type]+[Team]+[Survey Id]+[Reservation]+[Channel]+[Topic of the first Ticket]+[Language]+[Csat 2.0 Score] as [Concat], COUNT(*) as [Count]\n",
    "From     BCOM.CSAT_TP\n",
    "Where    cast([Sort by Dimension] as varchar)+[Staff]+[Type]+[Team]+[Survey Id]+[Reservation]+[Channel]+[Topic of the first Ticket]+[Language]+[Csat 2.0 Score] is not Null\n",
    "Group by cast([Sort by Dimension] as varchar)+[Staff]+[Type]+[Team]+[Survey Id]+[Reservation]+[Channel]+[Topic of the first Ticket]+[Language]+[Csat 2.0 Score] Having COUNT(*)>1),\n",
    "-- 5. CSAT Reso Raw üìù\n",
    "CSAT_Reso_Raw as\n",
    "(Select  cast([Sort by Dimension] as varchar)+[Staff]+[Type]+[Team]+[Survey Id]+[Reservation]+[Channel]+[Topic of the first Ticket]+[Language]+[Csat 2.0 Score] as [Concat], COUNT(*) as [Count]\n",
    "From     BCOM.CSAT_RS\n",
    "Where    cast([Sort by Dimension] as varchar)+[Staff]+[Type]+[Team]+[Survey Id]+[Reservation]+[Channel]+[Topic of the first Ticket]+[Language]+[Csat 2.0 Score] is not Null\n",
    "Group by cast([Sort by Dimension] as varchar)+[Staff]+[Type]+[Team]+[Survey Id]+[Reservation]+[Channel]+[Topic of the first Ticket]+[Language]+[Csat 2.0 Score] Having COUNT(*)>1),\n",
    "-- 6. CUIC Raw üìù\n",
    "CUIC_Raw1 as\n",
    "(Select [FullName]+[LoginName]+cast([Interval] as varchar)+cast([AgentLoggedOnTime] as varchar)+cast([AgentAvailTime] as varchar) as [Concat], COUNT(*) as [Count]\n",
    "From     BCOM.CUIC\n",
    "Where    [FullName]+[LoginName]+cast([Interval] as varchar)+cast([AgentLoggedOnTime] as varchar)+cast([AgentAvailTime] as varchar) is not Null\n",
    "Group by [FullName]+[LoginName]+cast([Interval] as varchar)+cast([AgentLoggedOnTime] as varchar)+cast([AgentAvailTime] as varchar) Having COUNT(*)>1),\n",
    "-- 7. EEAAO üìù\n",
    "\n",
    "-- 8. EPS Raw üìù\n",
    "EPS_Raw1 as\n",
    "(Select  [Username]+cast([Session Login] as varchar)+cast([Session Logout] as varchar)+[BPE Code]+[Session Time]+cast([Total Time] as varchar) as [Concat], COUNT(*) as [Count]\n",
    "From     BCOM.EPS\n",
    "Where    [Username]+cast([Session Login] as varchar)+cast([Session Logout] as varchar)+[BPE Code]+[Session Time]+cast([Total Time] as varchar) is not Null\n",
    "Group by [Username]+cast([Session Login] as varchar)+cast([Session Logout] as varchar)+[BPE Code]+[Session Time]+cast([Total Time] as varchar) Having COUNT(*)>1),\n",
    "-- 9. Exception Req üìù\n",
    "Exception_Req as\n",
    "(Select [Emp ID]+cast([Date (MM/DD/YYYY)] as varchar) as [Concat], COUNT(*) as [Count]\n",
    "From BCOM.ExceptionReq\n",
    "Where [Emp ID]+cast([Date (MM/DD/YYYY)] as varchar) is not null\n",
    "Group by [Emp ID]+cast([Date (MM/DD/YYYY)] as varchar) Having COUNT(*)>1),\n",
    "-- 10. HC Transfer üìù\n",
    "HC_Transfer as\n",
    "(Select  [EID]+cast([LWD] as varchar) as [Concat], COUNT(*) as [Count]\n",
    "From     BCOM.LTTransfers\n",
    "Where    [EID]+cast([LWD] as varchar) is not Null\n",
    "Group by [EID]+cast([LWD] as varchar) Having COUNT(*)>1),\n",
    "-- 11. Holiday Raw üìù\n",
    "Holiday_Raw1 as\n",
    "(Select cast([Date] as varchar) as [Concat], COUNT(*) as [Count]\n",
    "From GLB.PremHdays\n",
    "Group by cast([Date] as varchar) Having COUNT(*)>1),\n",
    "-- 12. IEX Raw üìù\n",
    "\n",
    "-- 13. IntervalReq_Raw1 üìù\n",
    "IntervalReq_Raw1 as\n",
    "(Select  [LOB]+cast([Datetime_VN] as varchar) as [Concat], COUNT(*) as [Count]\n",
    "From     BCOM.IntervalReq\n",
    "Where    [LOB]+cast([Datetime_VN] as varchar) is not Null\n",
    "Group by [LOB]+cast([Datetime_VN] as varchar) Having COUNT(*)>1),\n",
    "-- 14. KPI Targer (LOB) üìù\n",
    "LOB_Tar as\n",
    "(Select  cast([Week] as varchar)+[LOB]+[Tenure days] as [Concat], COUNT(*) as [Count]\n",
    "From     BCOM.KPI_Target\n",
    "Where    cast([Week] as varchar)+[LOB]+[Tenure days] is not Null\n",
    "Group by cast([Week] as varchar)+[LOB]+[Tenure days] Having COUNT(*)>1),\n",
    "-- 15. KPI Targer (LOB Group) üìù\n",
    "LOBGR_Tar as\n",
    "(Select  cast([Week] as varchar)+[LOB Group]+[Tenure days] as [Concat], COUNT(*) as [Count]\n",
    "From     BCOM.KPI_Target\n",
    "where    cast([Week] as varchar)+[LOB Group]+[Tenure days] is not Null And [LOB] is Null\n",
    "Group by cast([Week] as varchar)+[LOB Group]+[Tenure days] Having COUNT(*)>1),\n",
    "-- 16. Logout Count üìù\n",
    "LOGOUT_COUNT as\n",
    "(Select  [Aggregation]+cast([TimeDimension] as varchar) as [Concat], COUNT(*) as [Count]\n",
    "From     BCOM.LogoutCount\n",
    "Where    [Aggregation]+cast([TimeDimension] as varchar) is not Null\n",
    "Group by [Aggregation]+cast([TimeDimension] as varchar) Having COUNT(*)>1),\n",
    "-- 17. OT Ramco üìù\n",
    "OT_Ramco as\n",
    "(Select  cast([Date] as varchar)+[employee_code]+[OT Type] as [Concat], COUNT(*) as [Count]\n",
    "From     GLB.OT_RAMCO\n",
    "Where    cast([Date] as varchar)+[employee_code]+[OT Type] is not Null\n",
    "Group by cast([Date] as varchar)+[employee_code]+[OT Type] Having COUNT(*)>1),\n",
    "-- 18. OverTime Raw üìù\n",
    "OverTime_Raw1 as\n",
    "(Select  cast([Date] as varchar)+[Emp ID] as [Concat], COUNT(*) as [Count]\n",
    "From     BCOM.RegisteredOT\n",
    "WHere    cast([Date] as varchar)+[Emp ID] is not Null\n",
    "Group by cast([Date] as varchar)+[Emp ID] Having COUNT(*)>1),\n",
    "-- 19. PSAT üìù\n",
    "PSAT_Raw1 as\n",
    "(Select  cast([Date] as varchar)+[Survey Id]+[Has Comment]+[Channel]+[Final Topics] as [Concat], COUNT(*) as [Count]\n",
    "From     BCOM.PSAT\n",
    "Where    cast([Date] as varchar)+[Survey Id]+[Has Comment]+[Channel]+[Final Topics] Is not Null\n",
    "Group by cast([Date] as varchar)+[Survey Id]+[Has Comment]+[Channel]+[Final Topics] Having COUNT(*)>1),\n",
    "-- 20. Quality_Raw1 üìù\n",
    "Quality_Raw1 as\n",
    "(Select  cast([eval_date] as varchar)+[eval_id]+[agent_username]+[final_question_grouping]+[sections]+[template_group]+[csat_satisfied]+[tix_final_subtopic]+cast([score_n] as varchar)+cast([score_question_weight] as varchar)+[eval_language]+[eval_reference]+[csat_language_code]+[tix_final_topic] as [Concat], COUNT(*) as [Count]\n",
    "From     BCOM.Quality\n",
    "Where    cast([eval_date] as varchar)+[eval_id]+[agent_username]+[final_question_grouping]+[sections]+[template_group]+[csat_satisfied]+[tix_final_subtopic]+cast([score_n] as varchar)+cast([score_question_weight] as varchar)+[eval_language]+[eval_reference]+[csat_language_code]+[tix_final_topic] Is not Null\n",
    "Group by cast([eval_date] as varchar)+[eval_id]+[agent_username]+[final_question_grouping]+[sections]+[template_group]+[csat_satisfied]+[tix_final_subtopic]+cast([score_n] as varchar)+cast([score_question_weight] as varchar)+[eval_language]+[eval_reference]+[csat_language_code]+[tix_final_topic] Having COUNT(*)>1),\n",
    "-- 21. Ramco Raw üìù\n",
    "Ramco_Raw1 as\n",
    "(Select  cast([Date] as varchar)+[EID] as [Concat], COUNT(*) as [Count]\n",
    "From     GLB.RAMCO\n",
    "Where    cast([Date] as varchar)+[EID] is not Null\n",
    "Group by cast([Date] as varchar)+[EID] Having COUNT(*)>1),\n",
    "-- 22. RamUp HC üìù\n",
    "\n",
    "-- 23. Requirement Hours üìù\n",
    "Requirement_Hours as\n",
    "(Select  [LOB]+cast([Date] as varchar) as [Concat], COUNT(*) as [Count]\n",
    "From     BCOM.DailyReq\n",
    "Where    [LOB]+cast([Date] as varchar) is not Null\n",
    "Group by [LOB]+cast([Date] as varchar) Having COUNT(*)>1),\n",
    "-- 24. Resignation Dump üìù\n",
    "Resignation_Dump as\n",
    "(Select [Employee ID], COUNT(*) as [Count]\n",
    "From GLB.Resignation\n",
    "Group by [Employee ID] Having COUNT(*)>1),\n",
    "-- 25. RONA üìù\n",
    "RONA_Raw1 as\n",
    "(Select  [Agent]+cast([RONA] as varchar)+cast([DateTime] as varchar) as [Concat], COUNT(*) as [Count]\n",
    "From     BCOM.RONA\n",
    "where    [Agent]+cast([RONA] as varchar)+cast([DateTime] as varchar) is not null\n",
    "Group by [Agent]+cast([RONA] as varchar)+cast([DateTime] as varchar) Having COUNT(*)>1),\n",
    "-- 26. Roster Raw üìù\n",
    "Roster_Raw as\n",
    "(Select [Emp ID]+cast([Attribute] as varchar) as [Concat], COUNT(*) as [Count]\n",
    "From BCOM.ROSTER\n",
    "Where [Emp ID]+cast([Attribute] as varchar) is not Null\n",
    "Group by [Emp ID]+cast([Attribute] as varchar) Having COUNT(*)>1),\n",
    "-- 27. SC Labels üìù\n",
    "\n",
    "-- 28. Shrinkage Target üìù\n",
    "Shrinkage_Target as\n",
    "(Select  [LOB]+cast([Week] as varchar) as [Concat], COUNT(*) as [Count]\n",
    "From     BCOM.ProjectedShrink\n",
    "Where    [LOB]+cast([Week] as varchar) is not Null\n",
    "Group by [LOB]+cast([Week] as varchar) Having COUNT(*)>1),\n",
    "-- 29. Termination Dump üìù\n",
    "Termination_Dump as\n",
    "(Select [EMPLOYEE_ID]+cast([Termination Date] as varchar) as [Concat], COUNT(*) as [Count]\n",
    "From GLB.Termination\n",
    "Where [EMPLOYEE_ID]+cast([Termination Date] as varchar) is not Null\n",
    "Group by [EMPLOYEE_ID]+cast([Termination Date] as varchar) Having COUNT(*)>1),\n",
    "-- 30. Ticket Raw üìù\n",
    "Ticket_Raw1 as\n",
    "(Select  cast([Date] as varchar)+[Staff Name]+cast([Hour Interval Selected] as varchar)+[Channel]+[Item Label]+[Item ID]+['Item ID']+[Time Alert]+cast([Nr. Contacts] as varchar)+[Item Link]+[Time] as [Concat], COUNT(*) as [Count]\n",
    "From     BCOM.CPI\n",
    "Where    cast([Date] as varchar)+[Staff Name]+cast([Hour Interval Selected] as varchar)+[Channel]+[Item Label]+[Item ID]+['Item ID']+[Time Alert]+cast([Nr. Contacts] as varchar)+[Item Link]+[Time] is not Null\n",
    "Group by cast([Date] as varchar)+[Staff Name]+cast([Hour Interval Selected] as varchar)+[Channel]+[Item Label]+[Item ID]+['Item ID']+[Time Alert]+cast([Nr. Contacts] as varchar)+[Item Link]+[Time] Having COUNT(*)>1),\n",
    "-- 31. Workplan Raw üìù\n",
    "Workplan_Raw1 as\n",
    "(Select  [LOB]+[ID]+cast([DateTime_Act_Start] as varchar)+cast([DateTime_Act_End] as varchar)+cast([Act_Dur] as varchar)+[Action] as [Concat], COUNT(*) as [Count]\n",
    "From     BCOM.WpDetail\n",
    "Where    [LOB]+[ID]+cast([DateTime_Act_Start] as varchar)+cast([DateTime_Act_End] as varchar)+cast([Act_Dur] as varchar)+[Action] is not Null\n",
    "Group by [LOB]+[ID]+cast([DateTime_Act_Start] as varchar)+cast([DateTime_Act_End] as varchar)+cast([Act_Dur] as varchar)+[Action] Having COUNT(*)>1),\n",
    "-- 32. Workplan Summary Raw üìù\n",
    "Workplan_Summary as\n",
    "(Select  cast([Date] as varchar)+[LOB]+[Agent ID]+[Agent Name]+[Scheduled Activity]+cast([Length] as varchar)+cast([Percent] as varchar) as [Concat], COUNT(*) as [Count]\n",
    "From     BCOM.WpSummary\n",
    "Where    cast([Date] as varchar)+[LOB]+[Agent ID]+[Agent Name]+[Scheduled Activity]+cast([Length] as varchar)+cast([Percent] as varchar) is not Null\n",
    "Group by cast([Date] as varchar)+[LOB]+[Agent ID]+[Agent Name]+[Scheduled Activity]+cast([Length] as varchar)+cast([Percent] as varchar) Having COUNT(*)>1),\n",
    "-- 33. CSAT PEGA üìù\n",
    "\n",
    "-- 34. IPH PEGA üìù\n",
    "IPH_PEGA as\n",
    "(Select  cast([Day of Date] as varchar)+[Staff Name]+[Operator Def]+[Service Case Type New]+[Channel Def]+[Reason For No Service Case]+[Topic Def New]+[Subtopics]+[Case Id]+[Reservation Id Def]+cast([# Swivels] as varchar)+cast([Count of ServiceCase or Interaction] as varchar) as [Concat], COUNT(*) as [Count]\n",
    "From     BCOM.CPI_PEGA\n",
    "Where    cast([Day of Date] as varchar)+[Staff Name]+[Operator Def]+[Service Case Type New]+[Channel Def]+[Reason For No Service Case]+[Topic Def New]+[Subtopics]+[Case Id]+[Reservation Id Def]+cast([# Swivels] as varchar)+cast([Count of ServiceCase or Interaction] as varchar)  is not Null\n",
    "Group by cast([Day of Date] as varchar)+[Staff Name]+[Operator Def]+[Service Case Type New]+[Channel Def]+[Reason For No Service Case]+[Topic Def New]+[Subtopics]+[Case Id]+[Reservation Id Def]+cast([# Swivels] as varchar)+cast([Count of ServiceCase or Interaction] as varchar)  Having COUNT(*)>1),\n",
    "-- 35. Agents Raw(TEDNAME) üìù\n",
    "TEDNAME as\n",
    "(Select [TED Name], COUNT(*) as [Count]\n",
    "From BCOM.Staff\n",
    "Group by [TED Name] Having COUNT(*)>1),\n",
    "-- 36. OTREQ üìù\n",
    "OTREQ as\n",
    "(Select  cast([Date] as varchar)+[LOB]+[Type] as [Concat], COUNT(*) as [Count]\n",
    "From     BCOM.OTReq\n",
    "Where    cast([Date] as varchar)+[LOB]+[Type] is not Null\n",
    "Group by cast([Date] as varchar)+[LOB]+[Type] Having COUNT(*)>1),\n",
    "-- 37. PROHC üìù\n",
    "PROHC as\n",
    "(Select  cast([Date] as varchar)+[LOB] as [Concat], COUNT(*) as [Count]\n",
    "From     BCOM.ProjectedHC\n",
    "Where    cast([Date] as varchar)+[LOB] is not Null\n",
    "Group by cast([Date] as varchar)+[LOB] Having COUNT(*)>1)\n",
    "--\n",
    "--------------------------------------------------------[Nvidia]Prcess---------------------------------------------------------------\n",
    "--\n",
    "------[üì•]--(üëâÔæü„ÉÆÔæü)üëâ                   Agents_Raw1\n",
    "Select    '01' as [No.],    Count(*) as [CheckDup],    'Agents Raw' as [Table]            ,    'IMPORTANT' as [Note]\n",
    "From       Agents_Raw1                    UNION ALL\n",
    "------[üì•]--(üëâÔæü„ÉÆÔæü)üëâ                   AHT_Raw1\n",
    "Select    '02' as [No.],    Count(*) as [CheckDup],    'AHT Raw' as [Table]               ,    '' as [Note]\n",
    "From       AHT_Raw1                       UNION ALL\n",
    "------[üì•]--(üëâÔæü„ÉÆÔæü)üëâ                   CapacityHC_Raw1\n",
    "Select    '03' as [No.],    Count(*) as [CheckDup],    'Capacity HC' as [Table]           ,    '' as [Note]\n",
    "From       CapacityHC_Raw1                UNION ALL\n",
    "------[üì•]--(üëâÔæü„ÉÆÔæü)üëâ                   CSAT_Raw1\n",
    "Select    '04' as [No.],    Count(*) as [CheckDup],    'CSAT Raw' as [Table]              ,    '' as [Note]\n",
    "From       CSAT_Raw1                      UNION ALL\n",
    "------[üì•]--(üëâÔæü„ÉÆÔæü)üëâ                   CSAT_Reso_Raw\n",
    "Select    '05' as [No.],    Count(*) as [CheckDup],    'CSAT Reso Raw' as [Table]         ,    '' as [Note]\n",
    "From       CSAT_Reso_Raw                 UNION ALL\n",
    "------[üì•]--(üëâÔæü„ÉÆÔæü)üëâ                   CUIC_Raw1\n",
    "Select    '06' as [No.],    Count(*) as [CheckDup],    'CUIC Raw' as [Table]              ,    '' as [Note]\n",
    "From       CUIC_Raw1                      UNION ALL\n",
    "------[üì•]--(üëâÔæü„ÉÆÔæü)üëâ                   EPS_Raw1\n",
    "Select    '08' as [No.],    Count(*) as [CheckDup],    'EPS Raw' as [Table]               ,    '' as [Note]\n",
    "From       EPS_Raw1                       UNION ALL\n",
    "------[üì•]--(üëâÔæü„ÉÆÔæü)üëâ                   Exception_Req\n",
    "Select    '09' as [No.],    Count(*) as [CheckDup],    'Exception Req' as [Table]         ,    '' as [Note]\n",
    "From       Exception_Req                 UNION ALL\n",
    "------[üì•]--(üëâÔæü„ÉÆÔæü)üëâ                   HC_Transfer\n",
    "Select    '10' as [No.],    Count(*) as [CheckDup],    'HC Transfer' as [Table]           ,    '' as [Note]\n",
    "From       HC_Transfer                   UNION ALL\n",
    "------[üì•]--(üëâÔæü„ÉÆÔæü)üëâ                   Holiday_Raw1\n",
    "Select    '11' as [No.],    Count(*) as [CheckDup],    'Holiday Raw' as [Table]           ,    '' as [Note]\n",
    "From       Holiday_Raw1                   UNION ALL\n",
    "------[üì•]--(üëâÔæü„ÉÆÔæü)üëâ                   IntervalReq_Raw1\n",
    "Select    '13' as [No.],    Count(*) as [CheckDup],    'IntervalReq' as [Table]       ,    'IMPORTANT' as [Note]\n",
    "From       IntervalReq_Raw1               UNION ALL\n",
    "------[üì•]--(üëâÔæü„ÉÆÔæü)üëâ                   LOB_Tar\n",
    "Select    '14' as [No.],    Count(*) as [CheckDup],    'KPI Targer (LOB)' as [Table]      ,    'IMPORTANT' as [Note]\n",
    "From       LOB_Tar                       UNION ALL\n",
    "------[üì•]--(üëâÔæü„ÉÆÔæü)üëâ                   LOBGR_Tar\n",
    "Select    '15' as [No.],    Count(*) as [CheckDup],    'KPI Targer (LOB Group)' as [Table],    'IMPORTANT' as [Note]\n",
    "From       LOBGR_Tar                     UNION ALL\n",
    "------[üì•]--(üëâÔæü„ÉÆÔæü)üëâ                   LOGOUT_COUNT\n",
    "Select    '16' as [No.],    Count(*) as [CheckDup],    'Logout Count' as [Table]          ,    'IMPORTANT' as [Note]\n",
    "From       LOGOUT_COUNT                  UNION ALL\n",
    "------[üì•]--(üëâÔæü„ÉÆÔæü)üëâ                   OT_Ramco\n",
    "Select    '17' as [No.],    Count(*) as [CheckDup],    'OT Ramco' as [Table]              ,    'IMPORTANT' as [Note]\n",
    "From       OT_Ramco                      UNION ALL\n",
    "------[üì•]--(üëâÔæü„ÉÆÔæü)üëâ                   OverTime_Raw1\n",
    "Select    '18' as [No.],    Count(*) as [CheckDup],    'OverTime Raw' as [Table]          ,    'IMPORTANT' as [Note]\n",
    "From       OverTime_Raw1                  UNION ALL\n",
    "------[üì•]--(üëâÔæü„ÉÆÔæü)üëâ                   PSAT_Raw1\n",
    "Select    '19' as [No.],    Count(*) as [CheckDup],    'PSAT' as [Table]                  ,    '' as [Note]\n",
    "From       PSAT_Raw1                      UNION ALL\n",
    "------[üì•]--(üëâÔæü„ÉÆÔæü)üëâ                   Quality_Raw1\n",
    "Select    '20' as [No.],    Count(*) as [CheckDup],    'Quality_Raw' as [Table]           ,    '' as [Note]\n",
    "From       Quality_Raw1                   UNION ALL\n",
    "------[üì•]--(üëâÔæü„ÉÆÔæü)üëâ                   Ramco_Raw1\n",
    "Select    '21' as [No.],    Count(*) as [CheckDup],    'Ramco Raw' as [Table]             ,    'IMPORTANT' as [Note]\n",
    "From       Ramco_Raw1                     UNION ALL\n",
    "------[üì•]--(üëâÔæü„ÉÆÔæü)üëâ                   Requirement_Hours\n",
    "Select    '23' as [No.],    Count(*) as [CheckDup],    'Daily Requirement' as [Table]     ,    'IMPORTANT' as [Note]\n",
    "From       Requirement_Hours             UNION ALL\n",
    "------[üì•]--(üëâÔæü„ÉÆÔæü)üëâ                   Resignation_Dump\n",
    "Select    '24' as [No.],    Count(*) as [CheckDup],    'Resignation Dump' as [Table]      ,    '' as [Note]\n",
    "From       Resignation_Dump              UNION ALL\n",
    "------[üì•]--(üëâÔæü„ÉÆÔæü)üëâ                   RONA_Raw1\n",
    "Select    '25' as [No.],    Count(*) as [CheckDup],    'RONA' as [Table]                  ,    '' as [Note]\n",
    "From       RONA_Raw1                      UNION ALL\n",
    "------[üì•]--(üëâÔæü„ÉÆÔæü)üëâ                   Roster_Raw\n",
    "Select    '26' as [No.],    Count(*) as [CheckDup],    'Roster Raw' as [Table]            ,    'IMPORTANT' as [Note]\n",
    "From       Roster_Raw                    UNION ALL\n",
    "------[üì•]--(üëâÔæü„ÉÆÔæü)üëâ                   Shrinkage_Target\n",
    "Select    '28' as [No.],    Count(*) as [CheckDup],    'Shrinkage Target' as [Table]      ,    '' as [Note]\n",
    "From       Shrinkage_Target              UNION ALL\n",
    "------[üì•]--(üëâÔæü„ÉÆÔæü)üëâ                   Termination_Dump\n",
    "Select    '29' as [No.],    Count(*) as [CheckDup],    'Termination Dump' as [Table]      ,    '' as [Note]\n",
    "From       Termination_Dump              UNION ALL\n",
    "------[üì•]--(üëâÔæü„ÉÆÔæü)üëâ                   Ticket_Raw1\n",
    "Select    '30' as [No.],    Count(*) as [CheckDup],    'Ticket Raw' as [Table]            ,    '' as [Note]\n",
    "From       Ticket_Raw1                    UNION ALL\n",
    "------[üì•]--(üëâÔæü„ÉÆÔæü)üëâ                   Workplan_Raw1\n",
    "Select    '31' as [No.],    Count(*) as [CheckDup],    'Workplan Raw' as [Table]          ,    '' as [Note]\n",
    "From       Workplan_Raw1                  UNION ALL\n",
    "------[üì•]--(üëâÔæü„ÉÆÔæü)üëâ                   Workplan_Summary\n",
    "Select    '32' as [No.],    Count(*) as [CheckDup],    'Workplan Summary Raw' as [Table]  ,    '' as [Note]\n",
    "From       Workplan_Summary              UNION ALL\n",
    "------[üì•]--(üëâÔæü„ÉÆÔæü)üëâ                   IPH_PEGA\n",
    "Select    '34' as [No.],    Count(*) as [CheckDup],    'IPH_PEGA' as [Table]              ,    '' as [Note]\n",
    "From       IPH_PEGA                     UNION ALL     \n",
    "------[üì•]--(üëâÔæü„ÉÆÔæü)üëâ                   TED Name\n",
    "Select    '35' as [No.],    Count(*) as [CheckDup],    'TEDNAME' as [Table]              ,    'IMPORTANT' as [Note]\n",
    "From       TEDNAME                      UNION ALL\n",
    "------[üì•]--(üëâÔæü„ÉÆÔæü)üëâ                   TED Name\n",
    "Select    '36' as [No.],    Count(*) as [CheckDup],    'OTREQ' as [Table]              ,      'IMPORTANT' as [Note]\n",
    "From       OTREQ                        UNION ALL\n",
    "------[üì•]--(üëâÔæü„ÉÆÔæü)üëâ                   TED Name\n",
    "Select    '37' as [No.],    Count(*) as [CheckDup],    'PROHC' as [Table]              ,      'IMPORTANT' as [Note]\n",
    "From       PROHC\n",
    "--          \n",
    "\"\"\"\n",
    "# ƒê·ªçc d·ªØ li·ªáu v√†o DataFrame\n",
    "Code2_Result = pl.read_database(query=sql_query2, connection=engine)\n",
    "engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source collection\n",
    "user_credential = os.path.join(os.environ['USERPROFILE'],r'Concentrix Corporation//CNXVN - WFM Team - Documents//')\n",
    "\n",
    "# INPUT-----üíæ-----üíæ-----üíæ-----üíæ-----üíæ-----üíæ-----üíæ-----üíæ-----üíæ-----üíæ\n",
    "# [BKN]Error Log\n",
    "LOG_LINK = os.path.join(user_credential,\n",
    "                                r'DataBase//DataRaw//BKN//MODIFIED_LOG//*.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#DEFINITION\n",
    "#Name Pattern definition\n",
    "def Namepattern(folder_name):\n",
    "    match folder_name:\n",
    "        case \"Staff\":\n",
    "            pattern = r\"CNX Global Master Roster.xlsx\"\n",
    "        case \"ProjectedShrink\":\n",
    "            pattern = r\"IO Shrinkage.xlsx\"\n",
    "        case \"LTTransfers\":\n",
    "            pattern = r\"transfer.xlsx\"\n",
    "        case \"KPI_Target\":\n",
    "            pattern = r\"kpi_target.xlsx\"\n",
    "        case \"EPS\":\n",
    "            pattern = r\"EPS Tableau - (\\d{8})\\.csv\"\n",
    "        case \"DailyReq\":\n",
    "            pattern = r\"(\\d{6})(_(\\d{6}))?\\..{4}\"\n",
    "        case \"IntervalReq\":\n",
    "            pattern = r\"(\\d{6,8})(_(\\d{6,8}))?\\..{4}\"   \n",
    "        case \"Contrack\":\n",
    "            pattern = r\"(\\[WFM\\] Contact Tracker|(\\d{8})(_(\\d{8}))?|W(\\d{2})-(\\d{4}))\\..{4}\"  \n",
    "        case \"CapHC\":\n",
    "            pattern = r\"(\\d{4})(_(\\d{4}))\\..{4}\"  \n",
    "        case \"AHT\":\n",
    "            pattern = r\"(\\d{8})_(\\d{8})_Items_data\\..{4}\"  \n",
    "        case \"Termination\":\n",
    "            pattern = r\"WDD.xlsx\" \n",
    "        case \"Resignation\":\n",
    "            pattern = r\"WDD.xlsx\" \n",
    "        case \"PremHdays\":\n",
    "            pattern = r\"Holiday Mapping.csv\" \n",
    "        case \"NormHdays\":\n",
    "            pattern = r\"Holiday Nonbillable.csv\"\n",
    "        case \"EmpMaster\":\n",
    "            pattern = r\"WDD.xlsx\"\n",
    "        case \"CUIC_RTMonitor\":\n",
    "            pattern = r\"00_RTA_View-Agent Team Real Time.xlsx\"\n",
    "        case \"CUIC\":\n",
    "            pattern = r\"(\\d{4,8})_(\\d{2,8})(_(\\d{2}))?(_\\d)?\\..{4}\"\n",
    "        case _:\n",
    "            pattern = r\"(\\d{8})(_(\\d{8}))?\\..{3,4}\"\n",
    "    return pattern\n",
    "\n",
    "#table1\n",
    "Folder_column_name = 'FOLDER NAME'\n",
    "Name_Coumn_name = 'FileName - ModifiedDate'\n",
    "Row_column_name = 'ROW_NUMBER'\n",
    "#table2\n",
    "important_column= 'Note'\n",
    "Check_dup_column = 'CheckDup'\n",
    "Table_column = 'Table'\n",
    "#log file\n",
    "Error_log_column ='Error'\n",
    "Log_name_column='FileName'\n",
    "\n",
    "#Exception list\n",
    "Expection_list=[\n",
    "    \"EPS Tableau - 20240831\",\n",
    "    \"20231001_20231231\",\n",
    "    \"20220919_20230930\"\n",
    "    ]\n",
    "Exception_pattern = r\"\\..{3,4}\"\n",
    "#Color code\n",
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKCYAN = '\\033[96m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    FAILVIOLET = '\\033[35m'\n",
    "\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Func Compare row for the rest\n",
    "def Row_Compare_Special(Dataframe,Extract_file):\n",
    "    rowdata = Dataframe[Row_column_name].to_list()\n",
    "    # Standard Deviation Math\n",
    "    mean = statistics.mean(rowdata)  \n",
    "    std_dev = statistics.stdev(rowdata) \n",
    "#Tweak the sensitivity of the comparison, lower = more strict\n",
    "    tolerance = 2.5\n",
    "    catch_list=[]\n",
    "    for row in rowdata:\n",
    "         distance = abs(row -mean)\n",
    "         if distance > tolerance * std_dev:\n",
    "              catch_list.append(row)\n",
    "\n",
    "              \n",
    "    if not(catch_list):\n",
    "         return\n",
    "\n",
    "    # Filter the file has been mark out\n",
    "    Sus_file = Dataframe.filter(pl.col(Row_column_name).is_in(catch_list))\n",
    "    for row in Sus_file.iter_rows(named= True):\n",
    "        check =re.split(Exception_pattern,row[Name_Coumn_name])\n",
    "        if check[0] not in Expection_list:\n",
    "            Extract_file.append((row[Name_Coumn_name],row[Folder_column_name],\"Inconsistent row\"))\n",
    "            print(f'{bcolors.WARNING}File {row[Name_Coumn_name]} in {row[Folder_column_name]} has inconsistent number of rows {bcolors.ENDC}')\n",
    "            \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Func Compare Row with pattern\n",
    "def Row_Compare(Dataframe,Extract_file,Folder):\n",
    "    desired_pattern = r\"(\\d{8})_(\\d{8})\"\n",
    "    FilteredDataframe= Dataframe.sort(Name_Coumn_name,descending=False)\n",
    "    rowdivdate =[]\n",
    "    namefilter=[]\n",
    "    row_avg = []\n",
    "    #calculate average number for row in one day we should expected\n",
    "    for row  in FilteredDataframe.iter_rows(named= True):\n",
    "         \n",
    "        match = re.match(desired_pattern,row[Name_Coumn_name])\n",
    "        start_date_string,end_date_start_date_string = match.groups()\n",
    "        try:\n",
    "            start_date = dt.strptime(start_date_string,\"%Y%m%d\").date()\n",
    "            end_date = dt.strptime(end_date_start_date_string,\"%Y%m%d\").date()\n",
    "            #average number of rows should have per day\n",
    "            namefilter.append(row[Name_Coumn_name])\n",
    "            row_avg.append((row[Row_column_name]/((end_date - start_date).days)))\n",
    "        \n",
    "        #Find out if file has the same start date and end date  \n",
    "        except ZeroDivisionError:\n",
    "            print(f\"{bcolors.OKBLUE}File {row[Name_Coumn_name]} in {Folder} has the same start and end date{bcolors.ENDC}\")\n",
    "            Extract_file.append((row[Name_Coumn_name],Folder,\"Same Start and End date\"))\n",
    "            continue\n",
    "\n",
    "    namefilter_series = pl.Series(\"File_name\", namefilter)\n",
    "    row_avg_series = pl.Series(\"Average_Row\", row_avg)\n",
    "    #If none calculated, return\n",
    "    if len(namefilter)<2:\n",
    "        return\n",
    "\n",
    "     #Create Dataframe\n",
    "\n",
    "    dffinal = pl.DataFrame([namefilter_series,row_avg_series])\n",
    "    rowdata = dffinal['Average_Row'].to_list()\n",
    "    # Standard Deviation Math\n",
    "    mean = statistics.mean(rowdata)  \n",
    "    std_dev = statistics.stdev(rowdata) \n",
    "    #Tweak the sensitivity of the comparison, lower = more strict\n",
    "    tolerance = 2.5\n",
    "    catch_list=[]\n",
    "\n",
    "    for row in rowdata:\n",
    "        distance = abs(row -mean)\n",
    "        if distance > tolerance * std_dev:\n",
    "            catch_list.append(row)\n",
    "\n",
    "    \n",
    "    if not(catch_list):\n",
    "        return\n",
    "     \n",
    "     # Filter the file has been mark out\n",
    "    Sus_file = dffinal.filter(pl.col('Average_Row').is_in(catch_list))\n",
    "    for row in Sus_file.iter_rows(named= True):\n",
    "        check =re.split(Exception_pattern,row['File_name'])\n",
    "        if check[0] not in Expection_list:\n",
    "            Extract_file.append((row['File_name'],Folder,\"Inconsistent average row\"))\n",
    "            print(f'{bcolors.FAIL}File {row['File_name']} in {Folder} has inconsistent number of rows {bcolors.ENDC}')\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#Func Duplicate date check\n",
    "def Duplicate_Date_Check(list_name,extract_list,folder):\n",
    "    file_ranges = []\n",
    "    Valid_list = []\n",
    "    desired_pattern = r\"(\\d{8})_(\\d{8})\"\n",
    "    \n",
    "    for file_name in list_name:\n",
    "        match = re.match(desired_pattern,file_name)\n",
    "        if match:\n",
    "                start_date_string,end_date_start_date_string = match.groups()\n",
    "                try:\n",
    "                    start_date = dt.strptime(start_date_string,\"%Y%m%d\").date()\n",
    "                    end_date = dt.strptime(end_date_start_date_string,\"%Y%m%d\").date()\n",
    "                    #Put date into a list to compare\n",
    "                    file_ranges.append((start_date,end_date,file_name))\n",
    "                    # Valid file will be use to further check\n",
    "                    Valid_list.append(file_name)\n",
    "                except ValueError:\n",
    "                    print(f\"{bcolors.OKBLUE} {file_name} has invalid name format in {folder}, cannot compare date for further check{bcolors.ENDC}\")\n",
    "\n",
    "    file_ranges.sort()\n",
    "    \n",
    "    for i in range(len(file_ranges)):\n",
    "        start_i, end_i, file_i = file_ranges[i]\n",
    "        overlap_count =0\n",
    "        for j in range(len(file_ranges)):\n",
    "            if i!= j:\n",
    "                start_j, end_j, file_j = file_ranges[j]\n",
    "                #Check if current file is totally in another file\n",
    "                if start_j <= start_i <= end_j and start_j <= end_i <= end_j:\n",
    "                    extract_list.append((file_i,folder,\"Duplicate file\"))\n",
    "                    print(f'{bcolors.FAIL}Duplicate date file: {file_i}{bcolors.ENDC}')\n",
    "                else:\n",
    "                # Check if currentfile has a part in another file\n",
    "                    if start_i <= end_j and end_i >= start_j:\n",
    "                        overlap_count +=1\n",
    "                if overlap_count >=2:\n",
    "                    extract_list.append((file_i,folder,\"Has overlap days\"))\n",
    "                    print(f'{bcolors.WARNING}File might have issue with overlapping date: {file_i}{bcolors.ENDC}')\n",
    "\n",
    "                    break\n",
    "    \n",
    "    return Valid_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Func Check update \n",
    "\n",
    "def File_need_update_check(list_name,extract_list,Folder):\n",
    "    #pattern to get the modified date\n",
    "    time_pattern = r\"(\\d{4})\\-(\\d{1,2})\\-(\\d{1,2})\"\n",
    "    current_date = dt.now()\n",
    "    #flag to report\n",
    "    Most_recent_CUIC_file = 1\n",
    "    Most_recent_EPS_file = 5\n",
    "    Most_recent_Quality_file = 5\n",
    "    Most_recent_WPsummary_file = 7\n",
    "    Most_recent_IEX_hours_file = 7\n",
    "    Most_recent_WpDetail_file = 7\n",
    "    \n",
    "\n",
    "    for name in list_name:\n",
    "        #Check last time file was modified by seaching the pattern, assume that file is up to date\n",
    "        match = re.search(time_pattern,name)\n",
    "        How_Long_have_not_update=0\n",
    "        if match:\n",
    "            year_string,month_string,day_string = match.groups()\n",
    "            last_edit = dt(int(year_string),int(month_string),int(day_string))\n",
    "            How_Long_have_not_update = (current_date-last_edit).days\n",
    "        \n",
    "        #Threshhold for each type of folder\n",
    "        match Folder:\n",
    "            case 'ExceptionReq':\n",
    "                if  How_Long_have_not_update >1:\n",
    "                    print(f'{bcolors.OKCYAN}File {name} in {Folder} might need to update{bcolors.ENDC}')\n",
    "                    extract_list.append((name,Folder,\"May need update\"))           \n",
    "            #Update CPI\n",
    "            case 'CPI':\n",
    "                How_old_is_this_file =4\n",
    "                # Calculate how old is this file and should it be neccessary to update it\n",
    "                yyyymmdd_pattern = r'(\\d{8})'\n",
    "                matchformat = re.search(yyyymmdd_pattern,name)\n",
    "                if matchformat:\n",
    "                    start_date_string = matchformat.group()\n",
    "                    start_date = dt.strptime(start_date_string,\"%Y%m%d\")\n",
    "                    How_old_is_this_file = (current_date-start_date).days\n",
    "\n",
    "                if  How_old_is_this_file < 3 and How_Long_have_not_update >=1:\n",
    "                        print(f'{bcolors.OKCYAN}File {name} in {Folder} might need to update{bcolors.ENDC}')\n",
    "                        extract_list.append((name,Folder,\"May need update\"))\n",
    "            #Update CUIC\n",
    "            case 'CUIC':\n",
    "                How_old_is_this_file =4\n",
    "                # Calculate how old is this file and should it be neccessary to update it\n",
    "              \n",
    "                yyyy_dd_mm_pattern = r'(\\d{4})_(\\d{2})_(\\d{2})'\n",
    "                matchformat = re.match(yyyy_dd_mm_pattern,name)\n",
    "                if matchformat:\n",
    "                    year_s,mon_s,day_s = matchformat.groups()\n",
    "                    start_2 = dt(int(year_s),int(mon_s),int(day_s))\n",
    "                    How_old_is_this_file = (current_date-start_2).days\n",
    "\n",
    "                    #Flag to check for the newest file\n",
    "                    if How_old_is_this_file <=0:\n",
    "                        Most_recent_CUIC_file = 0\n",
    "\n",
    "                if  How_old_is_this_file < 3 and How_Long_have_not_update >1:\n",
    "                        print(f'{bcolors.OKCYAN}File {name} in {Folder} might need to update{bcolors.ENDC}')\n",
    "                        extract_list.append((name,Folder,\"May need update\"))\n",
    "\n",
    "            #update 1-3 days\n",
    "            case 'CSAT_RS'|'CSAT_TP'|'LogoutCount'|'PSAT'|'RAMCO'|'ROSTER'|'AHT':\n",
    "\n",
    "                # Calculate how old is this file and should it be neccessary to update it\n",
    "                start_end_pattern = r'(\\d{8})_(\\d{8})'\n",
    "                How_old_is_this_file =8\n",
    "                matchformat = re.search(start_end_pattern,name)\n",
    "                \n",
    "                if matchformat:\n",
    "                    start_date_string,end_date_string = matchformat.groups()\n",
    "                    end_date = dt.strptime(end_date_string,\"%Y%m%d\")\n",
    "                    How_old_is_this_file = (current_date-end_date).days\n",
    "\n",
    "                    if  How_Long_have_not_update > 2 and How_old_is_this_file <4:  \n",
    "                        print(f'{bcolors.OKCYAN}File {name} in {Folder} might need to update{bcolors.ENDC}')\n",
    "                        extract_list.append((name,Folder,\"May need update\"))\n",
    "\n",
    "            #update weekly\n",
    "            case 'CPI_PEGA'|'OT_RAMCO'|'OTReq'|'RegisteredOT'|'RONA':\n",
    "                # Calculate how old is this file and should it be neccessary to update it\n",
    "                start_end_pattern = r'(\\d{8})_(\\d{8})'\n",
    "                How_old_is_this_file =8\n",
    "                matchformat = re.search(start_end_pattern,name)\n",
    "                if matchformat:\n",
    "                    start_date_string,end_date_string = matchformat.groups()\n",
    "                    end_date = dt.strptime(end_date_string,\"%Y%m%d\")\n",
    "                    How_old_is_this_file = (current_date-end_date).days\n",
    "                \n",
    "                    if  How_Long_have_not_update > 5 and How_old_is_this_file <7:  \n",
    "                        print(f'{bcolors.OKCYAN}File {name} in {Folder} might need to update{bcolors.ENDC}')\n",
    "                        extract_list.append((name,Folder,\"May need update\"))\n",
    "\n",
    "            #update Contract\n",
    "            case 'Contrack':\n",
    "                # Calculate how old is this file and should it be neccessary to update it\n",
    "                wfm_pattern = r'[WFM] Contact Tracker'\n",
    "                matchformat = re.search(wfm_pattern,name)  \n",
    "                if How_Long_have_not_update> 2 and matchformat:  \n",
    "                    print(f'{bcolors.OKCYAN}File {name} in {Folder} might need to update{bcolors.ENDC}')\n",
    "                    extract_list.append((name,Folder,\"May need update\"))\n",
    "        \n",
    "            #Update IEX_Hrs\n",
    "            case 'IEX_Hrs':\n",
    "                # Calculate how old is this file and should it be neccessary to update it\n",
    "                yyyymmdd_pattern = r'(\\d{8})'\n",
    "                matchformat = re.search(yyyymmdd_pattern,name)                \n",
    "                How_old_is_this_file = 7\n",
    "                if matchformat:\n",
    "                    start_date_string = matchformat.group()\n",
    "                    start_date = dt.strptime(start_date_string,\"%Y%m%d\")\n",
    "                    How_old_is_this_file = (current_date-start_date).days\n",
    "                # Flag the folder if the file is not newest\n",
    "                    if  How_old_is_this_file < Most_recent_IEX_hours_file:  \n",
    "                        Most_recent_IEX_hours_file= How_old_is_this_file\n",
    "\n",
    "            #update monthly\n",
    "            case 'CapHC'|'DailyReq'|'IntervalReq'|'KPI_Target'|'LTTransfers'|'ProjectedHC':\n",
    "                if  How_Long_have_not_update > 30:  \n",
    "                    print(f'{bcolors.OKCYAN}File {name} in {Folder} might need to update{bcolors.ENDC}')\n",
    "                    extract_list.append((name,Folder,\"May need update\"))\n",
    "\n",
    "            #update yearly\n",
    "            case 'PremHdays'|'NormHdays':\n",
    "                if How_Long_have_not_update > 364:\n",
    "                    print(f'{bcolors.OKCYAN}File {name} in {Folder} might need to update{bcolors.ENDC}')\n",
    "                    extract_list.append((name,Folder,\"May need update\"))\n",
    "\n",
    "            #update WDD/ProjectedShrink/Staff\n",
    "            case 'EmpMaster'|'Resignation'|'Termination'|'ProjectedShrink'|'Staff':\n",
    "                if How_Long_have_not_update > 8 and Folder != 'ProjectedShrink' :\n",
    "                    print(f'{bcolors.OKCYAN}File {name} in {Folder} might need to update{bcolors.ENDC}')\n",
    "                    extract_list.append((name,Folder,\"May need update\"))\n",
    "\n",
    "            #Update EPS\n",
    "            case 'EPS':\n",
    "                How_old_is_this_file =5\n",
    "                yyyymmdd_pattern = r'(\\d{8})'\n",
    "                matchformat = re.search(yyyymmdd_pattern,name)\n",
    "                if matchformat:\n",
    "                    start_date_string = matchformat.group()\n",
    "                    start_date = dt.strptime(start_date_string,\"%Y%m%d\")\n",
    "                    How_old_is_this_file = (current_date-start_date).days\n",
    "                # Flag the folder if the file is not newest\n",
    "                    if How_old_is_this_file < Most_recent_EPS_file:\n",
    "                        Most_recent_EPS_file = How_old_is_this_file\n",
    "                \n",
    "                if  How_old_is_this_file < 4 and How_Long_have_not_update >=3:\n",
    "                        print(f'{bcolors.OKCYAN}File {name} in {Folder} might need to update{bcolors.ENDC}')\n",
    "                        extract_list.append((name,Folder,\"May need update\"))\n",
    "\n",
    "            #Update Quality\n",
    "            case 'Quality':\n",
    "                How_old_is_this_file =5\n",
    "                yyyymmdd_pattern = r'(\\d{8})'\n",
    "                matchformat = re.search(yyyymmdd_pattern,name)\n",
    "                if matchformat:\n",
    "                    start_date_string = matchformat.group()\n",
    "                    start_date = dt.strptime(start_date_string,\"%Y%m%d\")\n",
    "                    How_old_is_this_file = (current_date-start_date).days\n",
    "                # Flag the folder if the file is not newest\n",
    "                    if How_old_is_this_file < Most_recent_Quality_file:\n",
    "                        Most_recent_Quality_file = How_old_is_this_file\n",
    "\n",
    "                if  How_old_is_this_file < 4 and How_Long_have_not_update >=3:\n",
    "                        print(f'{bcolors.OKCYAN}File {name} in {Folder} might need to update{bcolors.ENDC}')\n",
    "                        extract_list.append((name,Folder,\"May need update\"))\n",
    "\n",
    "            #Update WPSUMA\n",
    "            case 'WpSummary':\n",
    "                How_old_is_this_file =7\n",
    "                yyyymmdd_pattern = r'(\\d{8})'\n",
    "                matchformat = re.search(yyyymmdd_pattern,name)\n",
    "                if matchformat:\n",
    "                    start_date_string = matchformat.group()\n",
    "                    start_date = dt.strptime(start_date_string,\"%Y%m%d\")\n",
    "                    How_old_is_this_file = (current_date-start_date).days\n",
    "                # Flag the folder if the file is not newest\n",
    "                    if How_old_is_this_file < Most_recent_WPsummary_file:\n",
    "                        Most_recent_WPsummary_file = How_old_is_this_file\n",
    "                if  How_old_is_this_file < 7 and How_Long_have_not_update >=2:\n",
    "                        print(f'{bcolors.OKCYAN}File {name} in {Folder} might need to update{bcolors.ENDC}')\n",
    "                        extract_list.append((name,Folder,\"May need update\"))\n",
    "\n",
    "            #Update WpDetail\n",
    "            case 'WpDetail':\n",
    "                start_end_pattern = r'(\\d{8})_(\\d{8})'\n",
    "                How_old_is_this_file =7\n",
    "                matchformat = re.search(start_end_pattern,name)\n",
    "                if matchformat:\n",
    "                    start_date_string,end_date_string = matchformat.groups()\n",
    "                    end_date = dt.strptime(end_date_string,\"%Y%m%d\")\n",
    "                    How_old_is_this_file = (current_date-end_date).days\n",
    "                else:\n",
    "                    extra_pattern = r'(\\d{8})'\n",
    "                    matchformat = re.search(extra_pattern,name)\n",
    "                    if matchformat:\n",
    "                        start_date_string = matchformat.group()\n",
    "                        end_date = dt.strptime(start_date_string,\"%Y%m%d\")\n",
    "                        How_old_is_this_file = (current_date-end_date).days\n",
    "\n",
    "\n",
    "                # Flag the folder if the file is not newest\n",
    "                if  How_old_is_this_file < Most_recent_WpDetail_file:\n",
    "                    Most_recent_WpDetail_file= How_old_is_this_file\n",
    "  \n",
    "            #folder not found\n",
    "            case _:\n",
    "                print(f'{bcolors.WARNING}{Folder} is not expected for the update check {bcolors.ENDC}')\n",
    "                return\n",
    "    \n",
    "    # Raise notify base on flag \n",
    "    if Most_recent_CUIC_file!=0 and Folder==\"CUIC\":\n",
    "        print(f'{bcolors.OKCYAN}{Folder} need to update the newest data {bcolors.ENDC}')\n",
    "        extract_list.append((\"Missing Today File\",Folder,\"Need update\"))\n",
    "    if Most_recent_EPS_file>=3 and Folder==\"EPS\":\n",
    "        print(f'{bcolors.OKCYAN}{Folder} need to update the newest data {bcolors.ENDC}')\n",
    "        extract_list.append((\"Missing This Week File\",Folder,\"Need update\"))\n",
    "    if Most_recent_Quality_file>=3 and Folder==\"Quality\":\n",
    "        print(f'{bcolors.OKCYAN}{Folder} need to update the newest data {bcolors.ENDC}')\n",
    "        extract_list.append((\"Missing This Week File\",Folder,\"Need update\"))\n",
    "    if Most_recent_WPsummary_file >6 and Folder==\"WpSummary\":\n",
    "        print(f'{bcolors.OKCYAN}{Folder} need to update the newest data {bcolors.ENDC}')\n",
    "        extract_list.append((\"Missing This Week File\",Folder,\"Need update\"))\n",
    "    if Most_recent_IEX_hours_file >6 and Folder==\"IEX_Hrs\":\n",
    "        print(f'{bcolors.OKCYAN}{Folder} need to update the newest data {bcolors.ENDC}')\n",
    "        extract_list.append((\"Missing This Week File\",Folder,\"Need update\"))\n",
    "    if Most_recent_WpDetail_file >6 and Folder==\"WpDetail\":\n",
    "        print(f'{bcolors.OKCYAN}{Folder} need to update the newest data {bcolors.ENDC}')\n",
    "        extract_list.append((\"Missing This Week File\",Folder,\"Need update\"))    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Func CODE 1\n",
    "def CategoryCheck (your_dataframe,file_error):\n",
    "     #Check each folder \n",
    "     for folder_name in your_dataframe[Folder_column_name].unique():\n",
    "               #Current folder checking\n",
    "               current_check = your_dataframe.filter(pl.col(Folder_column_name) == folder_name)\n",
    "               current_extract_grp=[]\n",
    "               #Check naming rule\n",
    "               for file_name in current_check[Name_Coumn_name]:\n",
    "                    match = re.match(Namepattern(folder_name),file_name)\n",
    "                    if match:\n",
    "                          #Extract valid file name to check further\n",
    "                        current_extract_grp.append(f\"{file_name}\")\n",
    "                    else:\n",
    "                        print(f\"{bcolors.FAILVIOLET}Weird Name Pattern at {file_name} in folder {folder_name}{bcolors.ENDC}\")\n",
    "                        current_extract_grp.append(f\"{file_name}\")\n",
    "                        file_error.append((file_name,folder_name,\"Weird Name\"))\n",
    "\n",
    "               #Check update\n",
    "               File_need_update_check(current_extract_grp,file_error,folder_name)\n",
    "               \n",
    "               #Check Duplicate Date, if current folder only have 1 file, skip this\n",
    "               filter_file = []\n",
    "               if current_check.count()[0,1]>1:\n",
    "                    filter_file = Duplicate_Date_Check(current_extract_grp,file_error,folder_name)\n",
    "                    \n",
    "               #Deviation in row check using valid list with format yyyymmdd_yyyymmdd from previous func\n",
    "               if filter_file:\n",
    "                    filtered_DF_standard_pattern = current_check.filter(pl.col(Name_Coumn_name).is_in(filter_file))\n",
    "                    Row_Compare(filtered_DF_standard_pattern,file_error,folder_name)\n",
    "               \n",
    "               #Row check for wild file name\n",
    "               filtered_DF_other_pattern = current_check.filter(pl.col(Name_Coumn_name).is_in(filter_file).not_())\n",
    "               if filtered_DF_other_pattern.count()[0,1]>1 and folder_name != \"CUIC\": \n",
    "                    #Delete the [and folder_name != \"CUIC\"] when feel like it's time (üëâÔæü„ÉÆÔæü)üëâDue to CUIC format is under renovation, temporarely remove it from the check so the number of file with new rows pattern will soon be sufficient to not bluffing during this report\n",
    "                    Row_Compare_Special(filtered_DF_other_pattern,file_error)\n",
    "\n",
    "                \n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error check CODE 2 and read LOG\n",
    "def Error_Check(Dataframe,extract_file,LINK):\n",
    "    #Check code 2\n",
    "    Only_Important = Dataframe.filter(pl.col(important_column)==\"IMPORTANT\",pl.col(Check_dup_column)>0)\n",
    "    if Only_Important.count()[0,1]>0:\n",
    "        for row in Only_Important.iter_rows(named=True):\n",
    "                print(f\"{bcolors.FAIL}Folder {row[Table_column]} has {row[Check_dup_column]} duplicate row{\"s\"if row[Check_dup_column]>1 else \" \"}{bcolors.ENDC}\")\n",
    "                extract_file.append((\"\",row[Table_column],F\"{row[Check_dup_column]} row(s) of duplicate data\"))\n",
    "    #Check log\n",
    "    for file in glob.glob(LINK):\n",
    "        #Get file name -> folder name\n",
    "        Current_file = pl.read_excel(file)\n",
    "        filename = os.path.basename(file)\n",
    "        foldername = re.split('_log',filename)\n",
    "        # Looking out for error in error column of log file\n",
    "        try:\n",
    "            if Current_file.filter(pl.col(Error_log_column)!=\"\").count()[0,1]>0:\n",
    "                for row in Current_file.filter(pl.col(Error_log_column)!=\"\").iter_rows(named=True):\n",
    "                    print(f\"{bcolors.FAIL}Folder {row[Log_name_column]} in {foldername[0]} a has fatal error{bcolors.ENDC}\")\n",
    "                    extract_file.append((row[Log_name_column],foldername[0],\"Fatal Error\"))        \n",
    "        except Exception:\n",
    "            continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[96mWpDetail need to update the newest data \u001b[0m\n",
      "\u001b[96mIEX_Hrs need to update the newest data \u001b[0m\n",
      "\u001b[96mFile transfer.xlsx - 2025-02-25 19:59:33 in LTTransfers might need to update\u001b[0m\n",
      "\u001b[93mFile 20250330.csv - 2025-04-03 18:13:34 in CPI has inconsistent number of rows \u001b[0m\n",
      "\u001b[96mFile 202303_202502.xlsx - 2025-02-27 20:34:37 in DailyReq might need to update\u001b[0m\n",
      "\u001b[96mQuality need to update the newest data \u001b[0m\n",
      "\u001b[96mFile EPS Tableau - 20250406.csv - 2025-04-03 23:15:28 in EPS might need to update\u001b[0m\n",
      "\u001b[93mFile [WFM] Contact Tracker.xlsx - 2025-04-07 20:32:15 in Contrack has inconsistent number of rows \u001b[0m\n",
      "\u001b[96mFile 20250101_20251231.xlsx - 2025-03-15 04:47:30 in RONA might need to update\u001b[0m\n",
      "\u001b[93mCUIC_RTMonitor is not expected for the update check \u001b[0m\n",
      "\u001b[96mFile 20240101_20250223.xlsm - 2025-02-24 17:17:24 in ProjectedHC might need to update\u001b[0m\n",
      "\u001b[96mFile 20250224_20250302.xlsm - 2025-02-25 20:16:13 in ProjectedHC might need to update\u001b[0m\n",
      "\u001b[91mFolder 2025-04-07_1.xlsx in CUIC a has fatal error\u001b[0m\n",
      "\u001b[91mFolder 2025-04-07_2.xlsx in CUIC a has fatal error\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#MAIN RUN\n",
    "file_error= []\n",
    "CategoryCheck(Code1_Result,file_error)\n",
    "Error_Check(Code2_Result,file_error,LOG_LINK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Final Report\n",
    "error_report = pl.DataFrame(file_error, schema=[\"File Name\",\"Folder\",\"Reason\"],)\n",
    "Output_path = os.path.join(user_credential,\n",
    "                                r'DataBase//DataRaw//BKN')\n",
    "Output_file = os.path.join(Output_path,\n",
    "                            r'Mod_Log.xlsx')\n",
    "error_report.write_excel(Output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
