{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6cef9a-1859-4587-8485-dec8860dee8a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Library listü§ñ\n",
    "import os, sys, urllib.parse\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, Any\n",
    "import pandas as pd\n",
    "import sqlalchemy as sa\n",
    "from sqlalchemy.engine import Engine\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "\n",
    "# --- Supabase Connection Information (Letungquan246) ---\n",
    "db_host = \"aws-0-ap-southeast-1.pooler.supabase.com\"\n",
    "db_port = \"6543\"\n",
    "db_name = \"postgres\"\n",
    "db_user = \"postgres.rlruseexdmwoiplbuzsv\"\n",
    "raw_password = \"Workforce@210997\"\n",
    "schema_name = \"public\"\n",
    "user_credential = Path(os.environ['USERPROFILE']) / r'Concentrix Corporation//CNXVN - WFM Team - Documents//'\n",
    "# URL-encode the password\n",
    "encoded_password = urllib.parse.quote_plus(raw_password)\n",
    "# Create connection string with the encoded password\n",
    "connection_string = f\"postgresql+psycopg2://{db_user}:{encoded_password}@{db_host}:{db_port}/{db_name}\"\n",
    "engine = None\n",
    "try:\n",
    "    print(f\"üîÑ Connecting to Supabase: {db_host}:{db_port}/{db_name} with user {db_user}...\")\n",
    "    engine = sa.create_engine(connection_string, pool_pre_ping=True)\n",
    "    with engine.connect() as connection:\n",
    "        print(\"‚úÖ Successfully connected to Supabase!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Supabase connection error:\")\n",
    "    print(f\"¬† ¬†Connection String: postgresql+psycopg2://{db_user}:******@{db_host}:{db_port}/{db_name}\") # Mask password when printing error\n",
    "    print(f\"¬† ¬†Error details: {e}\")\n",
    "    sys.exit(1) # Exit if connection fails\n",
    "\n",
    "# --- Supabase Connection Information (Letungquan97) ---\n",
    "db_user2 = \"postgres.cgkltabxmucfnhzuesqj\"\n",
    "# URL-encode the password\n",
    "encoded_password = urllib.parse.quote_plus(raw_password)\n",
    "# Create connection string with the encoded password\n",
    "connection_string = f\"postgresql+psycopg2://{db_user2}:{encoded_password}@{db_host}:{db_port}/{db_name}\"\n",
    "engine2 = None\n",
    "try:\n",
    "    print(f\"üîÑ Connecting to Supabase: {db_host}:{db_port}/{db_name} with user {db_user2}...\")\n",
    "    engine2 = sa.create_engine(connection_string, pool_pre_ping=True)\n",
    "    with engine2.connect() as connection:\n",
    "        print(\"‚úÖ Successfully connected to Supabase!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Supabase connection error:\")\n",
    "    print(f\"¬† ¬†Connection String: postgresql+psycopg2://{db_user2}:******@{db_host}:{db_port}/{db_name}\") # Mask password when printing error\n",
    "    print(f\"¬† ¬†Error details: {e}\")\n",
    "    sys.exit(1) # Exit if connection fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d229d2-2d33-4fb0-833a-a43ff5034b9a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function Definitionüõ†Ô∏è\n",
    "\n",
    "# print_write_summaryüí°\n",
    "def print_write_summary(schema_name: str, table_name: str, dataframe: pd.DataFrame, write_mode: str):\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"üîÑ Preparing to write data to table: \\\"{schema_name}\\\".\\\"{table_name}\\\"\")\n",
    "    print(f\"   Schema: {schema_name}\")\n",
    "    print(f\"   Table: {table_name}\")\n",
    "    print(f\"   Number of data rows: {len(dataframe)}\")\n",
    "    print(f\"   *** SELECTED WRITE MODE: '{write_mode}' ***\")\n",
    "    # Explain selected mode\n",
    "    if write_mode == \"replace\":\n",
    "        print(\"\\n   ‚ö†Ô∏è Mode 'replace':\")\n",
    "        print(f\"    - If table \\\"{schema_name}\\\".\\\"{table_name}\\\" exists, it will be DROPPED and recreated.\")\n",
    "        print(f\"    - The new table structure will be based on the CSV file.\")\n",
    "        print(f\"    - Required permissions: DROP, CREATE TABLE, INSERT.\")\n",
    "    elif write_mode == \"append\":\n",
    "        print(\"\\n   ‚ÑπÔ∏è Mode 'append':\")\n",
    "        print(f\"    - Existing data in table \\\"{schema_name}\\\".\\\"{table_name}\\\" will be CLEARED (TRUNCATE).\")\n",
    "        print(f\"    - New data from CSV will be ADDED to the table.\")\n",
    "        print(f\"    - Table structure must exist and be compatible with the CSV.\")\n",
    "        print(f\"    - Required permissions: TRUNCATE, INSERT.\")\n",
    "    elif write_mode == \"fail\":\n",
    "         print(\"\\n   ‚ÑπÔ∏è Mode 'fail':\")\n",
    "         print(f\"    - If table \\\"{schema_name}\\\".\\\"{table_name}\\\" already exists, the script will stop and raise an error.\")\n",
    "         print(f\"    - If the table does not exist, it will be created and data will be inserted.\")\n",
    "         print(f\"    - Required permissions: CREATE TABLE, INSERT.\")\n",
    "    else:\n",
    "         print(f\"\\n   ‚ùì Unknown write mode: '{write_mode}'. Behavior undefined.\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "# validate_csv_existsüí°\n",
    "def validate_csv_exists(file_path: str, db_engine: Optional[Engine] = None):\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"‚ùå Error: CSV file not found at path:\")\n",
    "        print(f\"   '{file_path}'\")\n",
    "        print(\"   Please double-check the file path.\")\n",
    "        if db_engine:\n",
    "            print(\"   Disposing database engine connection pool...\")\n",
    "            db_engine.dispose() # Dispose engine if provided and file not found\n",
    "        print(\"Exiting script due to missing file.\")\n",
    "        sys.exit(1) # Exit script with error code (1)\n",
    "\n",
    "# write_dataframe_to_dbüí°\n",
    "def write_dataframe_to_db(\n",
    "    dataframe: pd.DataFrame,\n",
    "    db_engine: Engine,\n",
    "    schema_name: str,\n",
    "    table_name: str,\n",
    "    write_mode: str,\n",
    "    db_user: str\n",
    "):\n",
    "    try:\n",
    "        print(f\"\\nüîÑ Starting data writing process (Mode: '{write_mode}')...\")\n",
    "        with db_engine.connect() as connection:\n",
    "            # Start transaction to ensure all-or-nothing success\n",
    "            with connection.begin():\n",
    "            # === ADD TRUNCATE LOGIC WHEN write_mode == 'append' ===\n",
    "                if write_mode == \"append\":\n",
    "                    print(f\"   [append mode] üîÑ Executing TRUNCATE on table \\\"{schema_name}\\\".\\\"{table_name}\\\"...\")\n",
    "                    try:\n",
    "                        truncate_sql = sa.text(f'TRUNCATE TABLE \"{schema_name}\".\"{table_name}\" RESTART IDENTITY;')\n",
    "                        connection.execute(truncate_sql)\n",
    "                        print(f\"   [append mode] ‚úÖ TRUNCATE successful.\")\n",
    "                    except Exception as te:\n",
    "                        print(f\"   [append mode] ‚ùå Error during TRUNCATE: {te}\")\n",
    "                        print(f\"      Please check if user '{db_user}' has TRUNCATE permission on table \\\"{schema_name}\\\".\\\"{table_name}\\\".\")\n",
    "                        raise # Re-raise error to stop transaction and report overall failure\n",
    "            # === END OF TRUNCATE LOGIC ===\n",
    "            # Determine if_exists for to_sql\n",
    "                to_sql_if_exists = 'append' if write_mode == 'append' else write_mode\n",
    "                print(f\"   üîÑ Writing data to table using df.to_sql (if_exists='{to_sql_if_exists}')...\")\n",
    "                dataframe.to_sql(\n",
    "                    name=table_name,\n",
    "                    con=connection,\n",
    "                    schema=schema_name,\n",
    "                    if_exists=to_sql_if_exists,\n",
    "                    index=False,\n",
    "                    chunksize=1000,\n",
    "                    method='multi'\n",
    "                    # dtype=your_dtype_map # (Advanced option)\n",
    "                )\n",
    "                print(f\"   ‚úîÔ∏è Writing data using df.to_sql complete.\")\n",
    "    # Print success message based on mode\n",
    "        print(f\"\\n‚úÖ Complete: Processed successfully for table \\\"{schema_name}\\\".\\\"{table_name}\\\".\")\n",
    "        if write_mode == \"replace\":\n",
    "            print(f\"   ‚û°Ô∏è Table was created/replaced and data was written.\")\n",
    "        elif write_mode == \"append\":\n",
    "            print(f\"   ‚û°Ô∏è Table was emptied (TRUNCATE) and new data was written.\")\n",
    "        elif write_mode == \"fail\":\n",
    "             print(f\"   ‚û°Ô∏è Data was written (table might have been created if it didn't exist).\")\n",
    "    except Exception as e:\n",
    "    # Update Error & permission hints\n",
    "        print(f\"\\n‚ùå Error during data writing process to Supabase:\")\n",
    "        print(f\"   Error details: {e}\")\n",
    "        print(\"\\n   Suggestions for common issues:\")\n",
    "        permissions_needed = {'replace': 'DROP, CREATE TABLE, INSERT', 'append': 'TRUNCATE, INSERT', 'fail': 'CREATE TABLE (if needed), INSERT'}\n",
    "        print(f\"    - Permissions: Does user '{db_user}' have sufficient permissions ({permissions_needed.get(write_mode, 'UNKNOWN')}) on schema '{schema_name}' and table '{table_name}'?\")\n",
    "        print(f\"    - Data Types: Are the data types inferred by Pandas from CSV compatible with PostgreSQL?\")\n",
    "        print(f\"    - Column/Table Names: Do names contain special characters?\")\n",
    "        print(f\"    - Invalid Data: Are there any values in the CSV unsuitable for the column type?\")\n",
    "        print(f\"    - Table does not exist (for 'append' mode): Target table must exist before running TRUNCATE/INSERT.\")\n",
    "        print(f\"    - Network connection error/Timeout.\")\n",
    "        raise e # Re-raise error to stop transaction and report overall failure\n",
    "\n",
    "# handle_csv_read_errorüí°\n",
    "def handle_csv_read_error(\n",
    "    exception_obj: Exception,\n",
    "    file_path: str,\n",
    "    db_engine: Optional[Engine] = None\n",
    "):\n",
    "    # Error message\n",
    "    print(f\"‚ùå Error reading CSV file: {file_path}\") \n",
    "    print(f\"   Error details: {exception_obj}\")\n",
    "    # Check encoding error\n",
    "    if \"encoding\" in str(exception_obj).lower():\n",
    "        print(\"   Suggestion: The CSV file might not be UTF-8. Try other encodings like 'latin1' or check the original file.\")\n",
    "    # Dispose engine if provided\n",
    "    if db_engine:\n",
    "        print(\"   Disposing database engine connection pool...\")\n",
    "        db_engine.dispose()\n",
    "    # Exit script\n",
    "    print(\"Exiting script due to CSV reading error.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# read_data_fileüí°\n",
    "def read_data_file(\n",
    "    file_path: str,\n",
    "    db_engine: Optional[Engine] = None,\n",
    "    excel_sheet_name: str = 'Sheet1'\n",
    ") -> pd.DataFrame:\n",
    "    print(f\"üîÑ Reading file: {file_path}...\")\n",
    "    df = None # Initialize df as None\n",
    "    # Get file (extension)\n",
    "    file_name, file_extension = os.path.splitext(file_path)\n",
    "    file_extension_lower = file_extension.lower()\n",
    "    # Read file\n",
    "    if file_extension_lower == '.csv':  #IF CSV\n",
    "        print(f\"   Detected CSV file. Reading with pd.read_csv...\")\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, encoding='utf-8-sig')\n",
    "        except Exception as csv_err:\n",
    "            print(f\"‚ùå Error reading CSV file '{file_path}': {csv_err}\")\n",
    "            if db_engine:\n",
    "                print(\"   Disposing database engine connection pool...\")\n",
    "                db_engine.dispose()\n",
    "            print(\"Exiting script.\")\n",
    "            sys.exit(1)\n",
    "    elif file_extension_lower == '.xlsx':  #IF XLSX\n",
    "        print(f\"   Detected XLSX file. Reading sheet '{excel_sheet_name}' with pd.read_excel...\")\n",
    "        try:\n",
    "            df = pd.read_excel(file_path, sheet_name=excel_sheet_name)\n",
    "        except Exception as excel_err:\n",
    "            print(f\"‚ùå Error reading Excel file '{file_path}': {excel_err}\")\n",
    "            if db_engine:\n",
    "                print(\"   Disposing database engine connection pool...\")\n",
    "                db_engine.dispose()\n",
    "            print(\"Exiting script.\")\n",
    "            sys.exit(1) \n",
    "    else: # Undefined\n",
    "        print(f\"‚ùå Error: Unsupported file type: '{file_extension}'. This script only supports .csv and .xlsx files.\")\n",
    "        if db_engine:\n",
    "            print(\"   Disposing database engine connection pool...\")\n",
    "            db_engine.dispose()\n",
    "        print(\"Exiting script.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # --- Successfully read process ---\n",
    "    if df is not None:\n",
    "        print(f\"‚úîÔ∏è Successfully read {len(df)} rows from {file_extension_lower} file.\")\n",
    "        print(f\"   Original columns: {df.columns.tolist()}\")\n",
    "        # Standardized columns\n",
    "        original_columns = df.columns.tolist()\n",
    "        df.columns = [''.join(filter(lambda x: x.isalnum() or x == '_', str(col).lower().replace(' ', '_'))) for col in df.columns]\n",
    "        print(f\"   Standardized columns for SQL: {df.columns.tolist()}\")\n",
    "        return df\n",
    "    else:\n",
    "        print(f\"‚ùå Error: DataFrame is None after attempting to read file '{file_path}'.\")\n",
    "        if db_engine: db_engine.dispose()\n",
    "        sys.exit(1)\n",
    "\n",
    "# convert_column_dateüí°\n",
    "def convert_column_date(\n",
    "    dataframe: pd.DataFrame,\n",
    "    column_name: str,\n",
    "    date_format: str = '%m/%d/%Y'\n",
    "):\n",
    "    # Check if column is exist\n",
    "    if column_name in dataframe.columns:\n",
    "        print(f\"   üîÑ Converting column '{column_name}' to datetime using format '{date_format}'...\")\n",
    "        try:      \n",
    "            dataframe[column_name] = pd.to_datetime(dataframe[column_name], format=date_format, errors='coerce') # Convert\n",
    "            print(f\"   ‚úîÔ∏è Conversion attempt for column '{column_name}' finished.\")\n",
    "            # Check NA value\n",
    "            nat_count = dataframe[column_name].isnull().sum()\n",
    "            if nat_count > 0:\n",
    "                print(f\"      ‚ö†Ô∏è Warning: Found {nat_count} values in '{column_name}' that could not be converted using format '{date_format}' (set to Null/NaT). Please check the source data.\")\n",
    "        except ValueError as ve:\n",
    "             print(f\"   ‚ùå ValueError during conversion of column '{column_name}': {ve}. This might happen if the format string '{date_format}' doesn't match any data.\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå An unexpected error occurred during conversion of column '{column_name}': {e}.\")\n",
    "    else:\n",
    "        # column is not exist\n",
    "        print(f\"   ‚ö†Ô∏è Warning: Column '{column_name}' not found in DataFrame. Skipping datetime conversion.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4701ac-d36c-4450-b5af-e0a2fdd512f2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Model_EEAAOüß†\n",
    "\n",
    "# --- 1. Table Information ---\n",
    "write_mode = \"replace\" # <<<----- CHANGE MODE HERE IF NEEDED\n",
    "# 'replace': Drop old table (DROP), create new table, insert data. || 'append': Clear old data (TRUNCATE), insert new data into existing table. || 'fail': Raise an error if the table already exists.\n",
    "csv_file_path = user_credential / r'DataBase//TrainModel//EEAAO//EEAAO_MODEL.xlsx'\n",
    "table_name = \"Model_EEAAO\"\n",
    "sheet_to_read = \"Query1\"\n",
    "validate_csv_exists(csv_file_path, engine)\n",
    "\n",
    "# --- 2. Read File ---\n",
    "try:\n",
    "    df = read_data_file(csv_file_path, engine, excel_sheet_name=sheet_to_read)\n",
    "# ==Edit Column===============================================\n",
    "\n",
    "# ============================================================\n",
    "    print(\"   Pandas inferred data types (dtypes):\")\n",
    "    df.info() # Provides an overview of the DataFrame\n",
    "except Exception as e: # Print additional info if it might help debugging (e.g., encoding error)\n",
    "    handle_csv_read_error(e, csv_file_path, engine)\n",
    "\n",
    "# --- 3. Write Data to Supabase ---\n",
    "print_write_summary(schema_name, table_name, df, write_mode)\n",
    "try:\n",
    "    write_dataframe_to_db(dataframe=df, db_engine=engine, schema_name=schema_name, table_name=table_name, write_mode=write_mode, db_user=db_user)    \n",
    "    print(\"\\nMain script: write_dataframe_to_db completed successfully.\") # Successfully\n",
    "except Exception as main_error:   \n",
    "    print(f\"\\nMain script: An error occurred during database write operation: {main_error}\") # Error\n",
    "finally:\n",
    "    if engine:\n",
    "        engine.dispose()\n",
    "        print(\"\\n‚ÑπÔ∏è Connection pool closed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dc78de-76be-4ade-83d3-cc18a12fa094",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# [Tony]_Outlook_Calendarüíæ\n",
    "\n",
    "# --- 1. Table Information ---\n",
    "write_mode = \"replace\" # <<<----- CHANGE MODE HERE IF NEEDED\n",
    "# 'replace': Drop old table (DROP), create new table, insert data. || 'append': Clear old data (TRUNCATE), insert new data into existing table. || 'fail': Raise an error if the table already exists.\n",
    "csv_file_path = user_credential / r'RTA_PersonalFile//Tony//Template//Meeting schedule.xlsx'\n",
    "table_name = \"Outlook_Calendar\"\n",
    "sheet_to_read = \"Sheet1\"\n",
    "validate_csv_exists(csv_file_path, engine)\n",
    "\n",
    "# --- 2. Read File ---\n",
    "try:\n",
    "    df = read_data_file(csv_file_path, engine, excel_sheet_name=sheet_to_read)\n",
    "# ==Edit Column===============================================\n",
    "\n",
    "# ============================================================\n",
    "    print(\"   Pandas inferred data types (dtypes):\")\n",
    "    df.info() # Provides an overview of the DataFrame\n",
    "except Exception as e: # Print additional info if it might help debugging (e.g., encoding error)\n",
    "    handle_csv_read_error(e, csv_file_path, engine2)\n",
    "\n",
    "# --- 3. Write Data to Supabase ---\n",
    "print_write_summary(schema_name, table_name, df, write_mode)\n",
    "try:\n",
    "    write_dataframe_to_db(dataframe=df, db_engine=engine2, schema_name=schema_name, table_name=table_name, write_mode=write_mode, db_user=db_user2)    \n",
    "    print(\"\\nMain script: write_dataframe_to_db completed successfully.\") # Successfully\n",
    "except Exception as main_error:   \n",
    "    print(f\"\\nMain script: An error occurred during database write operation: {main_error}\") # Error\n",
    "finally:\n",
    "    if engine2:\n",
    "        engine2.dispose()\n",
    "        print(\"\\n‚ÑπÔ∏è Connection pool closed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
